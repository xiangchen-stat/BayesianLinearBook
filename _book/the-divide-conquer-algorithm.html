<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 The Divide &amp; Conquer Algorithm | Bayesian Linear Regression Tutorial</title>
  <meta name="description" content="This is a first tutorial for Bayesian Linear Regression assembled in book form." />
  <meta name="generator" content="bookdown 0.31 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 The Divide &amp; Conquer Algorithm | Bayesian Linear Regression Tutorial" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a first tutorial for Bayesian Linear Regression assembled in book form." />
  <meta name="github-repo" content="xiangchen-stat/BayesianLinearBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 The Divide &amp; Conquer Algorithm | Bayesian Linear Regression Tutorial" />
  
  <meta name="twitter:description" content="This is a first tutorial for Bayesian Linear Regression assembled in book form." />
  

<meta name="author" content="Xiang Chen, Valentina Arputhasamy, Daniel Zhou, Sudipto Banerjee" />


<meta name="date" content="2023-02-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="basics-of-bayesian-linear-regression.html"/>
<link rel="next" href="posteriors-using-sufficient-statistics.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html"><i class="fa fa-check"></i><b>1</b> Basics of Bayesian linear regression</a><ul>
<li class="chapter" data-level="1.1" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html#bayes-theorem"><i class="fa fa-check"></i><b>1.1</b> Bayesâ€™ theorem</a></li>
<li class="chapter" data-level="1.2" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html#normal-inverse-gamma-nig-prior"><i class="fa fa-check"></i><b>1.2</b> Normal-Inverse-Gamma (NIG) prior</a><ul>
<li class="chapter" data-level="1.2.1" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html#joint-distribution-of-nig-prior"><i class="fa fa-check"></i><b>1.2.1</b> Joint distribution of NIG prior</a></li>
<li class="chapter" data-level="1.2.2" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html#marginal-distribution-of-nig-prior"><i class="fa fa-check"></i><b>1.2.2</b> Marginal distribution of NIG prior</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html#conjugate-bayesian-linear-regression-and-mm-formula"><i class="fa fa-check"></i><b>1.3</b> Conjugate Bayesian linear regression and M&amp;m formula</a></li>
<li class="chapter" data-level="1.4" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html#updating-form-of-the-posterior-distribution"><i class="fa fa-check"></i><b>1.4</b> Updating form of the posterior distribution</a><ul>
<li class="chapter" data-level="1.4.1" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html#method-1-sherman-woodbury-morrison-identity"><i class="fa fa-check"></i><b>1.4.1</b> Method 1: Sherman-Woodbury-Morrison identity</a></li>
<li class="chapter" data-level="1.4.2" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html#method-2-distribution-theory"><i class="fa fa-check"></i><b>1.4.2</b> Method 2: Distribution theory</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html#bayesian-prediction"><i class="fa fa-check"></i><b>1.5</b> Bayesian prediction</a></li>
<li class="chapter" data-level="1.6" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html#sampling-from-the-posterior-distribution"><i class="fa fa-check"></i><b>1.6</b> Sampling from the posterior distribution</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-divide-conquer-algorithm.html"><a href="the-divide-conquer-algorithm.html"><i class="fa fa-check"></i><b>2</b> The Divide &amp; Conquer Algorithm</a><ul>
<li class="chapter" data-level="2.1" data-path="the-divide-conquer-algorithm.html"><a href="the-divide-conquer-algorithm.html#the-problem"><i class="fa fa-check"></i><b>2.1</b> The Problem</a></li>
<li class="chapter" data-level="2.2" data-path="the-divide-conquer-algorithm.html"><a href="the-divide-conquer-algorithm.html#parallel-computing"><i class="fa fa-check"></i><b>2.2</b> Parallel Computing</a><ul>
<li class="chapter" data-level="2.2.1" data-path="the-divide-conquer-algorithm.html"><a href="the-divide-conquer-algorithm.html#background-and-motivation"><i class="fa fa-check"></i><b>2.2.1</b> Background and Motivation</a></li>
<li class="chapter" data-level="2.2.2" data-path="the-divide-conquer-algorithm.html"><a href="the-divide-conquer-algorithm.html#solution"><i class="fa fa-check"></i><b>2.2.2</b> Solution</a></li>
<li class="chapter" data-level="2.2.3" data-path="the-divide-conquer-algorithm.html"><a href="the-divide-conquer-algorithm.html#algebra"><i class="fa fa-check"></i><b>2.2.3</b> Algebra</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="the-divide-conquer-algorithm.html"><a href="the-divide-conquer-algorithm.html#sequential-computing"><i class="fa fa-check"></i><b>2.3</b> Sequential Computing</a><ul>
<li class="chapter" data-level="2.3.1" data-path="the-divide-conquer-algorithm.html"><a href="the-divide-conquer-algorithm.html#background-and-motivation-1"><i class="fa fa-check"></i><b>2.3.1</b> Background and Motivation</a></li>
<li class="chapter" data-level="2.3.2" data-path="the-divide-conquer-algorithm.html"><a href="the-divide-conquer-algorithm.html#solution-1"><i class="fa fa-check"></i><b>2.3.2</b> Solution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="posteriors-using-sufficient-statistics.html"><a href="posteriors-using-sufficient-statistics.html"><i class="fa fa-check"></i><b>3</b> Posteriors Using Sufficient Statistics</a><ul>
<li class="chapter" data-level="3.1" data-path="posteriors-using-sufficient-statistics.html"><a href="posteriors-using-sufficient-statistics.html#methods"><i class="fa fa-check"></i><b>3.1</b> Methods</a></li>
<li class="chapter" data-level="3.2" data-path="posteriors-using-sufficient-statistics.html"><a href="posteriors-using-sufficient-statistics.html#posterior-from-improper-priors"><i class="fa fa-check"></i><b>3.2</b> Posterior From Improper Priors</a></li>
<li class="chapter" data-level="3.3" data-path="posteriors-using-sufficient-statistics.html"><a href="posteriors-using-sufficient-statistics.html#extension-to-divide-conquer-algorithm"><i class="fa fa-check"></i><b>3.3</b> Extension to Divide &amp; Conquer Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="forward-filtering-backward-sampling---shared-variance.html"><a href="forward-filtering-backward-sampling---shared-variance.html"><i class="fa fa-check"></i><b>4</b> Forward Filtering Backward Sampling - Shared Variance</a><ul>
<li class="chapter" data-level="4.1" data-path="forward-filtering-backward-sampling---shared-variance.html"><a href="forward-filtering-backward-sampling---shared-variance.html#background"><i class="fa fa-check"></i><b>4.1</b> Background</a></li>
<li class="chapter" data-level="4.2" data-path="forward-filtering-backward-sampling---shared-variance.html"><a href="forward-filtering-backward-sampling---shared-variance.html#derivation-of-the-forward-filter"><i class="fa fa-check"></i><b>4.2</b> Derivation of the Forward Filter</a><ul>
<li class="chapter" data-level="4.2.1" data-path="forward-filtering-backward-sampling---shared-variance.html"><a href="forward-filtering-backward-sampling---shared-variance.html#deriving-beta_tvert-y_t"><i class="fa fa-check"></i><b>4.2.1</b> Deriving <span class="math inline">\(\beta_{t}\vert Y_{t}\)</span></a></li>
<li class="chapter" data-level="4.2.2" data-path="forward-filtering-backward-sampling---shared-variance.html"><a href="forward-filtering-backward-sampling---shared-variance.html#deriving-sigma2vert-y_t"><i class="fa fa-check"></i><b>4.2.2</b> Deriving <span class="math inline">\(\sigma^{2}\vert Y_{t}\)</span></a></li>
<li class="chapter" data-level="4.2.3" data-path="forward-filtering-backward-sampling---shared-variance.html"><a href="forward-filtering-backward-sampling---shared-variance.html#commentary"><i class="fa fa-check"></i><b>4.2.3</b> Commentary</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="forward-filtering-backward-sampling---shared-variance.html"><a href="forward-filtering-backward-sampling---shared-variance.html#derivation-of-the-backwards-sampling"><i class="fa fa-check"></i><b>4.3</b> Derivation of the Backwards Sampling</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#deriving-x_2-vert-x_1-when-x_1-x_2mathrmscriptscriptstyle-t-is-a-block-normal-multivariate-random-variable"><i class="fa fa-check"></i>Deriving <span class="math inline">\(x_{2} \vert x_{1}\)</span> when <span class="math inline">\((x_{1} x_{2})^{\mathrm{\scriptscriptstyle T}}\)</span> is a block-normal multivariate random variable</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Linear Regression Tutorial</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-divide-conquer-algorithm" class="section level1 hasAnchor">
<h1><span class="header-section-number">Chapter 2</span> The Divide &amp; Conquer Algorithm<a href="the-divide-conquer-algorithm.html#the-divide-conquer-algorithm" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>The divide and conquer algorithm is a strategy of solving a large problem by breaking it down into two or more smaller and more manageable sub-problems of the same or of a similar genre. The solutions to the sub-problems are then combined to get the desired output: the solution to the original problem.</p>
<p>In this section, we apply the divide and conquer algorithm to the Bayesian linear regression framework.</p>
<div id="the-problem" class="section level2 hasAnchor">
<h2><span class="header-section-number">2.1</span> The Problem<a href="the-divide-conquer-algorithm.html#the-problem" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let <span class="math inline">\(y = (y_1, y_2, ...,y_n)^{T}\)</span> be an <span class="math inline">\(n \times 1\)</span> random vector of outcomes and <span class="math inline">\(X\)</span> be a fixed <span class="math inline">\(n \times p\)</span> matrix of predictors with full column rank. Consider the following Bayesian (hierarchical) linear regression model,</p>
<p><span class="math display">\[ NIG(\beta, \sigma^2 \mid m_0, M_0, a_0, b_0) \times N(y \mid X\beta, \sigma^2I_n).\]</span>
The posterior density is <span class="math inline">\(p(\beta, \sigma^2 \mid y) = IG(\sigma^2 \mid a_1, b_1) \times N(\beta \mid M_1m_1, \sigma^2M_1)\)</span>, and to carry out Bayesian inference, we first sample <span class="math inline">\(\sigma^2 \sim IG(a_1, b_1)\)</span> and then, for each sampled <span class="math inline">\(\sigma^2\)</span>, we sample <span class="math inline">\(\beta \sim N(M_1m_1, \sigma^2 M_1)\)</span>.</p>
<p>Assume that <span class="math inline">\(n\)</span> is so large that we are unable to store or load <span class="math inline">\(y\)</span> or <span class="math inline">\(X\)</span> into our CPU to carry out computations for them. We decide to divide our data set into K mutually exclusive and exhaustive subsets, each comprising a manageable number of points. Note that <span class="math inline">\(p\)</span> is small, so computations involving <span class="math inline">\(p \times p\)</span> matrices are fine. Let <span class="math inline">\(y_k\)</span> denote the <span class="math inline">\(q_k \times 1\)</span> sub-vector of <span class="math inline">\(y\)</span>, and <span class="math inline">\(X_k\)</span> be the <span class="math inline">\(q_k \times p\)</span> sub-matrix of <span class="math inline">\(X\)</span> in subset <span class="math inline">\(k\)</span>, where each <span class="math inline">\(q_k\)</span> has been chosen by us so that <span class="math inline">\(q_k &gt; p\)</span>, and is small enough such that we can fit the above model on <span class="math inline">\(\{y_k, X_k\}\)</span>. This section will clearly explain how we can still compute <span class="math inline">\(a_1, b_1, M_1\)</span> and <span class="math inline">\(m_1\)</span> without ever having to store or compute with <span class="math inline">\(y\)</span> or <span class="math inline">\(X\)</span>, but with quantities computed using only the subsets <span class="math inline">\(\{y_k, X_k\}\)</span> for <span class="math inline">\(k = 1,2,...,K\)</span>.</p>
</div>
<div id="parallel-computing" class="section level2 hasAnchor">
<h2><span class="header-section-number">2.2</span> Parallel Computing<a href="the-divide-conquer-algorithm.html#parallel-computing" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="background-and-motivation" class="section level3 hasAnchor">
<h3><span class="header-section-number">2.2.1</span> Background and Motivation<a href="the-divide-conquer-algorithm.html#background-and-motivation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A first approach to the problem presented above is called â€˜parallel computingâ€™. Parallel computing is a type of computation in which many calculations or processes are carried out simultaneously - which in our context, entails dividing our data set into manageable subsets, calculating posteriors for each of these subsets simultaneously, and finally expressing the posteriors of the entire data set as functions of the posteriors of the subsets. The motivation behind this method is to make computation more efficient.</p>
</div>
<div id="solution" class="section level3 hasAnchor">
<h3><span class="header-section-number">2.2.2</span> Solution<a href="the-divide-conquer-algorithm.html#solution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using the multivariate completing the square method, we know that the explicit expressions for <span class="math inline">\(a_1, b_1, m_1\)</span> and <span class="math inline">\(M_1\)</span> are given by:</p>
<style type="text/css">
.ul { border: 2px solid black; }
</style>
<ul class="custom">
<li>
<span class="math inline">\(a_1\)</span> = <span class="math inline">\(a_0 + \frac{n}{2}\)</span><br />

<li>
<span class="math inline">\(b_1\)</span> = <span class="math inline">\(b_0 + \frac{c}{2}\)</span><br />

<li>
<span class="math inline">\(c\)</span> = <span class="math inline">\(m_0^{T}M^{-1}_0 m_0 + y^{\mathrm{\scriptscriptstyle T}}y - m_1^{\mathrm{\scriptscriptstyle T}}M_1m_1\)</span><br />

<li>
<span class="math inline">\(m_1\)</span> = <span class="math inline">\((X^{\mathrm{\scriptscriptstyle T}}y + M_0^{-1}m_0)\)</span><br />

<li>
<span class="math inline">\(M_1^{-1}\)</span> = <span class="math inline">\(X^{\mathrm{\scriptscriptstyle T}}X + M_0^{-1}\)</span>
</ul>
This implies that the explicit expressions for <span class="math inline">\(a_1, b_1, m_1\)</span> and <span class="math inline">\(M_1\)</span> for the <span class="math inline">\(i^{th}\)</span> subset are given by:
<ul class="custom">
<li>
<span class="math inline">\(a_{1i}\)</span> = <span class="math inline">\(a_0 + \frac{q_i}{2}\)</span><br />

<li>
<span class="math inline">\(b_{1i}\)</span> = <span class="math inline">\(b_0 + \frac{c}{2}\)</span><br />

<li>
<span class="math inline">\(c_i\)</span> = <span class="math inline">\(m_0^{\mathrm{\scriptscriptstyle T}}M^{-1}_0 m_0 + y_i^{\mathrm{\scriptscriptstyle T}}y_i - m_{1i}^{\mathrm{\scriptscriptstyle T}}M_{1i}m_{1i}\)</span>
<li>
<span class="math inline">\(m_i\)</span> = <span class="math inline">\((X_i^{\mathrm{\scriptscriptstyle T}}y_i + M_0^{-1}m_0)\)</span><br />

<li>
<span class="math inline">\(M_i^{-1}\)</span> = <span class="math inline">\(X_i^{\mathrm{\scriptscriptstyle T}}X_i + M_0^{-1}\)</span>
</ul>
<p>We can express the posteriors of the entire data set as a function of the posteriors of the subsets as follows:</p>
<ul class="custom">
<li>
<span class="math inline">\(a_1\)</span> = <span class="math inline">\(\sum_{i=1}^{k} a_{1i} + (k-1)a_0\)</span>
<li>
<span class="math inline">\(b_1\)</span> = <span class="math inline">\(b_0 + m_0^{\mathrm{\scriptscriptstyle T}}M^{-1}_0 m_0 + \sum_{i=1}^{k}y_i^{\mathrm{\scriptscriptstyle T}}y_i - m_{1i}^{\mathrm{\scriptscriptstyle T}}M_{1i}m_{1i}\)</span><br />

<li>
<span class="math inline">\(m_1\)</span> = <span class="math inline">\(\sum_{i=1}^{k}m_{1i} - (k-1)M^{-1}_0m_0\)</span><br />

<li>
<span class="math inline">\(M_1^{-1}\)</span> = <span class="math inline">\(\sum_{i=1}^{k}x_i^{\mathrm{\scriptscriptstyle T}}x_i + M^{-1}_0 = \sum_{i=1}^{k}M^{-1}_{1i} - (k-1)M^{-1}_0\)</span>
</ul>
</div>
<div id="algebra" class="section level3 hasAnchor">
<h3><span class="header-section-number">2.2.3</span> Algebra<a href="the-divide-conquer-algorithm.html#algebra" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This section details the algebra used to obtain the posteriors of the entire
data set expressed as functions of the posteriors of the subsets.</p>
<p class="round1">
Starting with <span class="math inline">\(a_1\)</span>:
</p>
<p><span class="math display">\[\begin{align*}
\sum_{i=1}^{k} a_{i1} &amp;= \sum_{i=1}^{k} a_0 + \sum_{i=1}^{k}\frac{q_i}{2} \\
&amp;= ka_0 + \frac{n}{2} \\
&amp;= (k-1)a_0 + a_0 + \frac{n}{2} \\
&amp;= (k-1)a_0 + a_1 \\
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{equation}
\Longrightarrow a_1 = \sum_{i=1}^{k} a_{i1} - (k-1)a_0
\end{equation}\]</span></p>
<p class="round1">
Moving onto <span class="math inline">\(m\)</span>:
</p>
<p>Recall that <span class="math inline">\(m_1 = X^{\mathrm{\scriptscriptstyle T}}y + V_\beta^{-1}\mu_\beta\)</span>,</p>
<p>where
<span class="math display">\[X = \begin{bmatrix}x_1\\
x_2 \\
\vdots \\
x_k
\end{bmatrix} \text{, where}\ x_i \in \mathbb{R}^{m_i \times 1} \ \text{and} \ y = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n 
\end{bmatrix} \text{, where}\ y_i \in \mathbb{R}.\]</span></p>
<p>Using linear algebra we can see that:</p>
<p><span class="math display">\[
X^{\mathrm{\scriptscriptstyle T}}y = \begin{bmatrix}x_1^{\mathrm{\scriptscriptstyle T}} &amp;
x_2^{\mathrm{\scriptscriptstyle T}} &amp; \dots &amp; x_k^{T} \end{bmatrix} \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix} = \sum_{i=1}^{k} x_i^{\mathrm{\scriptscriptstyle T}}y_i
\]</span>
Thus:</p>
<p><span class="math display">\[\begin{align*}
\sum_{i=1}^{k} m_i &amp;= \sum_{i=1}^{k} x_i^{\mathrm{\scriptscriptstyle T}}y_i + kV_\beta\mu_\beta^{-1} \\
&amp;= X^{\mathrm{\scriptscriptstyle T}}y + kV_\beta^{-1}\mu_\beta \\
&amp;= X^{\mathrm{\scriptscriptstyle T}}y + V_\beta^{-1}\mu_\beta + (k-1)V_\beta^{-1}\mu_\beta \\
&amp;= m + (k-1)V_\beta^{-1}\mu_\beta
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{equation}
\Longrightarrow m_1 = \sum_{i=1}^{k}m_i - (k-1)V_\beta^{-1}\mu_\beta
\end{equation}\]</span></p>
<p class="round1">
Next is <span class="math inline">\(M^{-1}_1\)</span>:
</p>
<p>Recall that <span class="math inline">\(M_1^{-1} = X^{\mathrm{\scriptscriptstyle T}}X + V_\beta^{-1}\)</span>.</p>
<p>Using linear algebra again, we can see that:</p>
<p><span class="math display">\[
X^{\mathrm{\scriptscriptstyle T}}X = \begin{bmatrix}x_1^{\mathrm{\scriptscriptstyle T}} &amp;
x_2^{\mathrm{\scriptscriptstyle T}} &amp; \dots &amp; x_k^{\mathrm{\scriptscriptstyle T}} \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_k \end{bmatrix} = \sum_{i=1}^{k} x_i^{\mathrm{\scriptscriptstyle T}}x_i
\]</span>
<span class="math display">\[\begin{equation}
\Longrightarrow M_1^{-1} = \sum_{i=1}^{k}x_i^{\mathrm{\scriptscriptstyle T}}x_i + V_\beta^{-1}
\end{equation}\]</span></p>
<p class="round1">
Last is <span class="math inline">\(b_1\)</span>:
</p>
<p>Recall that <span class="math inline">\(b_1 = b_0 + \frac{m_0^{\mathrm{\scriptscriptstyle T}}M^{-1}_0 m_0 + y^{\mathrm{\scriptscriptstyle T}}y - m_1^{\mathrm{\scriptscriptstyle T}}M_1m_1}{2}\)</span>.</p>
<p>Using linear algebra once again, we can see that:</p>
<p><span class="math display">\[
y^{\mathrm{\scriptscriptstyle T}}y = \begin{bmatrix}y_1^{\mathrm{\scriptscriptstyle T}} &amp;
y_2^{\mathrm{\scriptscriptstyle T}} &amp; \dots &amp; y_n^{\mathrm{\scriptscriptstyle T}} \end{bmatrix} \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix} = \sum_{i=1}^{n} y_i^{\mathrm{\scriptscriptstyle T}}y_i
\]</span>
<span class="math display">\[\begin{equation}
\Longrightarrow b_1 = b_0 + \frac{m_0^{\mathrm{\scriptscriptstyle T}}M^{-1}_0 m_0 + \sum_{i=1}^{n} y_i^{\mathrm{\scriptscriptstyle T}}y_i - m_1^{\mathrm{\scriptscriptstyle T}}M_1m_1}{2}
\end{equation}\]</span></p>
<p>Which concludes the algebra behind obtaining <span class="math inline">\(a_1, b_1, M_1, \text{and} \ m_1\)</span> from the posteriors computed using the subsetted data.</p>
</div>
</div>
<div id="sequential-computing" class="section level2 hasAnchor">
<h2><span class="header-section-number">2.3</span> Sequential Computing<a href="the-divide-conquer-algorithm.html#sequential-computing" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="background-and-motivation-1" class="section level3 hasAnchor">
<h3><span class="header-section-number">2.3.1</span> Background and Motivation<a href="the-divide-conquer-algorithm.html#background-and-motivation-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A second approach to the problem presented in this section is called â€˜sequential computingâ€™. Sequential computing is a type of computation where one instruction is given at a particular time and the next instruction has to wait for the first instruction to execute. For our problem, this entails
computing the posterior for the first subset, using it to
compute posterior of next subset, and so on until we get to the last
subset, which will upon computation will give us the posterior of the entire data set.</p>
</div>
<div id="solution-1" class="section level3 hasAnchor">
<h3><span class="header-section-number">2.3.2</span> Solution<a href="the-divide-conquer-algorithm.html#solution-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let <span class="math inline">\(D_k = \{y_k, X_k\}\)</span> be the <span class="math inline">\(i^{th}\)</span> subset of the entire data set for <span class="math inline">\(k = 1, ..., K\)</span>, such that <span class="math inline">\(D_i \perp D_j ,\ \forall \ i \neq j\)</span>. We will start with a simple example of how sequential computing works by setting <span class="math inline">\(k = 2\)</span>.<br />
</p>
<p>The posterior density of the first subset is:
<span class="math display">\[\begin{equation}
p(\beta, \sigma^2 \mid D_1) \propto IG(\sigma^2 \mid a_1, b_1) \times N(\beta \mid M_1m_1, \sigma^2M_1)
\end{equation}\]</span></p>
<p>The posteriors <span class="math inline">\(\{a_1, b_1, M_1, \text{and} \ m_1\}\)</span> then replace <span class="math inline">\(\{a_0, b_0\, M_0, \text{and} \ m_0\}\)</span> as priors when using the second subset to calculate posteriors:</p>
<p><span class="math display">\[\begin{align*}
p(\beta, \sigma^2 \mid D_1, D_1) &amp;\propto p(\beta, \sigma^2, D_1, D_2) \\
&amp;\propto p(\beta, \sigma^2) \times p(D_1, D_2, \mid \beta, \sigma^2) \\
&amp;\propto p(\beta, \sigma^2) \times p(D_1 \mid \beta, \sigma^2) \times p(D_2 \mid \beta, \sigma^2) \\
&amp;\propto p(\beta, \sigma^2 \mid D_1) \times p(D_2 \mid \beta, \sigma^2)
\end{align*}\]</span></p>
<p>Which illustrates how the posteriors of the previous subset act as a priors when calculating the posteriors of the current subset.</p>
<p>If we generalize this to <span class="math inline">\(k\)</span> subsets and do some algebra, we can derive equations for the posteriors of the last subset, which are equivalent to the posteriors obtained using the full data set.</p>
<ul class="custom">
<li>
<span class="math inline">\(a_{1k}\)</span> = <span class="math inline">\(a_0 + \sum_{i=1}^{k}\frac{q_i}{2}\)</span><br />

<li>
<span class="math inline">\(b_1\)</span> = <span class="math inline">\(b_0 + \frac{c}{2}\)</span><br />

<li>
<span class="math inline">\(c\)</span> = <span class="math inline">\(m_0^{\mathrm{\scriptscriptstyle T}}M^{-1}_0 m_0 + y^{\mathrm{\scriptscriptstyle T}}y - m_1^{\mathrm{\scriptscriptstyle T}}M_1m_1\)</span><br />

<li>
<span class="math inline">\(m_1\)</span> = <span class="math inline">\((X^{\mathrm{\scriptscriptstyle T}}y + M_0^{-1}m_0)\)</span><br />

<li>
<span class="math inline">\(M_1^{-1}\)</span> = <span class="math inline">\(X^{\mathrm{\scriptscriptstyle T}}X + M_0^{-1}\)</span>
</ul>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="basics-of-bayesian-linear-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="posteriors-using-sufficient-statistics.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
