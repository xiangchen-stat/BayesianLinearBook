[["basics-of-bayesian-linear-regression.html", "Chapter 1 Basics of Bayesian linear regression 1.1 Bayes’ theorem 1.2 Normal-Inverse-Gamma (NIG) prior 1.3 Conjugate Bayesian linear regression and M&amp;m formula 1.4 Updating form of the posterior distribution 1.5 Bayesian prediction 1.6 Sampling from the posterior distribution 1.7 Sample code", " Chapter 1 Basics of Bayesian linear regression 1.1 Bayes’ theorem Theorem 1.1 (Bayes' theorem) For events \\(U, K\\) and \\(P(K) \\neq 0\\), we have \\[\\begin{equation*} \\begin{split} P(U \\mid K) &amp;= \\frac{P(K \\mid U) P(U)}{P(K)} \\end{split} \\tag{1.1} \\end{equation*}\\] We denote \\(U\\) as unknown parameters and \\(K\\) as known parameters. We call \\(P(U)\\) prior and \\(P(K|U)\\) likelihood. The Bayes’ theorem gives us the posterior distribution of unknown parameters given the known parameters \\[\\begin{equation*} \\begin{split} P(U \\mid K) &amp;\\propto P(K \\mid U) \\cdot P(U) \\end{split} \\tag{1.2} \\end{equation*}\\] 1.2 Normal-Inverse-Gamma (NIG) prior 1.2.1 Joint distribution of NIG prior Definition 1.1 (Normal-Inverse-Gamma Distribution) Suppose \\[\\begin{equation*} \\begin{split} &amp; \\beta \\mid \\sigma^2, \\mu, M \\sim N(\\mu,\\sigma^2 M) \\\\ &amp; \\sigma^2 \\mid a, b \\sim IG(a, b) \\end{split} \\tag{1.3} \\end{equation*}\\] Then \\((\\beta,\\sigma^2)\\) has a Normal-Inverse-Gamma distribution, denoted as \\((\\beta,\\sigma^2) \\sim NIG(\\mu,M,a,b)\\). We use a Normal-Inverse-Gamma prior for \\((\\beta, \\sigma^2)\\) \\[\\begin{equation*} \\begin{split} P(\\beta, \\sigma^{2}) &amp;= NIG \\left(\\beta, \\sigma^{2} \\mid m_{0}, M_{0}, a_{0}, b_{0}\\right) \\\\ &amp;= \\frac{b_0^{a_0}}{\\Gamma\\left(a_{0}\\right)} \\left(\\frac{1}{\\sigma^{2}}\\right)^{a_{0}+1} e^{-\\frac{b_{0}}{\\sigma^{2}}} \\frac{1}{(2 \\pi \\sigma^{2})^{\\frac{p}{2}}\\left| M_{0}\\right|^{\\frac{1}{2}}} e^{-\\frac{1}{2 \\sigma^{2}} Q \\left(\\beta, m_{0}, M_{0}\\right)} \\end{split} \\tag{1.4} \\end{equation*}\\] where \\(Q(x, m, M)=(x-m)^{\\mathrm{\\scriptscriptstyle T}} M^{-1} (x-m) \\, .\\) Show details \\[\\begin{align} P(\\beta, \\sigma^{2}) &amp;= NIG \\left(\\beta, \\sigma^{2} \\mid m_{0}, M_{0}, a_{0}, b_{0}\\right) \\\\ &amp;= IG\\left(\\sigma^{2} \\mid a_{0}, b_{0}\\right) \\cdot N\\left(\\beta \\mid m_{0}, \\sigma^{2} M_{0}\\right) \\\\ &amp;= \\frac{b_0^{a_0}}{\\Gamma\\left(a_{0}\\right)} \\left(\\frac{1}{\\sigma^{2}}\\right)^{a_{0}+1} e^{-\\frac{b_{0}}{\\sigma^{2}}} \\frac{1}{(2 \\pi)^{\\frac{p}{2}}\\left|\\sigma^{2} M_{0}\\right|^{\\frac{1}{2}}} e^{-\\frac{1}{2 \\sigma^{2}} Q \\left(\\beta, m_{0}, M_{0}\\right)} \\\\ &amp;= \\frac{b_0^{a_0}}{\\Gamma\\left(a_{0}\\right)} \\left(\\frac{1}{\\sigma^{2}}\\right)^{a_{0}+1} e^{-\\frac{b_{0}}{\\sigma^{2}}} \\frac{1}{(2 \\pi \\sigma^{2})^{\\frac{p}{2}}\\left| M_{0}\\right|^{\\frac{1}{2}}} e^{-\\frac{1}{2 \\sigma^{2}} Q \\left(\\beta, m_{0}, M_{0}\\right)} \\\\ \\end{align}\\] where \\(Q(x, m, M)=(x-m)^{\\mathrm{\\scriptscriptstyle T}} M^{-1} (x-m) \\, .\\) Note: the Inverse-Gamma (\\(IG\\)) distribution has a relationship with Gamma distribution. Given \\(X \\sim Gamma(\\alpha, \\beta)\\), the density function of \\(X\\) is \\(f(x)=\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha-1} e^{-\\beta x}\\). Then \\(Y=\\frac{1}{X} \\sim IG\\left(\\alpha, \\beta\\right)\\) with density function \\(f(y)=\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{-\\alpha-1} e^{-\\frac{\\beta}{x}}\\). button#toggle-button1 { background-color: lightgray; color: black; border-radius: 5px; border: none; padding: 5px 10px; font-size: 16px; } button#toggle-button1:hover { background-color: gray; color: white; } 1.2.2 Marginal distribution of NIG prior As for marginal priors, we can can get it by integration \\[\\begin{equation*} \\begin{split} P(\\sigma^2) &amp; = \\int N I G\\left(\\beta, \\sigma^{2} \\mid m_{0}, M_{0}, a_{0}, b_{0}\\right) \\ d\\beta=I G\\left(\\sigma^{2} \\mid a_{0}, b_{0}\\right) \\\\ P(\\beta) &amp; = \\int N I G\\left(\\beta, \\sigma^{2} \\mid m_{0}, M_{0}, a_{0}, b_{0}\\right) \\ d\\sigma^{2}=t_{2a_0}\\left(m_0, \\frac{b_0}{a_0}M_0\\right) \\\\ \\end{split} \\tag{1.5} \\end{equation*}\\] Show details \\[\\begin{align} P\\left(\\sigma^{2} \\right) &amp;= \\int NIG\\left(\\beta, \\sigma^{2} \\mid m_{0}, M_{0}, a_{0}, b_{0}\\right) \\ d\\beta \\\\ &amp;=IG\\left(\\sigma^{2} \\mid a_{0}, b_{0}\\right) \\int N\\left(\\beta \\mid m_{0}, \\sigma^{2} M_{0}\\right) \\ d\\beta \\\\ &amp;=IG\\left(\\sigma^{2} \\mid a_{0}, b_{0}\\right) \\end{align}\\] \\[\\begin{align} P(\\beta ) &amp;=\\int NIG \\left(\\beta, \\sigma^{2} \\mid m_{0}, M_{0}, a_{0}, b_{0}\\right) \\ d\\sigma^{2} \\\\ &amp;= \\int \\frac{b_0^{a_0}}{\\Gamma\\left(a_{0}\\right)} \\left(\\frac{1}{\\sigma^{2}}\\right)^{a_{0}+1} e^{-\\frac{b_{0}}{\\sigma^{2}}} \\frac{1}{(2 \\pi \\sigma^{2})^{\\frac{p}{2}}\\left| M_{0}\\right|^{\\frac{1}{2}}} e^{-\\frac{1}{2 \\sigma^{2}} Q \\left(\\beta, m_{0}, M_{0}\\right)} \\ d\\sigma^{2} \\\\ &amp;= \\frac{b_0^{a_0}}{\\Gamma\\left(a_{0}\\right)(2 \\pi)^{\\frac{p}{2}}\\left| M_{0}\\right|^{\\frac{1}{2}}} \\int \\left(\\frac{1}{\\sigma^{2}}\\right)^{a_{0}+\\frac{p}{2}+1} e^{-\\frac{1}{\\sigma^{2}}\\left(b_{0}+\\frac{1}{2} Q \\left(\\beta, m_{0}, M_{0}\\right)\\right)} \\ d\\sigma^{2} \\\\ &amp; \\quad \\left(\\text{let } u = \\frac{1}{\\sigma^2}, \\left|d\\sigma^{2}\\right|=\\frac{1}{u^2} d u\\right) \\\\ &amp;= \\frac{b_0^{a_0}}{\\Gamma\\left(a_{0}\\right)(2 \\pi)^{\\frac{p}{2}}\\left| M_{0}\\right|^{\\frac{1}{2}}} \\int u^{a_{0}+\\frac{p}{2}+1} e^{-\\left(b_{0}+\\frac{1}{2} Q \\left(\\beta, m_{0}, M_{0}\\right)\\right) u } \\frac{1}{u^2} \\ du \\\\ &amp;= \\frac{b_0^{a_0}}{\\Gamma\\left(a_{0}\\right)(2 \\pi)^{\\frac{p}{2}}\\left| M_{0}\\right|^{\\frac{1}{2}}} \\int u^{a_{0}+\\frac{p}{2}-1} e^{-\\left(b_{0}+\\frac{1}{2} Q \\left(\\beta, m_{0}, M_{0}\\right)\\right) u} \\ du \\\\ &amp; \\quad \\left(\\text{by Gamma integral function:} \\int x^{\\alpha - 1} exp^{-\\beta x} dx = \\frac{\\Gamma(\\alpha)}{\\beta^{\\alpha}}\\right) \\\\ &amp;= \\frac{b_{0}^{a_{0}} }{\\Gamma\\left(a_{0}\\right)(2 \\pi)^\\frac{p}{2}\\left|M_{0}\\right|^{\\frac{1}{2}}} \\frac{\\Gamma\\left(a_{0}+\\frac{p}{2}\\right)}{\\left(b_{0}+\\frac{1}{2} Q(\\beta,m_0,M_0)\\right)^{\\left(a_{0}+\\frac{p}{2}\\right)}} \\\\ &amp; = \\frac{b_0^{a_0}\\Gamma\\left(a_{0}+\\frac{p}{2}\\right)}{\\Gamma\\left(a_{0}\\right)(2 \\pi)^ \\frac{p}{2}\\left|M_{0}\\right|^{\\frac{1}{2}}} \\left(b_0(1+\\frac{1}{2 b_0} Q(\\beta,m_0,M_0))\\right)^{-\\left(a_{0}+\\frac{p}{2}\\right)} \\\\ &amp; = \\frac{b_0^{a_0}\\Gamma\\left(a_{0}+\\frac{p}{2}\\right) b_0^{- \\left( a_0+\\frac{p}{2}\\right)}}{\\Gamma\\left(a_{0}\\right)(2 \\pi)^ \\frac{p}{2}\\left|M_{0}\\right|^{\\frac{1}{2}}} \\left(1+\\frac{1}{2 b_0} \\left(\\beta-m_{0}\\right)^{\\mathrm{\\scriptscriptstyle T}} M_{0}^{-1}\\left(\\beta-m_{0}\\right) \\right)^{-\\left(a_{0}+\\frac{p}{2}\\right)} \\\\ &amp; =\\frac{\\Gamma\\left(a_{0}+\\frac{p}{2}\\right)}{\\left(2 \\pi \\right)^{\\frac{p}{2}} b_{0}^{\\frac{p}{2}} \\Gamma\\left(a_{0}\\right)|M|^{\\frac{1}{2}}}\\left(1+\\frac{1}{2 b_{0}} \\left(\\beta-m_{0}\\right)^{\\mathrm{\\scriptscriptstyle T}} M_{0}^{-1}\\left(\\beta-m_{0}\\right) \\right)^{-\\left(a_{0}+\\frac{p}{2}\\right)} \\\\ &amp; =\\frac{\\Gamma\\left(a_{0}+\\frac{p}{2}\\right)} {\\left(2 \\pi \\right)^{\\frac{p}{2}}\\left(a_{0} \\cdot \\frac{b_{0}}{a_{0}}\\right)^{\\frac{p}{2}} \\Gamma\\left(a_{0}\\right)|M|^{\\frac{1}{2}}} \\left(1+\\frac{1}{2 a_{0} \\cdot \\frac{b_{0}}{a_{0}}} \\left(\\beta-m_{0}\\right)^{\\mathrm{\\scriptscriptstyle T}} M_{0}^{-1}\\left(\\beta-m_{0}\\right)\\right)^{-\\left(a_{0}+\\frac{p}{2}\\right)}\\\\ &amp; =\\frac{\\Gamma\\left(a_{0}+\\frac{p}{2}\\right)}{\\left(2 a_{0} \\pi\\right)^{\\frac{p}{2}} \\Gamma\\left(a_{0}\\right)\\left|\\frac{b_{0}}{a_{0}} M\\right|^{\\frac{1}{2}}}\\left(1+\\frac{1}{2 a_{0}} \\left(\\beta-m_{0}\\right)^{\\mathrm{\\scriptscriptstyle T}}\\left(\\frac{b_{0}}{a_{0}} M_{0}\\right)^{-1}\\left(\\beta-m_{0}\\right)\\right)^{-\\left(a_{0}+\\frac{p}{2}\\right)} \\\\ &amp; =t_{2a_0}\\left(m_0, \\frac{b_0}{a_0}M_0\\right) \\; \\end{align}\\] button#toggle-button2 { background-color: lightgray; color: black; border-radius: 5px; border: none; padding: 5px 10px; font-size: 16px; } button#toggle-button2:hover { background-color: gray; color: white; } Note: the density of multivariate t-distribution is given by \\[\\begin{equation*} \\begin{split} t_v(\\mu, \\Sigma)=\\frac{\\Gamma\\left(\\frac{v+p}{2}\\right)}{(v \\pi)^{\\frac{p}{2}} \\Gamma\\left(\\frac{v}{2}\\right) |\\Sigma|^{\\frac{1}{2}}}\\left(1+\\frac{1}{v}(x-\\mu)^{\\mathrm{\\scriptscriptstyle T}} \\Sigma^{-1}(x-\\mu)\\right)^{-\\frac{v+p}{2}} \\end{split} \\tag{1.6} \\end{equation*}\\] 1.3 Conjugate Bayesian linear regression and M&amp;m formula Let \\(y_{n \\times 1}\\) be outcome variable and \\(X_{n \\times p}\\) be corresponding covariates. Assume \\(V\\) is known. The model is given by \\[\\begin{equation*} \\begin{split} &amp; y=X \\beta+\\epsilon \\ , \\ \\epsilon \\sim N\\left(0, \\sigma^2 V\\right) \\\\ &amp; \\beta=m_0+\\omega \\ , \\ \\omega \\sim N\\left(0, \\sigma^2 M_0\\right) \\\\ &amp; \\sigma^2 \\sim I G\\left(a_0, b_0\\right) \\end{split} \\tag{1.7} \\end{equation*}\\] The posterior distribution of \\((\\beta, \\sigma^2)\\) is given by \\[\\begin{equation*} P\\left(\\beta, \\sigma^{2} \\mid y\\right) = NIG\\left(\\beta, \\sigma^{2} \\mid M_{1}m_{1}, M_{1}, a_{1}, b_{1}\\right) \\; \\tag{1.8} \\end{equation*}\\] where \\[\\begin{equation*} \\begin{split} M_{1}^{-1} &amp;= M_{0}^{-1}+X^{\\mathrm{\\scriptscriptstyle T}} V^{-1} X \\\\ m_{1}&amp;=M_{0}^{-1} m_{0}+X^{\\mathrm{\\scriptscriptstyle T}} V^{-1} y \\\\ a_{1}&amp;=a_{0}+\\frac{p}{2} \\\\ b_{1}&amp;=b_{0}+\\frac{c^{\\ast}}{2}= b_{0}+\\frac{1}{2}\\left(m_{0}^{\\mathrm{\\scriptscriptstyle T}} M_{0}^{-1} m_{0}+y^{\\mathrm{\\scriptscriptstyle T}} V^{-1} y-m_{1}^{\\mathrm{\\scriptscriptstyle T}} M_{1} m_{1}\\right) \\end{split} \\tag{1.9} \\end{equation*}\\] Show details \\[\\begin{align}\\label{eq:post_dist} P\\left(\\beta, \\sigma^{2} \\mid y\\right) &amp; \\propto NIG\\left(\\beta, \\sigma^{2} \\mid m_{0}, M_{0}, a_{0}, b_{0}\\right) \\cdot N\\left(y \\mid X \\beta, \\sigma^{2} V\\right) \\nonumber\\\\ &amp; \\propto IG\\left(\\sigma^{2} \\mid a_{0}, b_{0}\\right) \\cdot N\\left(\\beta \\mid m_{0}, \\sigma^{2} M_{0}\\right) \\cdot N\\left(y \\mid X \\beta, \\sigma^{2} V\\right) \\nonumber\\\\ &amp; \\propto \\frac{b_0^{a_0}}{\\Gamma\\left(a_{0}\\right)} \\left(\\frac{1}{\\sigma^{2}}\\right)^{a_{0}+1} e^{-\\frac{b_{0}}{\\sigma^{2}}} \\frac{1}{(2 \\pi \\sigma^{2})^{\\frac{p}{2}}\\left| M_{0}\\right|^{\\frac{1}{2}}} e^{-\\frac{1}{2 \\sigma^{2}} Q \\left(\\beta, m_{0}, M_{0}\\right)} \\frac{1}{(2 \\pi \\sigma^{2})^{\\frac{p}{2}}\\left| V\\right|^{\\frac{1}{2}}} e^{-\\frac{1}{2 \\sigma^{2}} Q \\left(y, X \\beta, V\\right)} \\nonumber\\\\ &amp; \\propto \\left(\\frac{1}{\\sigma^{2}}\\right)^{a_{0}+p+1} e^{-\\frac{b_{0}}{\\sigma^{2}}} e^{-\\frac{1}{2 \\sigma^{2}} \\left(Q \\left(\\beta, m_{0}, M_{0}\\right)+Q \\left(y, X \\beta, V\\right)\\right)}\\; \\end{align}\\] where \\[\\begin{align}\\label{eq:multivariate_completion_square} Q \\left(\\beta, m_{0}, M_{0}\\right)+Q \\left(y, X \\beta, V\\right) &amp;= (\\beta - m_{0})^{\\mathrm{\\scriptscriptstyle T}}M_{0}^{-1}(\\beta - m_{0}) + (y - X\\beta)^{\\mathrm{\\scriptscriptstyle T}}V^{-1}(y - X\\beta)\\; \\nonumber\\\\ &amp;= \\beta^{\\mathrm{\\scriptscriptstyle T}}M_{0}^{-1}\\beta - 2\\beta^{\\mathrm{\\scriptscriptstyle T}}M_{0}^{-1}m_{0} + m_{0}^{\\mathrm{\\scriptscriptstyle T}}M_{0}^{-1}m_{0} \\nonumber\\\\ &amp;\\qquad + \\beta^{\\mathrm{\\scriptscriptstyle T}}X^{\\mathrm{\\scriptscriptstyle T}}V^{-1}X\\beta - 2\\beta^{\\mathrm{\\scriptscriptstyle T}} X^{\\mathrm{\\scriptscriptstyle T}}V^{-1}y + y^{\\mathrm{\\scriptscriptstyle T}}V^{-1}y \\nonumber\\\\ &amp;= \\beta^{\\mathrm{\\scriptscriptstyle T}} \\left(M_{0}^{-1} + X^{\\mathrm{\\scriptscriptstyle T}}V^{-1}X\\right) \\beta - 2\\beta^{\\mathrm{\\scriptscriptstyle T}}\\left(M_{0}^{-1}m_{0} + X^{\\mathrm{\\scriptscriptstyle T}}V^{-1}y\\right) \\nonumber\\\\ &amp;\\qquad + m_{0}^{\\mathrm{\\scriptscriptstyle T}} M_{0}^{-1}m_{0} + y^{\\mathrm{\\scriptscriptstyle T}}V^{-1}y \\nonumber \\\\ &amp;= \\beta^{\\mathrm{\\scriptscriptstyle T}}M_{1}^{-1}\\beta - 2\\beta^{\\mathrm{\\scriptscriptstyle T}} m_{1} + c\\nonumber\\\\ &amp;= (\\beta - M_{1}m_{1})^{\\mathrm{\\scriptscriptstyle T}}M_{1}^{-1}(\\beta - M_{1}m_{1}) - m_{1}^{\\mathrm{\\scriptscriptstyle T}}M_{1}m_{1} +c \\nonumber\\\\ &amp;= (\\beta - M_{1}m_{1})^{\\mathrm{\\scriptscriptstyle T}}M_{1}^{-1}(\\beta - M_{1}m_{1}) +c^{\\ast}\\; \\end{align}\\] where \\(M_{1}\\) is a symmetric positive definite matrix, \\(m_{1}\\) is a vector, and \\(c\\) &amp; \\(c^{\\ast}\\) are scalars given by \\[\\begin{align} M_{1}^{-1} &amp;= M_{0}^{-1} + X^{\\mathrm{\\scriptscriptstyle T}}V^{-1}X \\\\ m_{1} &amp;= M_{0}^{-1}m_{0} + X^{\\mathrm{\\scriptscriptstyle T}}V^{-1}y \\\\ c &amp;= m_{0}^{\\mathrm{\\scriptscriptstyle T}} M_{0}^{-1}m_{0} + y^{\\mathrm{\\scriptscriptstyle T}}V^{-1}y \\\\ c^{\\ast} &amp;= c - m^{\\mathrm{\\scriptscriptstyle T}}Mm = m_{0}^{\\mathrm{\\scriptscriptstyle T}} M_{0}^{-1}m_{0} + y^{\\mathrm{\\scriptscriptstyle T}}V^{-1}y - m_{1}^{\\mathrm{\\scriptscriptstyle T}}M_{1}m_{1} \\end{align}\\] Note: \\(M_{1}\\), \\(m_{1}\\) and \\(c\\) do not depend upon \\(\\beta\\). Then, we have \\[\\begin{align} P\\left(\\beta, \\sigma^{2} \\mid y\\right) &amp; \\propto \\left(\\frac{1}{\\sigma^{2}}\\right)^{a_{0}+p+1} e^{-\\frac{b_{0}}{\\sigma^{2}}} e^{-\\frac{1}{2 \\sigma^{2}} ((\\beta - M_{1}m_{1})^{\\mathrm{\\scriptscriptstyle T}}M_{1}^{-1}(\\beta - M_{1}m_{1}) +c^{\\ast})}\\\\ &amp; \\propto \\left(\\frac{1}{\\sigma^{2}}\\right)^{a_{0}+p+1} e^{-\\frac{b_{0}+\\frac{c^{\\ast}}{2}}{\\sigma^{2}}} e^{-\\frac{1}{2 \\sigma^{2}} (\\beta - M_{1}m_{1})^{\\mathrm{\\scriptscriptstyle T}}M_{1}^{-1}(\\beta - M_{1}m_{1})}\\\\ &amp; \\propto \\left(\\frac{1}{\\sigma^{2}}\\right)^{a_{0}+\\frac{p}{2}+1} e^{-\\frac{b_{0}+\\frac{c^{\\ast}}{2}}{\\sigma^{2}}} (\\frac{1}{\\sigma^2})^{\\frac{p}{2}} e^{-\\frac{1}{2 \\sigma^{2}} (\\beta - M_{1}m_{1})^{\\mathrm{\\scriptscriptstyle T}}M_{1}^{-1}(\\beta - M_{1}m_{1})}\\\\ &amp; \\propto IG\\left(\\sigma^{2} \\mid a_{0}+\\frac{p}{2}, b_{0}+\\frac{c^{\\ast}}{2} \\right) \\cdot N\\left(\\beta \\mid M_{1}m_{1}, \\sigma^{2} M_{1}\\right) \\\\ &amp; \\propto IG\\left(\\sigma^{2} \\mid a_{1}, b_{1} \\right) \\cdot N\\left(\\beta \\mid M_{1}m_{1}, \\sigma^{2} M_{1}\\right) \\\\ &amp; \\propto NIG\\left(\\beta, \\sigma^{2} \\mid M_{1}m_{1}, M_{1}, a_{1}, b_{1}\\right) \\; \\end{align}\\] where \\[\\begin{align} M_{1}^{-1} &amp;= M_{0}^{-1}+X^{\\mathrm{\\scriptscriptstyle T}} V^{-1} X \\\\ m_{1}&amp;=M_{0}^{-1} m_{0}+X^{\\mathrm{\\scriptscriptstyle T}} V^{-1} y \\\\ a_{1}&amp;=a_{0}+\\frac{p}{2} \\\\ b_{1}&amp;=b_{0}+\\frac{c^{\\ast}}{2}= b_{0}+\\frac{1}{2}\\left(m_{0}^{\\mathrm{\\scriptscriptstyle T}} M_{0}^{-1} m_{0}+y^{\\mathrm{\\scriptscriptstyle T}} V^{-1} y-m_{1}^{\\mathrm{\\scriptscriptstyle T}} M_{1} m_{1}\\right) \\end{align}\\] button#toggle-button3 { background-color: lightgray; color: black; border-radius: 5px; border: none; padding: 5px 10px; font-size: 16px; } button#toggle-button3:hover { background-color: gray; color: white; } From derivation in marginal priors, the marginal posterior distributions can be easily get by updating corresponding parameters \\[\\begin{equation*} \\begin{split} &amp;P\\left(\\sigma^{2} \\mid y\\right)=I G\\left(\\sigma^{2} \\mid a_{1}, b_{1}\\right) \\\\ &amp;P(\\beta \\mid y)=t_{2a_1}\\left(M_1m_1, \\frac{b_1}{a_1}M_1\\right) \\end{split} \\tag{1.10} \\end{equation*}\\] 1.4 Updating form of the posterior distribution We will use two ways to derive the updating form of the posterior distribution. 1.4.1 Method 1: Sherman-Woodbury-Morrison identity Theorem 1.2 (Sherman-Woodbury-Morrison identity) We have \\[\\begin{equation}\\label{ShermanWoodburyMorrison} \\left(A + BDC\\right)^{-1} = A^{-1} - A^{-1}B\\left(D^{-1}+CA^{-1}B\\right)^{-1}CA^{-1} \\end{equation}\\] where \\(A\\) and \\(D\\) are square matrices that are invertible and \\(B\\) and \\(C\\) are rectangular (square if \\(A\\) and \\(D\\) have the same dimensions) matrices such that the multiplications are well-defined. Sherman-Woodbury-Morrison identity is easily verified by multiplying the right hand side with \\(A + BDC\\) and simplifying to reduce it to the identity matrix. Using this formula, we have \\[\\begin{equation*} \\begin{split} M_1 &amp; = (M_{0}^{-1} + X^{\\mathrm{\\scriptscriptstyle T}}V^{-1}X)^{-1} \\\\ &amp; = M_0-M_0 X^{\\mathrm{\\scriptscriptstyle T}}\\left(V+X M_0 X^{\\mathrm{\\scriptscriptstyle T}}\\right)^{-1} X M_0 \\\\ &amp; = M_0-M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1} X M_0 \\end{split} \\tag{1.11} \\end{equation*}\\] where \\(Q = V + X M_0 X^{\\mathrm{\\scriptscriptstyle T}}\\) We can show that \\[\\begin{equation*} \\begin{split} M_1 m_1 &amp;= m_0 + M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-X m_0\\right)\\\\ \\end{split} \\tag{1.12} \\end{equation*}\\] Show details \\[\\begin{align} M_1 m_1 &amp; = \\left(M_0^{-1}+X^{\\mathrm{\\scriptscriptstyle T}} V^{-1} X\\right)^{-1} m_1 \\\\ &amp; = \\left(M_0-M_0 X^{\\mathrm{\\scriptscriptstyle T}}\\left(V+X M_0 X^{\\mathrm{\\scriptscriptstyle T}}\\right)^{-1} X M_0\\right)m_1 \\\\ &amp; = \\left(M_0-M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1} X M_0\\right) m_1 \\\\ &amp; = \\left(M_0-M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1} X M_0\\right)\\left(M_0^{-1} m_0+X^{\\mathrm{\\scriptscriptstyle T}} V^{-1} y\\right) \\\\ &amp; = m_0+M_0 X^{\\mathrm{\\scriptscriptstyle T}} V^{-1} y-M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1} X m_0 - M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1} X M_0 X^{\\mathrm{\\scriptscriptstyle T}} V^{-1} y \\\\ &amp; = m_0+M_0 X^{\\mathrm{\\scriptscriptstyle T}}\\left(I-Q^{-1} X M_0 X^{\\mathrm{\\scriptscriptstyle T}}\\right) V^{-1} y - M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1} X m_0 \\\\ &amp; = m_0+M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(Q-X M_0 X^{\\mathrm{\\scriptscriptstyle T}}\\right)V^{-1} y - M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1} X m_0 \\\\ &amp; \\left(\\text { since } Q=V+X M_0 X^{\\mathrm{\\scriptscriptstyle T}}\\right) \\\\ &amp; = m_0+M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}V V^{-1} y-M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1} X m_0 \\\\ &amp; = m_0+M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1} y-M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1} X m_0 \\\\ &amp; = m_0+M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-X m_0\\right) \\\\ \\end{align}\\] button#toggle-button4 { background-color: lightgray; color: black; border-radius: 5px; border: none; padding: 5px 10px; font-size: 16px; } button#toggle-button4:hover { background-color: gray; color: white; } Furthermore, we can simplify that \\[\\begin{equation*} \\begin{split} m_0^{\\mathrm{\\scriptscriptstyle T}} M_0^{-1} m_0+y^{\\mathrm{\\scriptscriptstyle T}} V^{-1} y-m_1^{\\mathrm{\\scriptscriptstyle T}} M_1 m_1 &amp; = \\left(y-X m_0\\right)^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-X m_0\\right)\\\\ \\end{split} \\tag{1.13} \\end{equation*}\\] Show details \\[\\begin{align} &amp; \\quad \\ m_0^{\\mathrm{\\scriptscriptstyle T}} M_0^{-1} m_0+y^{\\mathrm{\\scriptscriptstyle T}} V^{-1} y-m_1^{\\mathrm{\\scriptscriptstyle T}} M_1 m_1 \\\\ &amp; = m_0^{\\mathrm{\\scriptscriptstyle T}} M_0^{-1} m_0+y^{\\mathrm{\\scriptscriptstyle T}} V^{-1} y-m_1^{\\mathrm{\\scriptscriptstyle T}} \\left(m_0+M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1} (y - X m_0)\\right) \\\\ &amp; = m_0^{\\mathrm{\\scriptscriptstyle T}} M_0^{-1} m_0+y^{\\mathrm{\\scriptscriptstyle T}} V^{-1} y-m_1^{\\mathrm{\\scriptscriptstyle T}} m_0 - m_1^{\\mathrm{\\scriptscriptstyle T}} M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-X m_0\\right) \\\\ &amp; = m_0^{\\mathrm{\\scriptscriptstyle T}} M_0^{-1} m_0+y^{\\mathrm{\\scriptscriptstyle T}} V^{-1} y -m_0^{\\mathrm{\\scriptscriptstyle T}}\\left(M_0^{-1} m_0+X^{\\mathrm{\\scriptscriptstyle T}} V^{-1} y\\right) \\\\ &amp; \\qquad \\qquad \\qquad - m_1^{\\mathrm{\\scriptscriptstyle T}} M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-X m_0\\right) \\\\ &amp; = y^{\\mathrm{\\scriptscriptstyle T}} V^{-1} y-y^{\\mathrm{\\scriptscriptstyle T}} V^{-1} X m_0 - m_1^{\\mathrm{\\scriptscriptstyle T}} M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-X m_0\\right) \\\\ &amp; = y^{\\mathrm{\\scriptscriptstyle T}} V^{-1}\\left(y-X m_0 \\right)-m_1^{\\mathrm{\\scriptscriptstyle T}} M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-X m_0\\right) \\\\ &amp; =y^{\\mathrm{\\scriptscriptstyle T}} V^{-1}\\left(y-X m_0\\right)-\\underbrace{m_1^{\\mathrm{\\scriptscriptstyle T}} M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-X m_0\\right)}_{\\substack{\\text { simplify from left to right }}} \\\\ &amp; =y^{\\mathrm{\\scriptscriptstyle T}} V^{-1}\\left(y-X m_0\\right)-\\left(M_0 m_1\\right)^{\\mathrm{\\scriptscriptstyle T}} X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-X m_0\\right) \\\\ &amp; =y^{\\mathrm{\\scriptscriptstyle T}} V^{-1}\\left(y-X m_0\\right)-\\left(m_0+M_0 X^{\\mathrm{\\scriptscriptstyle T}} V^{-1} y\\right)^{\\mathrm{\\scriptscriptstyle T}} X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-m_0\\right) \\\\ &amp; =y^{\\mathrm{\\scriptscriptstyle T}} V^{-1}\\left(y-X m_0\\right)-\\left(X m_0+X M_0 X^{\\mathrm{\\scriptscriptstyle T}} V^{-1} y\\right)^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-X m_0\\right)\\\\ &amp; =y^{\\mathrm{\\scriptscriptstyle T}} V^{-1}\\left(y-X m_0\\right)-\\left(Q^{-1} X m_0+Q^{-1}\\left(X M_0 X^{\\mathrm{\\scriptscriptstyle T}}\\right)V^{-1} y\\right)\\left(y-X m_0\\right) \\\\ &amp; =y^{\\mathrm{\\scriptscriptstyle T}} V^{-1}\\left(y-X m_0\\right)-\\left(Q^{-1} X m_0+Q^{-1}(Q-V) V^{-1} y\\right)^{\\mathrm{\\scriptscriptstyle T}}(y-X m_0) \\\\ &amp; =y^{\\mathrm{\\scriptscriptstyle T}} V^{-1}\\left(y-X m_0\\right)-\\left(Q^{-1} X m_0+V^{-1} y- Q^{-1} y \\right)^{\\mathrm{\\scriptscriptstyle T}}\\left(y-X m_0\\right) \\\\ &amp; =y^{\\mathrm{\\scriptscriptstyle T}} V^{-1}\\left(y-X m_0\\right)-\\left(V^{-1} y+Q^{-1}\\left(X m_0-y\\right)\\right)^{\\mathrm{\\scriptscriptstyle T}}\\left(y-X m_0\\right) \\\\ &amp; =y^{\\mathrm{\\scriptscriptstyle T}} V^{-1}\\left(y-X m_0\\right)-y^{\\mathrm{\\scriptscriptstyle T}} V^{-1}\\left(y-X m_0\\right) +\\left(y-X m_0\\right)^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-X m_0\\right) \\\\ &amp; =\\left(y-X m_0\\right)^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-X m_0\\right) \\\\ \\end{align}\\] button#toggle-button5 { background-color: lightgray; color: black; border-radius: 5px; border: none; padding: 5px 10px; font-size: 16px; } button#toggle-button5:hover { background-color: gray; color: white; } So, we get the following updating form of the posterior distribution from Bayesian linear regression \\[\\begin{equation*} P\\left(\\beta, \\sigma^{2} \\mid y\\right) = NIG\\left(\\beta, \\sigma^{2} \\mid \\tilde{m}_1, \\tilde{M}_1, a_{1}, b_{1}\\right) \\\\ \\tag{1.14} \\end{equation*}\\] where \\[\\begin{equation*} \\begin{split} \\tilde{m}_1 &amp; =M_1 m_1 =m_0+M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-X m_0\\right)\\\\ \\tilde{M}_1 &amp; =M_1 =M_0-M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1} X M_0\\\\ a_1 &amp; =a_0+\\frac{p}{2} \\\\ b_1 &amp; =b_0+\\frac{1}{2}\\left(y-X m_0\\right)^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-X m_0\\right) \\\\ Q &amp; =V+X M_0 X^{\\mathrm{\\scriptscriptstyle T}} \\end{split} \\tag{1.15} \\end{equation*}\\] 1.4.2 Method 2: Distribution theory Previously, we got the Bayesian Linear Regression Updater using Sherman-Woodbury-Morrison identity. Here, we will derive the results without resorting to it. Recall that the model is given by \\[\\begin{equation*} \\begin{split} &amp; y=X \\beta+\\epsilon \\ , \\ \\epsilon \\sim N\\left(0, \\sigma^2 V\\right) \\\\ &amp; \\beta=m_0+\\omega \\ , \\ \\omega \\sim N\\left(0, \\sigma^2 M_0\\right) \\\\ &amp; \\sigma^2 \\sim I G\\left(a_0, b_0\\right)\\\\ \\end{split} \\tag{1.16} \\end{equation*}\\] This corresponds to the posterior distribution \\[\\begin{equation*} \\begin{split} P\\left(\\beta, \\sigma^2 \\mid y\\right) \\propto I G\\left(\\sigma^2 \\mid a_0, b_0\\right) &amp; \\times N\\left(\\beta \\mid m_0, \\sigma^2 M_0\\right) \\times N\\left(y \\mid X \\beta, \\sigma^2 V\\right)\\\\ \\end{split} \\tag{1.17} \\end{equation*}\\] We will derive \\(P\\left(\\sigma^2 \\mid y\\right)\\) and \\(P\\left(\\beta \\mid \\sigma^2, y\\right)\\) in a form that will reflect updates from the prior to the posterior. Integrating out \\(\\beta\\) from the model is equivalent to substituting \\(\\beta\\) from its prior model. Thus, \\(P\\left(y \\mid \\sigma^2\\right)\\) is derived simply from \\(y =X \\beta+\\epsilon =X\\left(m_0+\\omega\\right)+\\epsilon =X m_0 + X \\omega + \\epsilon =X m_0+ \\eta\\) where \\(\\eta = X \\omega + \\epsilon \\sim N\\left(0, \\sigma^2Q\\right)\\) and \\(Q=X M_0 X^{\\mathrm{\\scriptscriptstyle T}}+V \\, .\\) Therefore, \\[\\begin{equation*} y \\mid \\sigma^2 \\sim N\\left(X m_0, \\sigma^2 Q\\right)\\\\ \\tag{1.18} \\end{equation*}\\] The posterior distribution is given by \\[\\begin{equation*} \\begin{split} P\\left(\\sigma^2 \\mid y\\right) &amp; \\propto IG \\left(\\sigma^2 \\mid a_1, b_1\\right)\\\\ \\end{split} \\tag{1.19} \\end{equation*}\\] where \\[\\begin{equation*} \\begin{split} &amp; a_1 = a_0 + \\frac{p}{2} \\\\ &amp; b_1 = b_0 + \\frac{1}{2} (y-Xm_0)^{\\mathrm{\\scriptscriptstyle T}} Q^{-1} \\left(y-Xm_0\\right)\\\\ \\end{split} \\tag{1.20} \\end{equation*}\\] Show details \\[\\begin{align} P\\left(\\sigma^2 \\mid y\\right) &amp; \\propto P\\left(\\sigma^2\\right) P\\left(y \\mid \\sigma^2\\right) \\\\ &amp; \\propto\\left(\\sigma^2 \\mid a_0, b_0\\right) \\times N\\left(y \\mid X m_0, \\sigma^2 Q\\right) \\\\ &amp; \\propto\\left(\\frac{1}{\\sigma^2}\\right)^{a_0+1} e^{-\\frac{b_0} {\\sigma^2} \\times\\left(\\frac{1}{\\sigma^2}\\right)^{\\frac{n}{2}} e^{-\\frac{1}{2 \\sigma^2}}\\left(y-Xm_0\\right)^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-Xm_0\\right)} \\\\ &amp; \\propto\\left(\\frac{1}{\\sigma^2}\\right)^{a_0+\\frac{p}{2}+1} e^{-\\frac{1}{\\sigma^2}\\left(b_0+\\frac{1}{2}\\left(y-Xm_0\\right)^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-Xm_0\\right)\\right)} \\\\ &amp; \\propto IG \\left(\\sigma^2 \\mid a_1, b_1\\right) \\end{align}\\] where \\[\\begin{align} &amp; a_1 = a_0 + \\frac{p}{2} \\\\ &amp; b_1 = b_0 + \\frac{1}{2} (y-Xm_0)^{\\mathrm{\\scriptscriptstyle T}} Q^{-1} \\left(y-Xm_0\\right) \\end{align}\\] button#toggle-button6 { background-color: lightgray; color: black; border-radius: 5px; border: none; padding: 5px 10px; font-size: 16px; } button#toggle-button6:hover { background-color: gray; color: white; } Next, we turn to \\(P\\left(\\beta \\mid \\sigma^2, y\\right)\\). Note that \\[\\begin{equation*} \\begin{split} \\left(\\begin{array}{l} y \\\\ \\beta \\end{array}\\right) \\mid \\sigma^2 \\sim N\\left(\\left(\\begin{array}{l} Xm_0 \\\\ m_0 \\end{array}\\right), \\quad \\sigma^2 \\left(\\begin{array}{cc} Q &amp; X M_0 \\\\ M_0 X^{\\mathrm{\\scriptscriptstyle T}} &amp; M_0 \\end{array}\\right)\\right) \\; \\end{split} \\tag{1.21} \\end{equation*}\\] Click to show or hide details We have used the facts \\[ \\begin{aligned} &amp; \\operatorname{E}\\left[y \\mid \\sigma^2\\right] = Xm_0 \\, , \\ \\operatorname{Var}\\left(y \\mid \\sigma^2\\right)=\\sigma^2 Q \\, ; \\\\ &amp; \\operatorname{E}\\left[\\beta \\mid \\sigma^2\\right] = m_0 \\, , \\ \\operatorname{Var}\\left(\\beta \\mid \\sigma^2\\right)=\\sigma^2 M_0 \\, ; \\end{aligned} \\] \\[ \\begin{aligned} \\operatorname{Cov}\\left(y, \\beta \\mid \\sigma^2\\right) &amp;= \\operatorname{Cov}\\left(X \\beta+\\epsilon, \\beta \\mid \\sigma^2\\right) \\\\ &amp; =\\operatorname{Cov}\\left(X\\left(m_0+\\omega\\right)+\\epsilon, m_0+\\omega \\mid \\sigma^2\\right) \\\\ &amp; \\quad \\left( \\text {Since } m_0 \\text { is constant and } \\operatorname{Cov}(\\omega, \\epsilon)=0 \\right) \\\\ &amp; =\\operatorname{Cov}\\left(X \\omega, \\omega \\mid \\sigma^2\\right) \\\\ &amp; =\\sigma^2 X M_0 \\end{aligned} \\] button#toggle-button7 { background-color: lightgray; color: black; border-radius: 5px; border: none; padding: 5px 10px; font-size: 16px; } button#toggle-button7:hover { background-color: gray; color: white; } From the expression of a conditional distribution derived from a multivariate Gaussian, we obtain \\[\\begin{equation*} \\begin{split} \\beta \\mid \\sigma^2, y \\sim N\\left(\\tilde{m}_1, \\sigma^2 \\tilde{M}_1\\right) \\end{split} \\tag{1.22} \\end{equation*}\\] where \\[\\begin{equation*} \\begin{split} &amp; \\tilde{m}_1=\\operatorname{E}\\left[\\beta \\mid \\sigma^2, y\\right]=m_0+M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-X{m_0}\\right) \\\\ &amp; \\tilde{M}_1=M_0-M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1} X M_0 \\\\ \\end{split} \\tag{1.23} \\end{equation*}\\] Show details Note: \\[\\begin{align} &amp; \\left(\\begin{array}{l} X_1 \\\\ X_2 \\end{array}\\right) \\sim N\\left(\\left(\\begin{array}{l} \\mu_1 \\\\ \\mu_2 \\end{array}\\right),\\left(\\begin{array}{ll} \\Sigma_{11} &amp; \\Sigma_{12} \\\\ \\Sigma_{21} &amp; \\Sigma_{22} \\end{array}\\right)\\right) \\text { with } \\Sigma_{21} = \\Sigma_{12}^{\\mathrm{\\scriptscriptstyle T}} \\end{align}\\] \\[\\begin{align} &amp; \\Rightarrow X_2 \\mid X_1 \\sim N\\left(\\mu_{2 \\cdot 1}, \\Sigma_{2 \\cdot 1}\\right) \\end{align}\\] where \\(\\mu_{2 \\cdot 1}= \\mu_2+\\Sigma_{21} \\Sigma_{11}^{-1}\\left(X_1-\\mu_1\\right) \\text { and } \\Sigma_{2 \\cdot 1}=\\Sigma_{22}-\\Sigma_{21} \\Sigma_{11}^{-1} \\Sigma_{12} \\, .\\) button#toggle-button8 { background-color: lightgray; color: black; border-radius: 5px; border: none; padding: 5px 10px; font-size: 16px; } button#toggle-button8:hover { background-color: gray; color: white; } 1.5 Bayesian prediction Assume \\(V=I_{n}\\). Let \\(\\tilde{y}\\) denote an \\(\\tilde{n}\\times 1\\) vector of outcomes. \\(\\tilde{X}\\) is corresponding predictors. We seek to predict \\(\\tilde{y}\\) based upon \\(y\\) \\[\\begin{equation*} \\begin{split} P(\\tilde{y} \\mid y) &amp;= t_{2a_1}\\left(\\tilde{X} M_1 m_1, \\frac{b_1}{a_1}\\left(I_{\\tilde{n}} + \\tilde{X} M_{1} \\tilde{X}^{\\mathrm{\\scriptscriptstyle T}}\\right)\\right) \\; \\end{split} \\tag{1.24} \\end{equation*}\\] Show details \\[\\begin{align} P\\left(\\beta, \\sigma^{2}, \\tilde{y} \\mid y\\right) &amp;=P\\left(\\beta, \\sigma^{2} \\mid y\\right) \\cdot P\\left(\\tilde{y} \\mid \\beta, \\sigma^{2}, y\\right) \\\\ &amp;= P\\left(\\beta, \\sigma^{2}\\right) \\cdot P\\left(y \\mid \\beta, \\sigma^{2}\\right) \\cdot P\\left(\\tilde{y} \\mid \\beta, \\sigma^{2}, y\\right) \\\\ &amp;= NIG \\left(\\beta, \\sigma^{2} \\mid m_{0}, M_{0}, a_{0}, b_{0}\\right) \\cdot N\\left(y \\mid X \\beta, \\sigma^{2} I_{n}\\right) \\cdot N\\left(\\tilde{y} \\mid \\tilde{X} \\beta, \\sigma^{2} I_{\\tilde{n}}\\right) \\\\ &amp;= NIG \\left(\\beta, \\sigma^{2} \\mid M_{1} m_{1}, M_{1}, a_{1}, b_{1}\\right) \\cdot N\\left(\\tilde{y} \\mid \\tilde{X} \\beta, \\sigma^{2} I_{\\tilde{n}}\\right) \\\\ &amp;= IG\\left(\\sigma^{2} \\mid a_{1}, b_{1}\\right) \\cdot N\\left(\\beta \\mid M_{1} m_{1}, \\sigma^{2} M_{1} \\right) \\cdot N\\left(\\tilde{y} \\mid \\tilde{X} \\beta, \\sigma^{2} I_{\\tilde{n}} \\right) \\; \\end{align}\\] Then we can calculate posterior predictive density \\(P\\left(\\tilde{y} \\mid y\\right)\\) from \\(P\\left(\\beta, \\sigma^{2}, \\tilde{y} \\mid y\\right)\\) \\[\\begin{align} P\\left(\\tilde{y} \\mid y\\right) &amp;=\\iint P\\left(\\beta, \\sigma^{2}, \\tilde{y} \\mid y\\right) \\ d\\beta \\ d\\sigma^{2} \\\\ &amp;=\\iint IG\\left(\\sigma^{2} \\mid a_{1}, b_{1}\\right) \\cdot N\\left(\\beta \\mid M_{1} m_{1}, \\sigma^{2} M_{1} \\right) \\cdot N\\left(\\tilde{y} \\mid \\tilde{X} \\beta, \\sigma^{2} I_{\\tilde{n}}\\right) \\ d\\beta \\ d\\sigma^{2} \\\\ &amp;=\\int IG\\left(\\sigma^{2} \\mid a_{1}, b_{1}\\right) \\int N\\left(\\beta \\mid M_{1} m_{1}, \\sigma^{2} M_{1} \\right) \\cdot N\\left(\\tilde{y} \\mid \\tilde{X} \\beta, \\sigma^{2} I_{\\tilde{n}}\\right) \\ d\\beta \\ d\\sigma^{2} \\\\ \\end{align}\\] As for \\(\\int N\\left(\\beta \\mid M_{1} m_{1}, \\sigma^{2} M_{1}\\right) \\cdot N\\left(\\tilde{y} \\mid \\tilde{X} \\beta, \\sigma^{2} I_{\\tilde{n}}\\right) \\ d\\beta\\), we provide an easy way to derive it avoiding any integration at all. Note that we can write the above model as \\[\\begin{align} \\tilde{y} &amp;= \\tilde{X} \\beta + \\tilde{\\epsilon}, \\text{ where } \\tilde{\\epsilon} \\sim N\\left(0,\\sigma^2 I_{\\tilde{n}}\\right) \\\\ \\beta &amp;= M_{1} m_{1} + \\epsilon_{\\beta \\mid y}, \\text{ where } \\epsilon_{\\beta \\mid y} \\sim N\\left(0,\\sigma^2M_{1}\\right) \\end{align}\\] where \\(\\tilde{\\epsilon}\\) and \\(\\epsilon_{\\beta \\mid y}\\) are independent of each other. It then follows that \\[\\begin{align} \\tilde{y} &amp;= \\tilde{X} M_{1} m_{1} + \\tilde{X} \\epsilon_{\\beta \\mid y} + \\tilde{\\epsilon} \\sim N\\left(\\tilde{X} M_{1} m_{1}, \\sigma^2\\left(I_{\\tilde{n}} + \\tilde{X} M_{1} \\tilde{X}^{\\mathrm{\\scriptscriptstyle T}}\\right)\\right) \\end{align}\\] As a result \\[\\begin{align} P\\left(\\tilde{y} \\mid y\\right) &amp;=\\int IG\\left(\\sigma^{2} \\mid a_{1}, b_{1}\\right) \\cdot N\\left(\\tilde{X} M_{1} m_{1}, \\sigma^2\\left(I_{\\tilde{n}} + \\tilde{X} M_{1} \\tilde{X}^{\\mathrm{\\scriptscriptstyle T}}\\right)\\right) \\ d\\sigma^{2} \\\\ &amp;= t_{2a_1}\\left(\\tilde{X} M_1 m_1, \\frac{b_1}{a_1}\\left(I_{\\tilde{n}} + \\tilde{X} M_{1} \\tilde{X}^{\\mathrm{\\scriptscriptstyle T}}\\right)\\right) \\; \\end{align}\\] button#toggle-button9 { background-color: lightgray; color: black; border-radius: 5px; border: none; padding: 5px 10px; font-size: 16px; } button#toggle-button9:hover { background-color: gray; color: white; } 1.6 Sampling from the posterior distribution We can get the joint posterior density \\(P\\left(\\beta, \\sigma^{2}, \\tilde{y} \\mid y\\right)\\) by sampling process Draw \\(\\hat{\\sigma}_{(i)}^{2}\\) from \\(I G\\left(a_{1}, b_{1}\\right)\\) Draw \\(\\hat{\\beta}_{(i)}\\) from \\(N\\left(M_{1} m_{1}, \\hat{\\sigma}_{(i)}^{2} M_{1}\\right)\\) Draw \\(\\tilde{y}_{(i)}\\) from \\(N\\left(\\tilde{X} \\hat{\\beta}_{(i)}, \\hat{\\sigma}_{(i)}^{2}I_{\\tilde{n}}\\right)\\) 1.7 Sample code if(TRUE){ library(pacman) options(tidyverse.quiet = TRUE) p_load(here, mvnfast, invgamma) p_load(tidyverse) theme_set(theme_minimal(base_size = 22)) #print(here()) } # set up parameters and data set.seed(100) n &lt;- 20 # number of observations p &lt;- 3 # dimention of covariates X &lt;- c(rep(1, n), seq(1:n), seq(1:n)^2/n) X &lt;- matrix(X, nrow = n, ncol = p) # parameters for sigma2 a0 &lt;- 1 b0 &lt;- 2 sigma2 &lt;- rinvgamma(n = 1, shape = a0, rate = b0) # generate random error V &lt;- diag(1, nrow = n, ncol = n) e &lt;- as.vector(rmvn(n = 1, mu = rep(0, n), sigma = sigma2 * V) / 100) # parameters for beta m0 &lt;- rnorm(p) M0 &lt;- diag(1, nrow = p, ncol = p) w &lt;- rmvn(n = 1, mu = rep(0, p), sigma = sigma2 * M0) / 100 beta &lt;- m0 + w %&gt;% as.vector() y &lt;- X %*% beta + e # function for calculating posterior distribution update_post &lt;- function(X, y, m0, M0, a0, b0){ # calculate intermediate values p &lt;- length(m0) XM0 &lt;- X %*% M0 y_Xm0 &lt;- y - X %*% m0 Q &lt;- V + XM0 %*% t(X) Qinv &lt;- solve(Q) M0XtQinv &lt;- t(XM0) %*% Qinv # update parameters m1 &lt;- m0 + M0XtQinv %*% y_Xm0 M1 &lt;- M0 - M0XtQinv %*% XM0 a1 &lt;- a0 + p / 2 b1 &lt;- b0 + 1/2 * t(y_Xm0) %*% Qinv %*% y_Xm0 res &lt;- list(m1, M1, a1, b1) return(res) } This is the initial values of parameters list(m0, M0, a0, b0) ## [[1]] ## [1] -0.07891709 0.88678481 0.11697127 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 1 0 ## [3,] 0 0 1 ## ## [[3]] ## [1] 1 ## ## [[4]] ## [1] 2 Here is the updated parameters of posterior distribution update_post(X, y, m0, M0, a0, b0) ## [[1]] ## [,1] ## [1,] -0.09067348 ## [2,] 0.87820336 ## [3,] 0.12281565 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] 0.34856278 -0.06719858 0.05429884 ## [2,] -0.06719858 0.01846990 -0.01719519 ## [3,] 0.05429884 -0.01719519 0.01721752 ## ## [[3]] ## [1] 2.5 ## ## [[4]] ## [,1] ## [1,] 2.010475 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
