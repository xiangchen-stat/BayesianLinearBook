[["index.html", "Bayesian Linear Regression Tutorial Preface Acknowledgements", " Bayesian Linear Regression Tutorial Xiang Chen, Valentina Arputhasamy, Daniel Zhou, Sudipto Banerjee 2023-03-01 Preface This is a tutorial for Bayesian Linear Regression assembled in book form. Acknowledgements "],["basics-of-bayesian-linear-regression.html", "Chapter 1 Basics of Bayesian linear regression 1.1 Bayes’ theorem 1.2 Normal-Inverse-Gamma (NIG) prior 1.3 Conjugate Bayesian linear regression and M&amp;m formula 1.4 Updating form of the posterior distribution 1.5 Bayesian prediction 1.6 Sampling from the posterior distribution", " Chapter 1 Basics of Bayesian linear regression 1.1 Bayes’ theorem Theorem 1.1 (Bayes' theorem) For events \\(U, K\\) and \\(P(K) \\neq 0\\), we have \\[P(U \\mid K) = \\frac{P(K \\mid U) P(U)}{P(K)}\\] We denote \\(U\\) as unknown parameters and \\(K\\) as known parameters. We call \\(P(U)\\) prior and \\(P(K|U)\\) likelihood. The Bayes’ theorem gives us the posterior distribution of unknown parameters given the known parameters \\[ P(U \\mid K) \\propto P(U) \\cdot P(K \\mid U)\\] 1.2 Normal-Inverse-Gamma (NIG) prior 1.2.1 Joint distribution of NIG prior Definition 1.1 (Normal-Inverse-Gamma Distribution) Suppose \\[ \\begin{aligned} &amp; \\beta \\mid \\sigma^2, \\mu, M \\sim N(\\mu,\\sigma^2 M) \\\\ &amp; \\sigma^2 \\mid a, b \\sim IG(a, b) \\end{aligned} \\] Then \\((\\beta,\\sigma^2)\\) has a Normal-Inverse-Gamma distribution, denoted as \\((\\beta,\\sigma^2) \\sim NIG(\\mu,M,a,b)\\). We use a Normal-Inverse-Gamma prior for \\((\\beta, \\sigma^2)\\) \\[\\begin{align} P(\\beta, \\sigma^{2}) &amp;= NIG \\left(\\beta, \\sigma^{2} \\mid m_{0}, M_{0}, a_{0}, b_{0}\\right) \\\\ &amp;= \\frac{b_0^{a_0}}{\\Gamma\\left(a_{0}\\right)} \\left(\\frac{1}{\\sigma^{2}}\\right)^{a_{0}+1} e^{-\\frac{b_{0}}{\\sigma^{2}}} \\frac{1}{(2 \\pi \\sigma^{2})^{\\frac{p}{2}}\\left| M_{0}\\right|^{\\frac{1}{2}}} e^{-\\frac{1}{2 \\sigma^{2}} Q \\left(\\beta, m_{0}, M_{0}\\right)} \\end{align}\\] where \\(Q(x, m, M)=(x-m)^{\\mathrm{\\scriptscriptstyle T}} M^{-1} (x-m) \\, .\\) Show details \\[\\begin{align} P(\\beta, \\sigma^{2}) &amp;= NIG \\left(\\beta, \\sigma^{2} \\mid m_{0}, M_{0}, a_{0}, b_{0}\\right) \\\\ &amp;= IG\\left(\\sigma^{2} \\mid a_{0}, b_{0}\\right) \\cdot N\\left(\\beta \\mid m_{0}, \\sigma^{2} M_{0}\\right) \\\\ &amp;= \\frac{b_0^{a_0}}{\\Gamma\\left(a_{0}\\right)} \\left(\\frac{1}{\\sigma^{2}}\\right)^{a_{0}+1} e^{-\\frac{b_{0}}{\\sigma^{2}}} \\frac{1}{(2 \\pi)^{\\frac{p}{2}}\\left|\\sigma^{2} M_{0}\\right|^{\\frac{1}{2}}} e^{-\\frac{1}{2 \\sigma^{2}} Q \\left(\\beta, m_{0}, M_{0}\\right)} \\\\ &amp;= \\frac{b_0^{a_0}}{\\Gamma\\left(a_{0}\\right)} \\left(\\frac{1}{\\sigma^{2}}\\right)^{a_{0}+1} e^{-\\frac{b_{0}}{\\sigma^{2}}} \\frac{1}{(2 \\pi \\sigma^{2})^{\\frac{p}{2}}\\left| M_{0}\\right|^{\\frac{1}{2}}} e^{-\\frac{1}{2 \\sigma^{2}} Q \\left(\\beta, m_{0}, M_{0}\\right)} \\\\ \\end{align}\\] where \\(Q(x, m, M)=(x-m)^{\\mathrm{\\scriptscriptstyle T}} M^{-1} (x-m) \\, .\\) Note: the Inverse-Gamma (\\(IG\\)) distribution has a relationship with Gamma distribution. Given \\(X \\sim Gamma(\\alpha, \\beta)\\), the density function of \\(X\\) is \\(f(x)=\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha-1} e^{-\\beta x}\\). Then \\(Y=\\frac{1}{X} \\sim IG\\left(\\alpha, \\beta\\right)\\) with density function \\(f(y)=\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{-\\alpha-1} e^{-\\frac{\\beta}{x}}\\). button#toggle-button1 { background-color: lightgray; color: black; border-radius: 5px; border: none; padding: 5px 10px; font-size: 16px; } button#toggle-button1:hover { background-color: gray; color: white; } 1.2.2 Marginal distribution of NIG prior As for marginal priors, we can can get it by integration \\[ \\begin{aligned} P(\\sigma^2) &amp; = \\int N I G\\left(\\beta, \\sigma^{2} \\mid m_{0}, M_{0}, a_{0}, b_{0}\\right) \\ d\\beta=I G\\left(\\sigma^{2} \\mid a_{0}, b_{0}\\right) \\\\ P(\\beta) &amp; = \\int N I G\\left(\\beta, \\sigma^{2} \\mid m_{0}, M_{0}, a_{0}, b_{0}\\right) \\ d\\sigma^{2}=t_{2a_0}\\left(m_0, \\frac{b_0}{a_0}M_0\\right) \\\\ \\end{aligned} \\] Show details \\[\\begin{align} P\\left(\\sigma^{2} \\right) &amp;= \\int NIG\\left(\\beta, \\sigma^{2} \\mid m_{0}, M_{0}, a_{0}, b_{0}\\right) \\ d\\beta \\\\ &amp;=IG\\left(\\sigma^{2} \\mid a_{0}, b_{0}\\right) \\int N\\left(\\beta \\mid m_{0}, \\sigma^{2} M_{0}\\right) \\ d\\beta \\\\ &amp;=IG\\left(\\sigma^{2} \\mid a_{0}, b_{0}\\right) \\end{align}\\] \\[\\begin{align} P(\\beta ) &amp;=\\int NIG \\left(\\beta, \\sigma^{2} \\mid m_{0}, M_{0}, a_{0}, b_{0}\\right) \\ d\\sigma^{2} \\\\ &amp;= \\int \\frac{b_0^{a_0}}{\\Gamma\\left(a_{0}\\right)} \\left(\\frac{1}{\\sigma^{2}}\\right)^{a_{0}+1} e^{-\\frac{b_{0}}{\\sigma^{2}}} \\frac{1}{(2 \\pi \\sigma^{2})^{\\frac{p}{2}}\\left| M_{0}\\right|^{\\frac{1}{2}}} e^{-\\frac{1}{2 \\sigma^{2}} Q \\left(\\beta, m_{0}, M_{0}\\right)} \\ d\\sigma^{2} \\\\ &amp;= \\frac{b_0^{a_0}}{\\Gamma\\left(a_{0}\\right)(2 \\pi)^{\\frac{p}{2}}\\left| M_{0}\\right|^{\\frac{1}{2}}} \\int \\left(\\frac{1}{\\sigma^{2}}\\right)^{a_{0}+\\frac{p}{2}+1} e^{-\\frac{1}{\\sigma^{2}}\\left(b_{0}+\\frac{1}{2} Q \\left(\\beta, m_{0}, M_{0}\\right)\\right)} \\ d\\sigma^{2} \\\\ &amp; \\quad \\left(\\text{let } u = \\frac{1}{\\sigma^2}, \\left|d\\sigma^{2}\\right|=\\frac{1}{u^2} d u\\right) \\\\ &amp;= \\frac{b_0^{a_0}}{\\Gamma\\left(a_{0}\\right)(2 \\pi)^{\\frac{p}{2}}\\left| M_{0}\\right|^{\\frac{1}{2}}} \\int u^{a_{0}+\\frac{p}{2}+1} e^{-\\left(b_{0}+\\frac{1}{2} Q \\left(\\beta, m_{0}, M_{0}\\right)\\right) u } \\frac{1}{u^2} \\ du \\\\ &amp;= \\frac{b_0^{a_0}}{\\Gamma\\left(a_{0}\\right)(2 \\pi)^{\\frac{p}{2}}\\left| M_{0}\\right|^{\\frac{1}{2}}} \\int u^{a_{0}+\\frac{p}{2}-1} e^{-\\left(b_{0}+\\frac{1}{2} Q \\left(\\beta, m_{0}, M_{0}\\right)\\right) u} \\ du \\\\ &amp; \\quad \\left(\\text{by Gamma integral function:} \\int x^{\\alpha - 1} exp^{-\\beta x} dx = \\frac{\\Gamma(\\alpha)}{\\beta^{\\alpha}}\\right) \\\\ &amp;= \\frac{b_{0}^{a_{0}} }{\\Gamma\\left(a_{0}\\right)(2 \\pi)^\\frac{p}{2}\\left|M_{0}\\right|^{\\frac{1}{2}}} \\frac{\\Gamma\\left(a_{0}+\\frac{p}{2}\\right)}{\\left(b_{0}+\\frac{1}{2} Q(\\beta,m_0,M_0)\\right)^{\\left(a_{0}+\\frac{p}{2}\\right)}} \\\\ &amp; = \\frac{b_0^{a_0}\\Gamma\\left(a_{0}+\\frac{p}{2}\\right)}{\\Gamma\\left(a_{0}\\right)(2 \\pi)^ \\frac{p}{2}\\left|M_{0}\\right|^{\\frac{1}{2}}} \\left(b_0(1+\\frac{1}{2 b_0} Q(\\beta,m_0,M_0))\\right)^{-\\left(a_{0}+\\frac{p}{2}\\right)} \\\\ &amp; = \\frac{b_0^{a_0}\\Gamma\\left(a_{0}+\\frac{p}{2}\\right) b_0^{- \\left( a_0+\\frac{p}{2}\\right)}}{\\Gamma\\left(a_{0}\\right)(2 \\pi)^ \\frac{p}{2}\\left|M_{0}\\right|^{\\frac{1}{2}}} \\left(1+\\frac{1}{2 b_0} \\left(\\beta-m_{0}\\right)^{\\mathrm{\\scriptscriptstyle T}} M_{0}^{-1}\\left(\\beta-m_{0}\\right) \\right)^{-\\left(a_{0}+\\frac{p}{2}\\right)} \\\\ &amp; =\\frac{\\Gamma\\left(a_{0}+\\frac{p}{2}\\right)}{\\left(2 \\pi \\right)^{\\frac{p}{2}} b_{0}^{\\frac{p}{2}} \\Gamma\\left(a_{0}\\right)|M|^{\\frac{1}{2}}}\\left(1+\\frac{1}{2 b_{0}} \\left(\\beta-m_{0}\\right)^{\\mathrm{\\scriptscriptstyle T}} M_{0}^{-1}\\left(\\beta-m_{0}\\right) \\right)^{-\\left(a_{0}+\\frac{p}{2}\\right)} \\\\ &amp; =\\frac{\\Gamma\\left(a_{0}+\\frac{p}{2}\\right)} {\\left(2 \\pi \\right)^{\\frac{p}{2}}\\left(a_{0} \\cdot \\frac{b_{0}}{a_{0}}\\right)^{\\frac{p}{2}} \\Gamma\\left(a_{0}\\right)|M|^{\\frac{1}{2}}} \\left(1+\\frac{1}{2 a_{0} \\cdot \\frac{b_{0}}{a_{0}}} \\left(\\beta-m_{0}\\right)^{\\mathrm{\\scriptscriptstyle T}} M_{0}^{-1}\\left(\\beta-m_{0}\\right)\\right)^{-\\left(a_{0}+\\frac{p}{2}\\right)}\\\\ &amp; =\\frac{\\Gamma\\left(a_{0}+\\frac{p}{2}\\right)}{\\left(2 a_{0} \\pi\\right)^{\\frac{p}{2}} \\Gamma\\left(a_{0}\\right)\\left|\\frac{b_{0}}{a_{0}} M\\right|^{\\frac{1}{2}}}\\left(1+\\frac{1}{2 a_{0}} \\left(\\beta-m_{0}\\right)^{\\mathrm{\\scriptscriptstyle T}}\\left(\\frac{b_{0}}{a_{0}} M_{0}\\right)^{-1}\\left(\\beta-m_{0}\\right)\\right)^{-\\left(a_{0}+\\frac{p}{2}\\right)} \\\\ &amp; =t_{2a_0}\\left(m_0, \\frac{b_0}{a_0}M_0\\right) \\; \\end{align}\\] button#toggle-button2 { background-color: lightgray; color: black; border-radius: 5px; border: none; padding: 5px 10px; font-size: 16px; } button#toggle-button2:hover { background-color: gray; color: white; } Note: the density of multivariate t-distribution is given by \\[ t_v(\\mu, \\Sigma)=\\frac{\\Gamma\\left(\\frac{v+p}{2}\\right)}{(v \\pi)^{\\frac{p}{2}} \\Gamma\\left(\\frac{v}{2}\\right) |\\Sigma|^{\\frac{1}{2}}}\\left(1+\\frac{1}{v}(x-\\mu)^{\\mathrm{\\scriptscriptstyle T}} \\Sigma^{-1}(x-\\mu)\\right)^{-\\frac{v+p}{2}} \\] 1.3 Conjugate Bayesian linear regression and M&amp;m formula Let \\(y_{n \\times 1}\\) be outcome variable and \\(X_{n \\times p}\\) be corresponding covariates. Assume \\(V\\) is known. The model is given by \\[\\begin{align} &amp; y=X \\beta+\\epsilon \\ , \\ \\epsilon \\sim N\\left(0, \\sigma^2 V\\right) \\\\ &amp; \\beta=m_0+\\omega \\ , \\ \\omega \\sim N\\left(0, \\sigma^2 M_0\\right) \\\\ &amp; \\sigma^2 \\sim I G\\left(a_0, b_0\\right) \\end{align}\\] The posterior distribution of \\((\\beta, \\sigma^2)\\) is given by \\[\\begin{align} P\\left(\\beta, \\sigma^{2} \\mid y\\right) = NIG\\left(\\beta, \\sigma^{2} \\mid M_{1}m_{1}, M_{1}, a_{1}, b_{1}\\right) \\; \\end{align}\\] where \\[\\begin{align} M_{1}^{-1} &amp;= M_{0}^{-1}+X^{\\mathrm{\\scriptscriptstyle T}} V^{-1} X \\\\ m_{1}&amp;=M_{0}^{-1} m_{0}+X^{\\mathrm{\\scriptscriptstyle T}} V^{-1} y \\\\ a_{1}&amp;=a_{0}+\\frac{p}{2} \\\\ b_{1}&amp;=b_{0}+\\frac{c^{\\ast}}{2}= b_{0}+\\frac{1}{2}\\left(m_{0}^{\\mathrm{\\scriptscriptstyle T}} M_{0}^{-1} m_{0}+y^{\\mathrm{\\scriptscriptstyle T}} V^{-1} y-m_{1}^{\\mathrm{\\scriptscriptstyle T}} M_{1} m_{1}\\right) \\end{align}\\] Show details \\[\\begin{align}\\label{eq:post_dist} P\\left(\\beta, \\sigma^{2} \\mid y\\right) &amp; \\propto NIG\\left(\\beta, \\sigma^{2} \\mid m_{0}, M_{0}, a_{0}, b_{0}\\right) \\cdot N\\left(y \\mid X \\beta, \\sigma^{2} V\\right) \\nonumber\\\\ &amp; \\propto IG\\left(\\sigma^{2} \\mid a_{0}, b_{0}\\right) \\cdot N\\left(\\beta \\mid m_{0}, \\sigma^{2} M_{0}\\right) \\cdot N\\left(y \\mid X \\beta, \\sigma^{2} V\\right) \\nonumber\\\\ &amp; \\propto \\frac{b_0^{a_0}}{\\Gamma\\left(a_{0}\\right)} \\left(\\frac{1}{\\sigma^{2}}\\right)^{a_{0}+1} e^{-\\frac{b_{0}}{\\sigma^{2}}} \\frac{1}{(2 \\pi \\sigma^{2})^{\\frac{p}{2}}\\left| M_{0}\\right|^{\\frac{1}{2}}} e^{-\\frac{1}{2 \\sigma^{2}} Q \\left(\\beta, m_{0}, M_{0}\\right)} \\frac{1}{(2 \\pi \\sigma^{2})^{\\frac{p}{2}}\\left| V\\right|^{\\frac{1}{2}}} e^{-\\frac{1}{2 \\sigma^{2}} Q \\left(y, X \\beta, V\\right)} \\nonumber\\\\ &amp; \\propto \\left(\\frac{1}{\\sigma^{2}}\\right)^{a_{0}+p+1} e^{-\\frac{b_{0}}{\\sigma^{2}}} e^{-\\frac{1}{2 \\sigma^{2}} \\left(Q \\left(\\beta, m_{0}, M_{0}\\right)+Q \\left(y, X \\beta, V\\right)\\right)}\\; \\end{align}\\] where \\[\\begin{align}\\label{eq:multivariate_completion_square} Q \\left(\\beta, m_{0}, M_{0}\\right)+Q \\left(y, X \\beta, V\\right) &amp;= (\\beta - m_{0})^{\\mathrm{\\scriptscriptstyle T}}M_{0}^{-1}(\\beta - m_{0}) + (y - X\\beta)^{\\mathrm{\\scriptscriptstyle T}}V^{-1}(y - X\\beta)\\; \\nonumber\\\\ &amp;= \\beta^{\\mathrm{\\scriptscriptstyle T}}M_{0}^{-1}\\beta - 2\\beta^{\\mathrm{\\scriptscriptstyle T}}M_{0}^{-1}m_{0} + m_{0}^{\\mathrm{\\scriptscriptstyle T}}M_{0}^{-1}m_{0} \\nonumber\\\\ &amp;\\qquad + \\beta^{\\mathrm{\\scriptscriptstyle T}}X^{\\mathrm{\\scriptscriptstyle T}}V^{-1}X\\beta - 2\\beta^{\\mathrm{\\scriptscriptstyle T}} X^{\\mathrm{\\scriptscriptstyle T}}V^{-1}y + y^{\\mathrm{\\scriptscriptstyle T}}V^{-1}y \\nonumber\\\\ &amp;= \\beta^{\\mathrm{\\scriptscriptstyle T}} \\left(M_{0}^{-1} + X^{\\mathrm{\\scriptscriptstyle T}}V^{-1}X\\right) \\beta - 2\\beta^{\\mathrm{\\scriptscriptstyle T}}\\left(M_{0}^{-1}m_{0} + X^{\\mathrm{\\scriptscriptstyle T}}V^{-1}y\\right) \\nonumber\\\\ &amp;\\qquad + m_{0}^{\\mathrm{\\scriptscriptstyle T}} M_{0}^{-1}m_{0} + y^{\\mathrm{\\scriptscriptstyle T}}V^{-1}y \\nonumber \\\\ &amp;= \\beta^{\\mathrm{\\scriptscriptstyle T}}M_{1}^{-1}\\beta - 2\\beta^{\\mathrm{\\scriptscriptstyle T}} m_{1} + c\\nonumber\\\\ &amp;= (\\beta - M_{1}m_{1})^{\\mathrm{\\scriptscriptstyle T}}M_{1}^{-1}(\\beta - M_{1}m_{1}) - m_{1}^{\\mathrm{\\scriptscriptstyle T}}M_{1}m_{1} +c \\nonumber\\\\ &amp;= (\\beta - M_{1}m_{1})^{\\mathrm{\\scriptscriptstyle T}}M_{1}^{-1}(\\beta - M_{1}m_{1}) +c^{\\ast}\\; \\end{align}\\] where \\(M_{1}\\) is a symmetric positive definite matrix, \\(m_{1}\\) is a vector, and \\(c\\) &amp; \\(c^{\\ast}\\) are scalars given by \\[\\begin{align} M_{1}^{-1} &amp;= M_{0}^{-1} + X^{\\mathrm{\\scriptscriptstyle T}}V^{-1}X \\\\ m_{1} &amp;= M_{0}^{-1}m_{0} + X^{\\mathrm{\\scriptscriptstyle T}}V^{-1}y \\\\ c &amp;= m_{0}^{\\mathrm{\\scriptscriptstyle T}} M_{0}^{-1}m_{0} + y^{\\mathrm{\\scriptscriptstyle T}}V^{-1}y \\\\ c^{\\ast} &amp;= c - m^{\\mathrm{\\scriptscriptstyle T}}Mm = m_{0}^{\\mathrm{\\scriptscriptstyle T}} M_{0}^{-1}m_{0} + y^{\\mathrm{\\scriptscriptstyle T}}V^{-1}y - m_{1}^{\\mathrm{\\scriptscriptstyle T}}M_{1}m_{1} \\end{align}\\] Note: \\(M_{1}\\), \\(m_{1}\\) and \\(c\\) do not depend upon \\(\\beta\\). Then, we have \\[\\begin{align} P\\left(\\beta, \\sigma^{2} \\mid y\\right) &amp; \\propto \\left(\\frac{1}{\\sigma^{2}}\\right)^{a_{0}+p+1} e^{-\\frac{b_{0}}{\\sigma^{2}}} e^{-\\frac{1}{2 \\sigma^{2}} ((\\beta - M_{1}m_{1})^{\\mathrm{\\scriptscriptstyle T}}M_{1}^{-1}(\\beta - M_{1}m_{1}) +c^{\\ast})}\\\\ &amp; \\propto \\left(\\frac{1}{\\sigma^{2}}\\right)^{a_{0}+p+1} e^{-\\frac{b_{0}+\\frac{c^{\\ast}}{2}}{\\sigma^{2}}} e^{-\\frac{1}{2 \\sigma^{2}} (\\beta - M_{1}m_{1})^{\\mathrm{\\scriptscriptstyle T}}M_{1}^{-1}(\\beta - M_{1}m_{1})}\\\\ &amp; \\propto \\left(\\frac{1}{\\sigma^{2}}\\right)^{a_{0}+\\frac{p}{2}+1} e^{-\\frac{b_{0}+\\frac{c^{\\ast}}{2}}{\\sigma^{2}}} (\\frac{1}{\\sigma^2})^{\\frac{p}{2}} e^{-\\frac{1}{2 \\sigma^{2}} (\\beta - M_{1}m_{1})^{\\mathrm{\\scriptscriptstyle T}}M_{1}^{-1}(\\beta - M_{1}m_{1})}\\\\ &amp; \\propto IG\\left(\\sigma^{2} \\mid a_{0}+\\frac{p}{2}, b_{0}+\\frac{c^{\\ast}}{2} \\right) \\cdot N\\left(\\beta \\mid M_{1}m_{1}, \\sigma^{2} M_{1}\\right) \\\\ &amp; \\propto IG\\left(\\sigma^{2} \\mid a_{1}, b_{1} \\right) \\cdot N\\left(\\beta \\mid M_{1}m_{1}, \\sigma^{2} M_{1}\\right) \\\\ &amp; \\propto NIG\\left(\\beta, \\sigma^{2} \\mid M_{1}m_{1}, M_{1}, a_{1}, b_{1}\\right) \\; \\end{align}\\] where \\[\\begin{align} M_{1}^{-1} &amp;= M_{0}^{-1}+X^{\\mathrm{\\scriptscriptstyle T}} V^{-1} X \\\\ m_{1}&amp;=M_{0}^{-1} m_{0}+X^{\\mathrm{\\scriptscriptstyle T}} V^{-1} y \\\\ a_{1}&amp;=a_{0}+\\frac{p}{2} \\\\ b_{1}&amp;=b_{0}+\\frac{c^{\\ast}}{2}= b_{0}+\\frac{1}{2}\\left(m_{0}^{\\mathrm{\\scriptscriptstyle T}} M_{0}^{-1} m_{0}+y^{\\mathrm{\\scriptscriptstyle T}} V^{-1} y-m_{1}^{\\mathrm{\\scriptscriptstyle T}} M_{1} m_{1}\\right) \\end{align}\\] button#toggle-button3 { background-color: lightgray; color: black; border-radius: 5px; border: none; padding: 5px 10px; font-size: 16px; } button#toggle-button3:hover { background-color: gray; color: white; } From derivation in marginal priors, the marginal posterior distributions can be easily get by updating corresponding parameters \\[ \\begin{aligned} &amp;P\\left(\\sigma^{2} \\mid y\\right)=I G\\left(\\sigma^{2} \\mid a_{1}, b_{1}\\right) \\\\ &amp;P(\\beta \\mid y)=t_{2a_1}\\left(M_1m_1, \\frac{b_1}{a_1}M_1\\right) \\end{aligned} \\] 1.4 Updating form of the posterior distribution We will use two ways to derive the updating form of the posterior distribution. 1.4.1 Method 1: Sherman-Woodbury-Morrison identity Theorem 1.2 (Sherman-Woodbury-Morrison identity) We have \\[\\begin{equation}\\label{ShermanWoodburyMorrison} \\left(A + BDC\\right)^{-1} = A^{-1} - A^{-1}B\\left(D^{-1}+CA^{-1}B\\right)^{-1}CA^{-1} \\end{equation}\\] where \\(A\\) and \\(D\\) are square matrices that are invertible and \\(B\\) and \\(C\\) are rectangular (square if \\(A\\) and \\(D\\) have the same dimensions) matrices such that the multiplications are well-defined. Sherman-Woodbury-Morrison identity is easily verified by multiplying the right hand side with \\(A + BDC\\) and simplifying to reduce it to the identity matrix. Using this formula, we have \\[ \\begin{aligned} M_1 &amp; = (M_{0}^{-1} + X^{\\mathrm{\\scriptscriptstyle T}}V^{-1}X)^{-1} \\\\ &amp; = M_0-M_0 X^{\\mathrm{\\scriptscriptstyle T}}\\left(V+X M_0 X^{\\mathrm{\\scriptscriptstyle T}}\\right)^{-1} X M_0 \\\\ &amp; = M_0-M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1} X M_0 \\end{aligned} \\] where \\(Q = V + X M_0 X^{\\mathrm{\\scriptscriptstyle T}}\\) We can show that \\[ \\begin{align} M_1 m_1 &amp; =m_0+M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-X m_0\\right) \\end{align} \\] Show details \\[\\begin{align} M_1 m_1 &amp; = \\left(M_0^{-1}+X^{\\mathrm{\\scriptscriptstyle T}} V^{-1} X\\right)^{-1} m_1 \\\\ &amp; = \\left(M_0-M_0 X^{\\mathrm{\\scriptscriptstyle T}}\\left(V+X M_0 X^{\\mathrm{\\scriptscriptstyle T}}\\right)^{-1} X M_0\\right)m_1 \\\\ &amp; = \\left(M_0-M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1} X M_0\\right) m_1 \\\\ &amp; = \\left(M_0-M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1} X M_0\\right)\\left(M_0^{-1} m_0+X^{\\mathrm{\\scriptscriptstyle T}} V^{-1} y\\right) \\\\ &amp; = m_0+M_0 X^{\\mathrm{\\scriptscriptstyle T}} V^{-1} y-M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1} X m_0 - M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1} X M_0 X^{\\mathrm{\\scriptscriptstyle T}} V^{-1} y \\\\ &amp; = m_0+M_0 X^{\\mathrm{\\scriptscriptstyle T}}\\left(I-Q^{-1} X M_0 X^{\\mathrm{\\scriptscriptstyle T}}\\right) V^{-1} y - M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1} X m_0 \\\\ &amp; = m_0+M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(Q-X M_0 X^{\\mathrm{\\scriptscriptstyle T}}\\right)V^{-1} y - M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1} X m_0 \\\\ &amp; \\left(\\text { since } Q=V+X M_0 X^{\\mathrm{\\scriptscriptstyle T}}\\right) \\\\ &amp; = m_0+M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}V V^{-1} y-M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1} X m_0 \\\\ &amp; = m_0+M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1} y-M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1} X m_0 \\\\ &amp; = m_0+M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-X m_0\\right) \\\\ \\end{align}\\] button#toggle-button4 { background-color: lightgray; color: black; border-radius: 5px; border: none; padding: 5px 10px; font-size: 16px; } button#toggle-button4:hover { background-color: gray; color: white; } Furthermore, we can simplify that \\[ \\begin{align} m_0^{\\mathrm{\\scriptscriptstyle T}} M_0^{-1} m_0+y^{\\mathrm{\\scriptscriptstyle T}} V^{-1} y-m_1^{\\mathrm{\\scriptscriptstyle T}} M_1 m_1 &amp; = \\left(y-X m_0\\right)^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-X m_0\\right) \\end{align} \\] Show details \\[\\begin{align} &amp; \\quad \\ m_0^{\\mathrm{\\scriptscriptstyle T}} M_0^{-1} m_0+y^{\\mathrm{\\scriptscriptstyle T}} V^{-1} y-m_1^{\\mathrm{\\scriptscriptstyle T}} M_1 m_1 \\\\ &amp; = m_0^{\\mathrm{\\scriptscriptstyle T}} M_0^{-1} m_0+y^{\\mathrm{\\scriptscriptstyle T}} V^{-1} y-m_1^{\\mathrm{\\scriptscriptstyle T}} \\left(m_0+M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1} (y - X m_0)\\right) \\\\ &amp; = m_0^{\\mathrm{\\scriptscriptstyle T}} M_0^{-1} m_0+y^{\\mathrm{\\scriptscriptstyle T}} V^{-1} y-m_1^{\\mathrm{\\scriptscriptstyle T}} m_0 - m_1^{\\mathrm{\\scriptscriptstyle T}} M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-X m_0\\right) \\\\ &amp; = m_0^{\\mathrm{\\scriptscriptstyle T}} M_0^{-1} m_0+y^{\\mathrm{\\scriptscriptstyle T}} V^{-1} y -m_0^{\\mathrm{\\scriptscriptstyle T}}\\left(M_0^{-1} m_0+X^{\\mathrm{\\scriptscriptstyle T}} V^{-1} y\\right) \\\\ &amp; \\qquad \\qquad \\qquad - m_1^{\\mathrm{\\scriptscriptstyle T}} M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-X m_0\\right) \\\\ &amp; = y^{\\mathrm{\\scriptscriptstyle T}} V^{-1} y-y^{\\mathrm{\\scriptscriptstyle T}} V^{-1} X m_0 - m_1^{\\mathrm{\\scriptscriptstyle T}} M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-X m_0\\right) \\\\ &amp; = y^{\\mathrm{\\scriptscriptstyle T}} V^{-1}\\left(y-X m_0 \\right)-m_1^{\\mathrm{\\scriptscriptstyle T}} M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-X m_0\\right) \\\\ &amp; =y^{\\mathrm{\\scriptscriptstyle T}} V^{-1}\\left(y-X m_0\\right)-\\underbrace{m_1^{\\mathrm{\\scriptscriptstyle T}} M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-X m_0\\right)}_{\\substack{\\text { simplify from left to right }}} \\\\ &amp; =y^{\\mathrm{\\scriptscriptstyle T}} V^{-1}\\left(y-X m_0\\right)-\\left(M_0 m_1\\right)^{\\mathrm{\\scriptscriptstyle T}} X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-X m_0\\right) \\\\ &amp; =y^{\\mathrm{\\scriptscriptstyle T}} V^{-1}\\left(y-X m_0\\right)-\\left(m_0+M_0 X^{\\mathrm{\\scriptscriptstyle T}} V^{-1} y\\right)^{\\mathrm{\\scriptscriptstyle T}} X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-m_0\\right) \\\\ &amp; =y^{\\mathrm{\\scriptscriptstyle T}} V^{-1}\\left(y-X m_0\\right)-\\left(X m_0+X M_0 X^{\\mathrm{\\scriptscriptstyle T}} V^{-1} y\\right)^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-X m_0\\right)\\\\ &amp; =y^{\\mathrm{\\scriptscriptstyle T}} V^{-1}\\left(y-X m_0\\right)-\\left(Q^{-1} X m_0+Q^{-1}\\left(X M_0 X^{\\mathrm{\\scriptscriptstyle T}}\\right)V^{-1} y\\right)\\left(y-X m_0\\right) \\\\ &amp; =y^{\\mathrm{\\scriptscriptstyle T}} V^{-1}\\left(y-X m_0\\right)-\\left(Q^{-1} X m_0+Q^{-1}(Q-V) V^{-1} y\\right)^{\\mathrm{\\scriptscriptstyle T}}(y-X m_0) \\\\ &amp; =y^{\\mathrm{\\scriptscriptstyle T}} V^{-1}\\left(y-X m_0\\right)-\\left(Q^{-1} X m_0+V^{-1} y- Q^{-1} y \\right)^{\\mathrm{\\scriptscriptstyle T}}\\left(y-X m_0\\right) \\\\ &amp; =y^{\\mathrm{\\scriptscriptstyle T}} V^{-1}\\left(y-X m_0\\right)-\\left(V^{-1} y+Q^{-1}\\left(X m_0-y\\right)\\right)^{\\mathrm{\\scriptscriptstyle T}}\\left(y-X m_0\\right) \\\\ &amp; =y^{\\mathrm{\\scriptscriptstyle T}} V^{-1}\\left(y-X m_0\\right)-y^{\\mathrm{\\scriptscriptstyle T}} V^{-1}\\left(y-X m_0\\right) +\\left(y-X m_0\\right)^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-X m_0\\right) \\\\ &amp; =\\left(y-X m_0\\right)^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-X m_0\\right) \\\\ \\end{align}\\] button#toggle-button5 { background-color: lightgray; color: black; border-radius: 5px; border: none; padding: 5px 10px; font-size: 16px; } button#toggle-button5:hover { background-color: gray; color: white; } So, we get the following updating form of the posterior distribution from Bayesian linear regression \\[ \\begin{aligned} P\\left(\\beta, \\sigma^{2} \\mid y\\right) = NIG\\left(\\beta, \\sigma^{2} \\mid \\tilde{m}_1, \\tilde{M}_1, a_{1}, b_{1}\\right) \\; \\end{aligned} \\] where \\[ \\begin{aligned} \\tilde{m}_1 &amp; =M_1 m_1=m_0+M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-X m_0\\right) \\\\ \\tilde{M}_1 &amp; =M_1=M_0-M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1} X M_0 \\\\ a_1 &amp; =a_0+\\frac{p}{2} \\\\ b_1 &amp; =b_0+\\frac{1}{2}\\left(y-X m_0\\right)^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-X m_0\\right) \\\\ Q &amp; =V+X M_0 X^{\\mathrm{\\scriptscriptstyle T}} \\end{aligned} \\] 1.4.2 Method 2: Distribution theory Previously, we got the Bayesian Linear Regression Updater using Sherman-Woodbury-Morrison identity. Here, we will derive the results without resorting to it. Recall that the model is given by \\[\\begin{align} &amp; y=X \\beta+\\epsilon \\ , \\ \\epsilon \\sim N\\left(0, \\sigma^2 V\\right) \\\\ &amp; \\beta=m_0+\\omega \\ , \\ \\omega \\sim N\\left(0, \\sigma^2 M_0\\right) \\\\ &amp; \\sigma^2 \\sim I G\\left(a_0, b_0\\right) \\end{align}\\] This corresponds to the posterior distribution \\[\\begin{align} P\\left(\\beta, \\sigma^2 \\mid y\\right) \\propto I G\\left(\\sigma^2 \\mid a_0, b_0\\right) &amp; \\times N\\left(\\beta \\mid m_0, \\sigma^2 M_0\\right) \\times N\\left(y \\mid X \\beta, \\sigma^2 V\\right) \\end{align}\\] We will derive \\(P\\left(\\sigma^2 \\mid y\\right)\\) and \\(P\\left(\\beta \\mid \\sigma^2, y\\right)\\) in a form that will reflect updates from the prior to the posterior. Integrating out \\(\\beta\\) from the model is equivalent to substituting \\(\\beta\\) from its prior model. Thus, \\(P\\left(y \\mid \\sigma^2\\right)\\) is derived simply from \\(y =X \\beta+\\epsilon =X\\left(m_0+\\omega\\right)+\\epsilon =X m_0 + X \\omega + \\epsilon =X m_0+ \\eta\\) where \\(\\eta = X \\omega + \\epsilon \\sim N\\left(0, \\sigma^2Q\\right)\\) and \\(Q=X M_0 X^{\\mathrm{\\scriptscriptstyle T}}+V \\, .\\) Therefore, \\[\\begin{align} y \\mid \\sigma^2 \\sim N\\left(X m_0, \\sigma^2 Q\\right)\\\\ \\end{align}\\] The posterior distribution is given by \\[\\begin{align} P\\left(\\sigma^2 \\mid y\\right) &amp; \\propto IG \\left(\\sigma^2 \\mid a_1, b_1\\right) \\end{align}\\] where \\[\\begin{align} &amp; a_1 = a_0 + \\frac{p}{2} \\\\ &amp; b_1 = b_0 + \\frac{1}{2} (y-Xm_0)^{\\mathrm{\\scriptscriptstyle T}} Q^{-1} \\left(y-Xm_0\\right) \\end{align}\\] Show details \\[\\begin{align} P\\left(\\sigma^2 \\mid y\\right) &amp; \\propto P\\left(\\sigma^2\\right) P\\left(y \\mid \\sigma^2\\right) \\\\ &amp; \\propto\\left(\\sigma^2 \\mid a_0, b_0\\right) \\times N\\left(y \\mid X m_0, \\sigma^2 Q\\right) \\\\ &amp; \\propto\\left(\\frac{1}{\\sigma^2}\\right)^{a_0+1} e^{-\\frac{b_0} {\\sigma^2} \\times\\left(\\frac{1}{\\sigma^2}\\right)^{\\frac{n}{2}} e^{-\\frac{1}{2 \\sigma^2}}\\left(y-Xm_0\\right)^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-Xm_0\\right)} \\\\ &amp; \\propto\\left(\\frac{1}{\\sigma^2}\\right)^{a_0+\\frac{p}{2}+1} e^{-\\frac{1}{\\sigma^2}\\left(b_0+\\frac{1}{2}\\left(y-Xm_0\\right)^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-Xm_0\\right)\\right)} \\\\ &amp; \\propto IG \\left(\\sigma^2 \\mid a_1, b_1\\right) \\end{align}\\] where \\[\\begin{align} &amp; a_1 = a_0 + \\frac{p}{2} \\\\ &amp; b_1 = b_0 + \\frac{1}{2} (y-Xm_0)^{\\mathrm{\\scriptscriptstyle T}} Q^{-1} \\left(y-Xm_0\\right) \\end{align}\\] button#toggle-button6 { background-color: lightgray; color: black; border-radius: 5px; border: none; padding: 5px 10px; font-size: 16px; } button#toggle-button6:hover { background-color: gray; color: white; } Next, we turn to \\(P\\left(\\beta \\mid \\sigma^2, y\\right)\\). Note that \\[ \\left(\\begin{array}{l} y \\\\ \\beta \\end{array}\\right) \\mid \\sigma^2 \\sim N\\left(\\left(\\begin{array}{l} Xm_0 \\\\ m_0 \\end{array}\\right), \\quad \\sigma^2 \\left(\\begin{array}{cc} Q &amp; X M_0 \\\\ M_0 X^{\\mathrm{\\scriptscriptstyle T}} &amp; M_0 \\end{array}\\right)\\right) \\; \\] Show details We have used the facts \\[ \\begin{aligned} &amp; \\operatorname{E}\\left[y \\mid \\sigma^2\\right] = Xm_0 \\, , \\ \\operatorname{Var}\\left(y \\mid \\sigma^2\\right)=\\sigma^2 Q \\, ; \\\\ &amp; \\operatorname{E}\\left[\\beta \\mid \\sigma^2\\right] = m_0 \\, , \\ \\operatorname{Var}\\left(\\beta \\mid \\sigma^2\\right)=\\sigma^2 M_0 \\, ; \\end{aligned} \\] \\[ \\begin{aligned} \\operatorname{Cov}\\left(y, \\beta \\mid \\sigma^2\\right) &amp;= \\operatorname{Cov}\\left(X \\beta+\\epsilon, \\beta \\mid \\sigma^2\\right) \\\\ &amp; =\\operatorname{Cov}\\left(X\\left(m_0+\\omega\\right)+\\epsilon, m_0+\\omega \\mid \\sigma^2\\right) \\\\ &amp; \\quad \\left( \\text {Since } m_0 \\text { is constant and } \\operatorname{Cov}(\\omega, \\epsilon)=0 \\right) \\\\ &amp; =\\operatorname{Cov}\\left(X \\omega, \\omega \\mid \\sigma^2\\right) \\\\ &amp; =\\sigma^2 X M_0 \\end{aligned} \\] button#toggle-button7 { background-color: lightgray; color: black; border-radius: 5px; border: none; padding: 5px 10px; font-size: 16px; } button#toggle-button7:hover { background-color: gray; color: white; } From the expression of a conditional distribution derived from a multivariate Gaussian, we obtain \\[ \\beta \\mid \\sigma^2, y \\sim N\\left(\\tilde{m}_1, \\sigma^2 \\tilde{M}_1\\right) \\] where \\[\\begin{align} &amp; \\tilde{m}_1=\\operatorname{E}\\left[\\beta \\mid \\sigma^2, y\\right]=m_0+M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1}\\left(y-X{m_0}\\right) \\\\ &amp; \\tilde{M}_1=M_0-M_0 X^{\\mathrm{\\scriptscriptstyle T}} Q^{-1} X M_0 \\\\ \\end{align}\\] Show details Note: \\[\\begin{align} &amp; \\left(\\begin{array}{l} X_1 \\\\ X_2 \\end{array}\\right) \\sim N\\left(\\left(\\begin{array}{l} \\mu_1 \\\\ \\mu_2 \\end{array}\\right),\\left(\\begin{array}{ll} \\Sigma_{11} &amp; \\Sigma_{12} \\\\ \\Sigma_{21} &amp; \\Sigma_{22} \\end{array}\\right)\\right) \\text { with } \\Sigma_{21} = \\Sigma_{12}^{\\mathrm{\\scriptscriptstyle T}} \\end{align}\\] \\[\\begin{align} &amp; \\Rightarrow X_2 \\mid X_1 \\sim N\\left(\\mu_{2 \\cdot 1}, \\Sigma_{2 \\cdot 1}\\right) \\end{align}\\] where \\(\\mu_{2 \\cdot 1}= \\mu_2+\\Sigma_{21} \\Sigma_{11}^{-1}\\left(X_1-\\mu_1\\right) \\text { and } \\Sigma_{2 \\cdot 1}=\\Sigma_{22}-\\Sigma_{21} \\Sigma_{11}^{-1} \\Sigma_{12} \\, .\\) button#toggle-button8 { background-color: lightgray; color: black; border-radius: 5px; border: none; padding: 5px 10px; font-size: 16px; } button#toggle-button8:hover { background-color: gray; color: white; } 1.5 Bayesian prediction Assume \\(V=I_{n}\\). Let \\(\\tilde{y}\\) denote an \\(\\tilde{n}\\times 1\\) vector of outcomes. \\(\\tilde{X}\\) is corresponding predictors. We seek to predict \\(\\tilde{y}\\) based upon \\(y\\) \\[\\begin{align} P(\\tilde{y} \\mid y) &amp;= t_{2a_1}\\left(\\tilde{X} M_1 m_1, \\frac{b_1}{a_1}\\left(I_{\\tilde{n}} + \\tilde{X} M_{1} \\tilde{X}^{\\mathrm{\\scriptscriptstyle T}}\\right)\\right) \\; \\end{align}\\] Show details \\[\\begin{align} P\\left(\\beta, \\sigma^{2}, \\tilde{y} \\mid y\\right) &amp;=P\\left(\\beta, \\sigma^{2} \\mid y\\right) \\cdot P\\left(\\tilde{y} \\mid \\beta, \\sigma^{2}, y\\right) \\\\ &amp;= P\\left(\\beta, \\sigma^{2}\\right) \\cdot P\\left(y \\mid \\beta, \\sigma^{2}\\right) \\cdot P\\left(\\tilde{y} \\mid \\beta, \\sigma^{2}, y\\right) \\\\ &amp;= NIG \\left(\\beta, \\sigma^{2} \\mid m_{0}, M_{0}, a_{0}, b_{0}\\right) \\cdot N\\left(y \\mid X \\beta, \\sigma^{2} I_{n}\\right) \\cdot N\\left(\\tilde{y} \\mid \\tilde{X} \\beta, \\sigma^{2} I_{\\tilde{n}}\\right) \\\\ &amp;= NIG \\left(\\beta, \\sigma^{2} \\mid M_{1} m_{1}, M_{1}, a_{1}, b_{1}\\right) \\cdot N\\left(\\tilde{y} \\mid \\tilde{X} \\beta, \\sigma^{2} I_{\\tilde{n}}\\right) \\\\ &amp;= IG\\left(\\sigma^{2} \\mid a_{1}, b_{1}\\right) \\cdot N\\left(\\beta \\mid M_{1} m_{1}, \\sigma^{2} M_{1} \\right) \\cdot N\\left(\\tilde{y} \\mid \\tilde{X} \\beta, \\sigma^{2} I_{\\tilde{n}} \\right) \\; \\end{align}\\] Then we can calculate posterior predictive density \\(P\\left(\\tilde{y} \\mid y\\right)\\) from \\(P\\left(\\beta, \\sigma^{2}, \\tilde{y} \\mid y\\right)\\) \\[\\begin{align} P\\left(\\tilde{y} \\mid y\\right) &amp;=\\iint P\\left(\\beta, \\sigma^{2}, \\tilde{y} \\mid y\\right) \\ d\\beta \\ d\\sigma^{2} \\\\ &amp;=\\iint IG\\left(\\sigma^{2} \\mid a_{1}, b_{1}\\right) \\cdot N\\left(\\beta \\mid M_{1} m_{1}, \\sigma^{2} M_{1} \\right) \\cdot N\\left(\\tilde{y} \\mid \\tilde{X} \\beta, \\sigma^{2} I_{\\tilde{n}}\\right) \\ d\\beta \\ d\\sigma^{2} \\\\ &amp;=\\int IG\\left(\\sigma^{2} \\mid a_{1}, b_{1}\\right) \\int N\\left(\\beta \\mid M_{1} m_{1}, \\sigma^{2} M_{1} \\right) \\cdot N\\left(\\tilde{y} \\mid \\tilde{X} \\beta, \\sigma^{2} I_{\\tilde{n}}\\right) \\ d\\beta \\ d\\sigma^{2} \\\\ \\end{align}\\] As for \\(\\int N\\left(\\beta \\mid M_{1} m_{1}, \\sigma^{2} M_{1}\\right) \\cdot N\\left(\\tilde{y} \\mid \\tilde{X} \\beta, \\sigma^{2} I_{\\tilde{n}}\\right) \\ d\\beta\\), we provide an easy way to derive it avoiding any integration at all. Note that we can write the above model as \\[\\begin{align} \\tilde{y} &amp;= \\tilde{X} \\beta + \\tilde{\\epsilon}, \\text{ where } \\tilde{\\epsilon} \\sim N\\left(0,\\sigma^2 I_{\\tilde{n}}\\right) \\\\ \\beta &amp;= M_{1} m_{1} + \\epsilon_{\\beta \\mid y}, \\text{ where } \\epsilon_{\\beta \\mid y} \\sim N\\left(0,\\sigma^2M_{1}\\right) \\end{align}\\] where \\(\\tilde{\\epsilon}\\) and \\(\\epsilon_{\\beta \\mid y}\\) are independent of each other. It then follows that \\[\\begin{align} \\tilde{y} &amp;= \\tilde{X} M_{1} m_{1} + \\tilde{X} \\epsilon_{\\beta \\mid y} + \\tilde{\\epsilon} \\sim N\\left(\\tilde{X} M_{1} m_{1}, \\sigma^2\\left(I_{\\tilde{n}} + \\tilde{X} M_{1} \\tilde{X}^{\\mathrm{\\scriptscriptstyle T}}\\right)\\right) \\end{align}\\] As a result \\[\\begin{align} P\\left(\\tilde{y} \\mid y\\right) &amp;=\\int IG\\left(\\sigma^{2} \\mid a_{1}, b_{1}\\right) \\cdot N\\left(\\tilde{X} M_{1} m_{1}, \\sigma^2\\left(I_{\\tilde{n}} + \\tilde{X} M_{1} \\tilde{X}^{\\mathrm{\\scriptscriptstyle T}}\\right)\\right) \\ d\\sigma^{2} \\\\ &amp;= t_{2a_1}\\left(\\tilde{X} M_1 m_1, \\frac{b_1}{a_1}\\left(I_{\\tilde{n}} + \\tilde{X} M_{1} \\tilde{X}^{\\mathrm{\\scriptscriptstyle T}}\\right)\\right) \\; \\end{align}\\] button#toggle-button9 { background-color: lightgray; color: black; border-radius: 5px; border: none; padding: 5px 10px; font-size: 16px; } button#toggle-button9:hover { background-color: gray; color: white; } 1.6 Sampling from the posterior distribution We can get the joint posterior density \\(P\\left(\\beta, \\sigma^{2}, \\tilde{y} \\mid y\\right)\\) by sampling process Draw \\(\\hat{\\sigma}_{(i)}^{2}\\) from \\(I G\\left(a_{1}, b_{1}\\right)\\) Draw \\(\\hat{\\beta}_{(i)}\\) from \\(N\\left(M_{1} m_{1}, \\hat{\\sigma}_{(i)}^{2} M_{1}\\right)\\) Draw \\(\\tilde{y}_{(i)}\\) from \\(N\\left(\\tilde{X} \\hat{\\beta}_{(i)}, \\hat{\\sigma}_{(i)}^{2}I_{\\tilde{n}}\\right)\\) "],["the-divide-conquer-algorithm.html", "Chapter 2 The Divide &amp; Conquer Algorithm 2.1 The Problem 2.2 Parallel Computing 2.3 Sequential Computing", " Chapter 2 The Divide &amp; Conquer Algorithm The divide and conquer algorithm is a strategy of solving a large problem by breaking it down into two or more smaller and more manageable sub-problems of the same or of a similar genre. The solutions to the sub-problems are then combined to get the desired output: the solution to the original problem. In this section, we apply the divide and conquer algorithm to the Bayesian linear regression framework. 2.1 The Problem Let \\(y = (y_1, y_2, ...,y_n)^{T}\\) be an \\(n \\times 1\\) random vector of outcomes and \\(X\\) be a fixed \\(n \\times p\\) matrix of predictors with full column rank. Consider the following Bayesian (hierarchical) linear regression model, \\[ NIG(\\beta, \\sigma^2 \\mid m_0, M_0, a_0, b_0) \\times N(y \\mid X\\beta, \\sigma^2I_n).\\] The posterior density is \\(p(\\beta, \\sigma^2 \\mid y) = IG(\\sigma^2 \\mid a_1, b_1) \\times N(\\beta \\mid M_1m_1, \\sigma^2M_1)\\), and to carry out Bayesian inference, we first sample \\(\\sigma^2 \\sim IG(a_1, b_1)\\) and then, for each sampled \\(\\sigma^2\\), we sample \\(\\beta \\sim N(M_1m_1, \\sigma^2 M_1)\\). Assume that \\(n\\) is so large that we are unable to store or load \\(y\\) or \\(X\\) into our CPU to carry out computations for them. We decide to divide our data set into K mutually exclusive and exhaustive subsets, each comprising a manageable number of points. Note that \\(p\\) is small, so computations involving \\(p \\times p\\) matrices are fine. Let \\(y_k\\) denote the \\(q_k \\times 1\\) sub-vector of \\(y\\), and \\(X_k\\) be the \\(q_k \\times p\\) sub-matrix of \\(X\\) in subset \\(k\\), where each \\(q_k\\) has been chosen by us so that \\(q_k &gt; p\\), and is small enough such that we can fit the above model on \\(\\{y_k, X_k\\}\\). This section will clearly explain how we can still compute \\(a_1, b_1, M_1\\) and \\(m_1\\) without ever having to store or compute with \\(y\\) or \\(X\\), but with quantities computed using only the subsets \\(\\{y_k, X_k\\}\\) for \\(k = 1,2,...,K\\). 2.2 Parallel Computing 2.2.1 Background and Motivation A first approach to the problem presented above is called ‘parallel computing’. Parallel computing is a type of computation in which many calculations or processes are carried out simultaneously - which in our context, entails dividing our data set into manageable subsets, calculating posteriors for each of these subsets simultaneously, and finally expressing the posteriors of the entire data set as functions of the posteriors of the subsets. The motivation behind this method is to make computation more efficient. 2.2.2 Solution Using the multivariate completing the square method, we know that the explicit expressions for \\(a_1, b_1, m_1\\) and \\(M_1\\) are given by: .ul { border: 2px solid black; } \\(a_1\\) = \\(a_0 + \\frac{n}{2}\\) \\(b_1\\) = \\(b_0 + \\frac{c}{2}\\) \\(c\\) = \\(m_0^{T}M^{-1}_0 m_0 + y^{\\mathrm{\\scriptscriptstyle T}}y - m_1^{\\mathrm{\\scriptscriptstyle T}}M_1m_1\\) \\(m_1\\) = \\((X^{\\mathrm{\\scriptscriptstyle T}}y + M_0^{-1}m_0)\\) \\(M_1^{-1}\\) = \\(X^{\\mathrm{\\scriptscriptstyle T}}X + M_0^{-1}\\) This implies that the explicit expressions for \\(a_1, b_1, m_1\\) and \\(M_1\\) for the \\(i^{th}\\) subset are given by: \\(a_{1i}\\) = \\(a_0 + \\frac{q_i}{2}\\) \\(b_{1i}\\) = \\(b_0 + \\frac{c}{2}\\) \\(c_i\\) = \\(m_0^{\\mathrm{\\scriptscriptstyle T}}M^{-1}_0 m_0 + y_i^{\\mathrm{\\scriptscriptstyle T}}y_i - m_{1i}^{\\mathrm{\\scriptscriptstyle T}}M_{1i}m_{1i}\\) \\(m_i\\) = \\((X_i^{\\mathrm{\\scriptscriptstyle T}}y_i + M_0^{-1}m_0)\\) \\(M_i^{-1}\\) = \\(X_i^{\\mathrm{\\scriptscriptstyle T}}X_i + M_0^{-1}\\) We can express the posteriors of the entire data set as a function of the posteriors of the subsets as follows: \\(a_1\\) = \\(\\sum_{i=1}^{k} a_{1i} + (k-1)a_0\\) \\(b_1\\) = \\(b_0 + m_0^{\\mathrm{\\scriptscriptstyle T}}M^{-1}_0 m_0 + \\sum_{i=1}^{k}y_i^{\\mathrm{\\scriptscriptstyle T}}y_i - m_{1i}^{\\mathrm{\\scriptscriptstyle T}}M_{1i}m_{1i}\\) \\(m_1\\) = \\(\\sum_{i=1}^{k}m_{1i} - (k-1)M^{-1}_0m_0\\) \\(M_1^{-1}\\) = \\(\\sum_{i=1}^{k}x_i^{\\mathrm{\\scriptscriptstyle T}}x_i + M^{-1}_0 = \\sum_{i=1}^{k}M^{-1}_{1i} - (k-1)M^{-1}_0\\) 2.2.3 Algebra This section details the algebra used to obtain the posteriors of the entire data set expressed as functions of the posteriors of the subsets. Starting with \\(a_1\\): \\[\\begin{align*} \\sum_{i=1}^{k} a_{i1} &amp;= \\sum_{i=1}^{k} a_0 + \\sum_{i=1}^{k}\\frac{q_i}{2} \\\\ &amp;= ka_0 + \\frac{n}{2} \\\\ &amp;= (k-1)a_0 + a_0 + \\frac{n}{2} \\\\ &amp;= (k-1)a_0 + a_1 \\\\ \\end{align*}\\] \\[\\begin{equation} \\Longrightarrow a_1 = \\sum_{i=1}^{k} a_{i1} - (k-1)a_0 \\end{equation}\\] Moving onto \\(m\\): Recall that \\(m_1 = X^{\\mathrm{\\scriptscriptstyle T}}y + V_\\beta^{-1}\\mu_\\beta\\), where \\[X = \\begin{bmatrix}x_1\\\\ x_2 \\\\ \\vdots \\\\ x_k \\end{bmatrix} \\text{, where}\\ x_i \\in \\mathbb{R}^{m_i \\times 1} \\ \\text{and} \\ y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} \\text{, where}\\ y_i \\in \\mathbb{R}.\\] Using linear algebra we can see that: \\[ X^{\\mathrm{\\scriptscriptstyle T}}y = \\begin{bmatrix}x_1^{\\mathrm{\\scriptscriptstyle T}} &amp; x_2^{\\mathrm{\\scriptscriptstyle T}} &amp; \\dots &amp; x_k^{T} \\end{bmatrix} \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = \\sum_{i=1}^{k} x_i^{\\mathrm{\\scriptscriptstyle T}}y_i \\] Thus: \\[\\begin{align*} \\sum_{i=1}^{k} m_i &amp;= \\sum_{i=1}^{k} x_i^{\\mathrm{\\scriptscriptstyle T}}y_i + kV_\\beta\\mu_\\beta^{-1} \\\\ &amp;= X^{\\mathrm{\\scriptscriptstyle T}}y + kV_\\beta^{-1}\\mu_\\beta \\\\ &amp;= X^{\\mathrm{\\scriptscriptstyle T}}y + V_\\beta^{-1}\\mu_\\beta + (k-1)V_\\beta^{-1}\\mu_\\beta \\\\ &amp;= m + (k-1)V_\\beta^{-1}\\mu_\\beta \\end{align*}\\] \\[\\begin{equation} \\Longrightarrow m_1 = \\sum_{i=1}^{k}m_i - (k-1)V_\\beta^{-1}\\mu_\\beta \\end{equation}\\] Next is \\(M^{-1}_1\\): Recall that \\(M_1^{-1} = X^{\\mathrm{\\scriptscriptstyle T}}X + V_\\beta^{-1}\\). Using linear algebra again, we can see that: \\[ X^{\\mathrm{\\scriptscriptstyle T}}X = \\begin{bmatrix}x_1^{\\mathrm{\\scriptscriptstyle T}} &amp; x_2^{\\mathrm{\\scriptscriptstyle T}} &amp; \\dots &amp; x_k^{\\mathrm{\\scriptscriptstyle T}} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_k \\end{bmatrix} = \\sum_{i=1}^{k} x_i^{\\mathrm{\\scriptscriptstyle T}}x_i \\] \\[\\begin{equation} \\Longrightarrow M_1^{-1} = \\sum_{i=1}^{k}x_i^{\\mathrm{\\scriptscriptstyle T}}x_i + V_\\beta^{-1} \\end{equation}\\] Last is \\(b_1\\): Recall that \\(b_1 = b_0 + \\frac{m_0^{\\mathrm{\\scriptscriptstyle T}}M^{-1}_0 m_0 + y^{\\mathrm{\\scriptscriptstyle T}}y - m_1^{\\mathrm{\\scriptscriptstyle T}}M_1m_1}{2}\\). Using linear algebra once again, we can see that: \\[ y^{\\mathrm{\\scriptscriptstyle T}}y = \\begin{bmatrix}y_1^{\\mathrm{\\scriptscriptstyle T}} &amp; y_2^{\\mathrm{\\scriptscriptstyle T}} &amp; \\dots &amp; y_n^{\\mathrm{\\scriptscriptstyle T}} \\end{bmatrix} \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = \\sum_{i=1}^{n} y_i^{\\mathrm{\\scriptscriptstyle T}}y_i \\] \\[\\begin{equation} \\Longrightarrow b_1 = b_0 + \\frac{m_0^{\\mathrm{\\scriptscriptstyle T}}M^{-1}_0 m_0 + \\sum_{i=1}^{n} y_i^{\\mathrm{\\scriptscriptstyle T}}y_i - m_1^{\\mathrm{\\scriptscriptstyle T}}M_1m_1}{2} \\end{equation}\\] Which concludes the algebra behind obtaining \\(a_1, b_1, M_1, \\text{and} \\ m_1\\) from the posteriors computed using the subsetted data. 2.3 Sequential Computing 2.3.1 Background and Motivation A second approach to the problem presented in this section is called ‘sequential computing’. Sequential computing is a type of computation where one instruction is given at a particular time and the next instruction has to wait for the first instruction to execute. For our problem, this entails computing the posterior for the first subset, using it to compute posterior of next subset, and so on until we get to the last subset, which will upon computation will give us the posterior of the entire data set. 2.3.2 Solution Let \\(D_k = \\{y_k, X_k\\}\\) be the \\(i^{th}\\) subset of the entire data set for \\(k = 1, ..., K\\), such that \\(D_i \\perp D_j ,\\ \\forall \\ i \\neq j\\). We will start with a simple example of how sequential computing works by setting \\(k = 2\\). The posterior density of the first subset is: \\[\\begin{equation} p(\\beta, \\sigma^2 \\mid D_1) \\propto IG(\\sigma^2 \\mid a_1, b_1) \\times N(\\beta \\mid M_1m_1, \\sigma^2M_1) \\end{equation}\\] The posteriors \\(\\{a_1, b_1, M_1, \\text{and} \\ m_1\\}\\) then replace \\(\\{a_0, b_0\\, M_0, \\text{and} \\ m_0\\}\\) as priors when using the second subset to calculate posteriors: \\[\\begin{align*} p(\\beta, \\sigma^2 \\mid D_1, D_1) &amp;\\propto p(\\beta, \\sigma^2, D_1, D_2) \\\\ &amp;\\propto p(\\beta, \\sigma^2) \\times p(D_1, D_2, \\mid \\beta, \\sigma^2) \\\\ &amp;\\propto p(\\beta, \\sigma^2) \\times p(D_1 \\mid \\beta, \\sigma^2) \\times p(D_2 \\mid \\beta, \\sigma^2) \\\\ &amp;\\propto p(\\beta, \\sigma^2 \\mid D_1) \\times p(D_2 \\mid \\beta, \\sigma^2) \\end{align*}\\] Which illustrates how the posteriors of the previous subset act as a priors when calculating the posteriors of the current subset. If we generalize this to \\(k\\) subsets and do some algebra, we can derive equations for the posteriors of the last subset, which are equivalent to the posteriors obtained using the full data set. \\(a_{1k}\\) = \\(a_0 + \\sum_{i=1}^{k}\\frac{q_i}{2}\\) \\(b_1\\) = \\(b_0 + \\frac{c}{2}\\) \\(c\\) = \\(m_0^{\\mathrm{\\scriptscriptstyle T}}M^{-1}_0 m_0 + y^{\\mathrm{\\scriptscriptstyle T}}y - m_1^{\\mathrm{\\scriptscriptstyle T}}M_1m_1\\) \\(m_1\\) = \\((X^{\\mathrm{\\scriptscriptstyle T}}y + M_0^{-1}m_0)\\) \\(M_1^{-1}\\) = \\(X^{\\mathrm{\\scriptscriptstyle T}}X + M_0^{-1}\\) "],["posteriors-using-sufficient-statistics.html", "Chapter 3 Posteriors Using Sufficient Statistics 3.1 Methods 3.2 Posterior From Improper Priors 3.3 Extension to Divide &amp; Conquer Algorithm", " Chapter 3 Posteriors Using Sufficient Statistics Recall the following Bayesian (hierarchical) linear regression model from part 1: \\[NIG(M_0, m_0, a_0, b_0) \\times N(y \\mid X\\beta, \\sigma^2 I_n)\\] where \\(y = (y_1, y_2, ..., y_n)^{\\mathrm{\\scriptscriptstyle T}}\\) is an \\(n \\times 1\\) vector of outcomes and \\(X\\) is a fixed \\(n \\times p\\) matrix of predictors with full column rank. Suppose that due to privacy reasons, instead of \\(y = (y_1, y_2, ..., y_n)^{\\mathrm{\\scriptscriptstyle T}}\\) we are given data that only consists of the sufficient statistics, \\(s^2\\) and \\(\\bar{y}\\). Further suppose that we are working with the following simpler model: \\[y = 1_n \\beta_0 + \\epsilon.\\] How do we obtain \\(p(\\beta_0, \\sigma^2 \\mid y)\\) using this data set? 3.1 Methods We can use the factorization theorem to factor the normal distribution’s pdf into a function of the sufficient statistics, \\(s^2\\) and \\(\\bar{y}.\\) \\[\\begin{align*} f(y \\mid \\beta_0, \\sigma^2) &amp;= \\Pi_{i=1}^{n} (2\\pi\\sigma^2)^{-\\frac{1}{2}} \\exp\\{-\\frac{1}{2\\sigma^2}(y_i-\\beta_0)^2\\}\\\\ &amp;= (2\\pi\\sigma^2)^{-\\frac{n}{2}} \\exp\\{-\\frac{1}{2\\sigma^2}\\Sigma(y_i - \\beta_0)^2\\} \\\\ &amp;= (2\\pi\\sigma^2)^{-\\frac{n}{2}} \\exp\\{-\\frac{1}{2\\sigma^2}\\Sigma(y_i - \\bar{y} + \\bar{y} - \\beta_0)^2\\} \\\\ &amp;= (2\\pi\\sigma^2)^{-\\frac{n}{2}} \\exp\\{-\\frac{1}{2\\sigma^2}\\Sigma[(y_i - \\bar{y})^2 + (\\bar{y} - \\beta_0)^2]\\} \\\\ &amp;= (2\\pi\\sigma^2)^{-\\frac{n}{2}} \\exp\\{-\\frac{1}{2\\sigma^2}[(n-1)s^2 + n(\\bar{y} - \\beta_0)^2]\\} \\end{align*}\\] We have now successfully expressed our likelihood as a function of the sufficient statistics \\(s^2\\) and \\(\\bar{x}\\) and can use this factorized likelihood along with Bayes’ Rule to compute \\(p(\\beta_0, \\sigma^2)\\). \\[\\begin{align*} p(\\beta_0, \\sigma^2 \\mid y) &amp;= p(\\beta_0 \\mid \\mu_\\beta, \\sigma^2) \\times p(\\sigma^2 \\mid a_0, b_0) \\times p(y \\mid \\beta_0, \\sigma^2) \\\\ &amp;= N(\\beta_0 \\mid \\mu_\\beta, \\sigma^2) \\times IG(\\sigma^2 \\mid a_0,b_0) \\times N(y \\mid \\beta_0, \\sigma^2) \\\\ &amp;= (2\\pi\\sigma^2)^{-\\frac{1}{2}}\\exp\\{-\\frac{1}{2\\sigma^2}(\\beta_0 - \\mu_\\beta)^2\\} \\times \\frac{b_0^{a_0}}{\\Gamma(a_0)}(\\sigma^2)^{-a-1}\\exp\\{-\\frac{b}{\\sigma^2}\\} \\\\ &amp;\\times (2\\pi\\sigma^2)^{-\\frac{n}{2}}\\exp\\{-\\frac{1}{2\\sigma^2}[(n-1)s^2 + n(\\bar{y} - \\beta_0)^2]\\} \\\\ &amp;\\propto (\\sigma^2)^{-a_0-1} \\times (\\sigma^2)^{-\\frac{n+1}{2}} \\times \\exp\\{-\\frac{b_0}{\\sigma^2}\\} \\times \\exp\\{-\\frac{1}{2\\sigma^2}(\\beta_0 - \\mu_\\beta)^2\\} \\\\ &amp;\\times \\exp\\{-\\frac{1}{2\\sigma^2}[(n-1)s^2 + n(\\bar{y} - \\beta_0)^2]\\} \\\\ &amp;= (\\sigma^2)^{-a_0-1} \\times (\\sigma^2)^{-\\frac{n+1}{2}} \\exp\\{-\\frac{b_0}{\\sigma^2}\\} \\times \\exp\\{-\\frac{1}{2\\sigma^2}[(\\beta_0 - \\mu_{\\beta})^2 +(n-1)s^2 + n(\\bar{y}-\\beta_0)^2]\\} \\end{align*}\\] Complete the square on the following term: \\[\\begin{align*} &amp;(\\beta_0 - \\mu_{\\beta})^2 +(n-1)s^2 + n(\\bar{y}-\\beta_0)^2 \\\\ &amp;= (n+1)\\beta_0^2 - 2(\\mu_{\\beta} + n\\bar{y})\\beta_0 + n\\bar{y}^2 + \\mu_\\beta^2 + (n-1)s^2 \\\\ &amp;= (n+1)(\\beta_0 - \\frac{\\mu_{\\beta} + n\\bar{y}}{n+1})^2 + n\\bar{y}^2 + \\mu_\\beta^2 + (n-1)s^2 - \\frac{(\\mu_\\beta + n\\bar{y})^2}{n+1}\\\\ &amp;= (n+1)(\\beta_0 - \\frac{\\mu_{\\beta} + n\\bar{y}}{n+1})^2 + y^{\\mathrm{\\scriptscriptstyle T}}\\Big(\\frac{1_n1_n^{\\mathrm{\\scriptscriptstyle T}}}{n}\\Big)y + y^{\\mathrm{\\scriptscriptstyle T}}\\Big(I - \\frac{1_n1_n^{\\mathrm{\\scriptscriptstyle T}}}{n}\\Big)y + \\mu_\\beta^2 - \\frac{(\\mu_\\beta + n\\bar{y})^2}{n+1} \\\\ &amp;= (n+1)(\\beta_0 - \\frac{\\mu_{\\beta} + n\\bar{y}}{n+1})^2 + y^{\\mathrm{\\scriptscriptstyle T}}y + \\mu_\\beta^2 - \\frac{(\\mu_\\beta + n\\bar{y})^2}{n+1} \\end{align*}\\] .ul { border: 2px solid black; } We now let: \\(\\mu_\\beta^{*} = \\frac{n\\bar{y} + \\mu_\\beta}{n+1}\\) \\(c = y^{\\mathrm{\\scriptscriptstyle T}}y + \\mu_\\beta^2 - \\frac{(n\\bar{y} + \\mu_\\beta)^2}{n+1}\\), and apply this expansion of this term into the original problem. \\[\\begin{align*} &amp;=(\\sigma^2)^{-a_0-1} \\times (\\sigma^2)^{-\\frac{n+1}{2}} \\times \\exp\\{-\\frac{b_0}{\\sigma^2}\\} \\times \\exp\\{-\\frac{1}{2\\sigma^2}(n+1)(\\beta_0 - \\mu_\\beta^{*})^2\\} \\times \\exp\\{-\\frac{c}{2\\sigma^2}\\} \\\\ &amp;= (\\sigma^2)^{-a_0-1} \\times (\\sigma^2)^{-\\frac{n+1}{2}} \\times \\exp\\{-\\frac{b_0}{\\sigma^2}\\} \\times \\exp\\{-\\frac{1}{2\\sigma^{2\\ast}}(\\beta_0 - \\mu_\\beta^{*})^2\\} \\times \\exp\\{-\\frac{c}{2\\sigma^2}\\} \\\\ &amp;= (\\sigma^2)^{-a_0-1-\\frac{n}{2}} \\times \\exp\\{-\\frac{(b_0 + \\frac{c}{2})}{\\sigma^2}\\} \\times (\\sigma^2)^{-\\frac{1}{2}} \\times \\exp\\{-\\frac{1}{2\\sigma^{2\\ast}}(\\beta_0 - \\mu_\\beta^{*})^2\\} \\\\ &amp;= \\underbrace{(\\sigma^2)^{a_1-1} \\times \\exp\\{-\\frac{b_1}{\\sigma^2}\\}}_\\text{kernal of $IG(a_1, b_1)$} \\times \\underbrace{(\\sigma^2)^{\\frac{1}{2}} \\times \\exp\\{-\\frac{1}{2\\sigma^{2\\ast}}(\\beta_0 - \\mu_\\beta^{*})^2\\}}_\\text{kernal of $N(\\mu_\\beta^{\\ast}, \\sigma^{2\\ast})$} \\end{align*}\\] Where: \\(\\mu_\\beta^{*} = \\frac{n\\bar{y} + \\mu_\\beta}{n+1}\\) \\(\\sigma^{2*} = \\frac{\\sigma^2}{n+1}\\) \\(a_1 = a_0 + \\frac{n}{2}\\) \\(b_1 = b_0 + \\frac{c}{2}\\) \\(c = y^{\\mathrm{\\scriptscriptstyle T}}y + \\mu_\\beta^2 - \\frac{(n\\bar{y} + \\mu_\\beta)^2}{n+1}\\) 3.2 Posterior From Improper Priors Recall that when we work with this model in a general regression setting where \\(X\\) is an \\(n \\times p\\) matrix predictors with full column rank, the posterior density is: \\[\\begin{align*} p(\\beta, \\sigma^2 \\mid y) &amp;= IG(\\sigma^2 \\mid a_1, b_1) \\times N(\\beta \\mid M_1m_1, \\sigma^2M_1) \\end{align*}\\] We end up with: \\(m_0^{*} = Mm\\) \\(M_1 = (X^{\\mathrm{\\scriptscriptstyle T}}X + M_{0}^{-1})^{-1}\\) \\(m_1 = X^{\\mathrm{\\scriptscriptstyle T}}y + M_{0}^{-1}m_0\\) \\(a_1 = a_0 + \\frac{n}{2}\\) \\(b_1 = b_0 + \\frac{c}{2}\\) \\(c = y^{\\mathrm{\\scriptscriptstyle T}}y + m_0^{\\mathrm{\\scriptscriptstyle T}}M_0^{-1}m_0 - m_1^{\\mathrm{\\scriptscriptstyle T}}M_1m_1\\) If we take \\(M_0^{-1} \\rightarrow 0\\) (i.e. the null matrix), and \\(a_0 \\rightarrow -\\frac{p}{2}\\), and \\(b_0 \\rightarrow 0\\) we can see that it leads to the improper prior \\(p(\\beta, \\sigma^2) \\propto \\frac{1}{\\sigma^2}\\) \\[\\begin{align*} p(\\beta, \\sigma^2) &amp;= IG(\\sigma^2 \\mid a_0, b_0) \\times N(\\beta \\mid m_0, \\sigma^2M_0) \\\\ &amp;= \\frac{b_0^{a_0}}{\\Gamma(a_0)}(\\sigma^2)^{-a_0-1} \\exp\\{-\\frac{b_0}{\\sigma^2}\\} \\times (2\\pi)^{-p/2}\\det(\\sigma^2M_0)^{-\\frac{1}{2}} \\exp\\{-\\frac{1}{2\\sigma^2}(\\beta - m_0)^{\\mathrm{\\scriptscriptstyle T}}\\frac{M_0^{-1}}{\\sigma^2}(\\beta - m_0)\\} \\\\ &amp;\\propto (\\sigma^2)^{-a_0-1}\\exp\\{-\\frac{b_0}{\\sigma^2}\\} \\times (\\sigma^2)^{-\\frac{p}{2}}\\exp\\{-\\frac{1}{2}(\\beta - m_0)^{\\mathrm{\\scriptscriptstyle T}}\\frac{M_0^{-1}}{\\sigma^2}(\\beta - m_0)\\} \\\\ &amp;= (\\sigma^2)^{-(\\frac{p}{2}+1})\\exp\\{-\\frac{b_0}{\\sigma^2}\\} \\times (\\sigma^2)^{-\\frac{p}{2}}\\exp\\{-\\frac{1}{2}(\\beta - m_0)^{\\mathrm{\\scriptscriptstyle T}} \\frac{0}{\\sigma^2}(\\beta - m_0)\\}\\\\ &amp;= \\frac{1}{\\sigma^2}, \\end{align*}\\] and ultimately yields a posterior distribution with: \\(m_1 = (X^{\\mathrm{\\scriptscriptstyle T}}X)^{-1}X^{\\mathrm{\\scriptscriptstyle T}}y\\) \\(a_1 = \\frac{n-p}{2}\\) \\(b_1 = b_0 + \\frac{c^{*}}{2}\\) \\(c^{*} = y^{\\mathrm{\\scriptscriptstyle T}}y - m_1^{\\mathrm{\\scriptscriptstyle T}}M_1m_1 = y^{\\mathrm{\\scriptscriptstyle T}}(I - P_x)y = (n-p)s^2\\) where \\(P_x = X(X^{\\mathrm{\\scriptscriptstyle T}}X)^{-1}X^{\\mathrm{\\scriptscriptstyle T}}\\). If we apply this framework, to the simpler model presented at the beginning of the section, \\(y = 1_n\\beta_0 + \\epsilon\\), where \\(X\\) is a vector of 1’s instead of an \\(n \\times p\\) matrix, our posteriors should be: \\(\\mu_\\beta^{*} = \\bar{y}\\) \\(a_1 = \\frac{n-1}{2}\\) \\(b_1 = \\frac{(n-1)s^2}{2}\\) \\(c = y^{\\mathrm{\\scriptscriptstyle T}}y - m_1^{\\mathrm{\\scriptscriptstyle T}}M_1m_1 = y^{\\mathrm{\\scriptscriptstyle T}}(I - P_x)y = (n-1)s^2\\) 3.3 Extension to Divide &amp; Conquer Algorithm While maintaining the frameworks we established in parts 1 and 2, let’s now additionally assume n is so large that we are unable to store or load \\(y\\) or \\(X\\) into our CPU to carry out computations. As a result, we divide the data set into \\(K\\) mutually exclusive and exhaustive subsets, each comprising a manageable number of points. Let \\(y_k\\) denote the \\(q_k \\times 1\\) subvector of \\(y\\) and \\(X_k\\) be the \\(q_k \\times p\\) submatrix of \\(X\\) in subset \\(k\\), such that \\(q_k &gt; p\\) and is small enough so that we can now fit the desired model on \\(\\{y_k, X_k\\}\\). It is shown below that it is possible to still compute \\(a_1\\), \\(b_1\\), \\(M_1\\) and \\(m_1\\) wihout ever having to store or compute with \\(y\\) or \\(X\\), but with quantities computed using only the subsets \\(\\{y_k, X_k\\}\\), for \\(k\\) = 1,2,…,\\(K\\). Starting with a simple example where \\(K\\) = 2, and where we are taking the assumptions made in parts 1 and 2 into consideration, we note that: \\(\\bar{y_1} = \\frac{\\sum_{i=1}^{q_1}y_{1i}}{q_1}\\) \\(\\bar{y_2} = \\frac{\\sum_{i=1}^{q_2}y_{2i}}{q_2}\\) This implies that: \\(\\bar{y} = \\frac{q_1\\bar{y_1} + q_2\\bar{y2}}{n}\\), where \\(q_1+q_2 = n\\) Extending this to k subsets: \\(\\bar{y} = \\frac{\\sum_{i=1}^K{q_i\\bar{y_i}}}{n}\\), where \\(\\sum_{i=1}^k{q_i} = n\\) Similarly, \\(s_1^2 = \\sum_{i=1}^{q_1}\\frac{({y_{1i}-\\bar{y_1}})^2}{q_1}\\) \\(s_2^2 = \\sum_{i=1}^{q_2}\\frac{({y_{2i}-\\bar{y_2}})^2}{q_2}\\) This implies that: \\(s^2 = \\frac{(q_1-1)s_1^2 + (q_2-1)s_2^2}{q_1+q_2-1}\\), where \\(q_1+q_2 = n\\) Extending this to k subsets: \\(s^2 = \\frac{\\sum_{i=1}^k(q_i -1)s_i^2}{\\sum_{i=1}^{k}{q_i}}\\), where \\(\\sum_{i=1}^{k}{q_i-1} = n\\) Applying what we already derived in the previous parts, we can see that for \\(i=1,2\\),: \\(\\mu_{i\\beta}^* = \\bar{y_i}\\) \\(a_{i1}^ = \\frac{q_i- 1}{2}\\) \\(b_{i1}^{*} = \\frac{(q_i - 1)s_i^2}{2}\\) \\(c_{i1}= (q_i -1)s_i^2\\) We also know that the posteriors for the full data set are: \\(\\mu_\\beta^{*} = \\bar{y}\\) \\(a_1 = \\frac{n-1}{2}\\) \\(b_1 = \\frac{(n-1)s^2}{2}\\) \\(c = y^{\\mathrm{\\scriptscriptstyle T}}y - m_1^{\\mathrm{\\scriptscriptstyle T}}M_1m_1 = y^{\\mathrm{\\scriptscriptstyle T}}(I - P_x)y = (n-1)s^2\\) We can see that the posteriors for the full data set can be expressed as functions of the posteriors of the subsets \\((k = 1, 2)\\), \\(\\mu_\\beta^{*} = \\bar{y} = \\frac{q_1\\bar{y_1} + q_2\\bar{y2}}{n}\\) \\(a_1 = \\frac{q_1 + q_2-1}{2}\\) \\(b_1 = \\frac{(q_1 - 1)s_1^2 + (q_2 -1)s_2^2}{2}\\) \\(c = (q_1 - 1)s_1^2 + (q_2 -1)s_2^2\\) which proves that it is possible to still compute \\(a_1\\), \\(b_1\\), \\(M_1\\) and \\(m_1\\) without ever having to store or compute with \\(y\\) or \\(X\\), but with quantities computed using only the subsets (for \\(k = 1,2\\)). "],["forward-filtering-backward-sampling---shared-variance.html", "Chapter 4 Forward Filtering Backward Sampling - Shared Variance 4.1 Background 4.2 Derivation of the Forward Filter 4.3 Derivation of the Backwards Sampling", " Chapter 4 Forward Filtering Backward Sampling - Shared Variance This chapter discusses the Dynamic Linear Model with a scale factor for the variance shared across time and its derivations at each step. The approach taken in this chapter is borrowed from West and Harrison (1997), with some details derived from Petris et al (2009). The solution we take to estimate the parameters of this model is utilized via Forward Filtering Backward Sampling. For full generality and to maintain a multivariate normal system in both the data and parameter matrices, we assume all \\(Y_{t} \\in \\mathbb{R}^{n}\\), \\(\\beta_{t} \\in \\mathbb{R}^{p}\\), and \\(t \\in \\{1,\\ldots,T\\}\\) for some integer \\(T\\). 4.1 Background The model we are concerned with studying is a class of time-varying models called the Dynamic Linear Model. The setup for the equation follows: \\[\\begin{eqnarray*} Y_{t}\\vert\\beta_{t}, \\sigma^{2} &amp;\\sim&amp; N(F_{t}^{\\mathrm{\\scriptscriptstyle T}}\\beta_{t}, \\sigma^{2}V_{t})\\\\ \\beta_{t}\\vert \\beta_{t-1},\\sigma^{2} &amp;\\sim&amp; N(G_{t}\\beta_{t-1}, \\sigma^{2}W_{t})\\\\ \\sigma^{-2} &amp;\\sim&amp; \\Gamma(a_{t-1},b_{t-1})\\\\ \\beta_{t-1}\\vert \\sigma^{2} &amp;\\sim&amp; N(m_{t-1}, \\sigma^{2}C_{t-1})\\\\ \\end{eqnarray*}\\] Alternatively, using Normal-Inverse Gamma notation, where, if \\(\\sigma^{-2} \\sim \\Gamma(a_{t-1},b_{t-1})\\), \\(\\sigma^{2} \\sim IG(a_{t-1},b_{t-1})\\), where \\(IG\\) denotes an inverse Gamma distribution, we may write the above set of equations as the following: \\[\\begin{eqnarray*} Y_{t},\\sigma^{2}\\vert \\beta_{t} &amp;\\sim&amp; NIG(F_{t}^{\\mathrm{\\scriptscriptstyle T}}\\beta_{t}, V_{t}, a_{t-1}, b_{t-1})\\\\ \\beta_{t},\\sigma^{2}\\vert \\beta_{t-1} &amp;\\sim&amp; NIG(G_{t}\\beta_{t-1}, W_{t}, a_{t-1}, b_{t-1})\\\\ \\beta_{t-1},\\sigma^{2} &amp;\\sim&amp; NIG(m_{t-1}, C_{t-1}, a_{t-1}, b_{t-1}) \\end{eqnarray*}\\] The task is to acquire estimates for \\(\\beta_{0,\\ldots,T}\\) and \\(\\sigma^{2}\\). This task may be divided into the forward filter and backwards sampling steps (collectively referred to as the Forward Filter-Backwards Sampling (FFBS) algorithm): The forward filter to acquire sequential estimates, and the backwards sampling step to retroactively “smooth” our initial estimates given estimates at the last time stamp. We are given a set of observations \\(Y_{t,j}\\), and known parameters \\(F_{t}\\), \\(G_{t}\\), \\(V_{t}\\), \\(W_{t}\\), and \\(n_{t-1}\\), although Frankenburg and Banerjee also apply FFBS to cases where \\(F_{t}\\) and \\(G_{t}\\) are not pre-specified. 4.2 Derivation of the Forward Filter We proceed for some arbitrary \\(t\\): \\[\\begin{eqnarray*} \\beta_{t} &amp;=&amp; G_{t}\\beta_{t-1} + \\omega_{t}, \\omega_{t} \\sim N(0, \\sigma^{2}W_{t})\\\\ \\beta_{t}\\vert \\sigma^{2} &amp;\\sim&amp; N(G_{t}m_{t-1}, \\sigma^{2}(G_{t}C_{t-1}G_{t}^{\\mathrm{\\scriptscriptstyle T}} + W_{t}))\\\\ \\end{eqnarray*}\\] Now, let \\(m^{*}_{t} = G_{t}m_{t-1}\\) and \\(R_{t} = G_{t}C_{t-1}G_{t}^{\\mathrm{\\scriptscriptstyle T}} + W_{t}\\). We then have: \\[\\begin{eqnarray*} Y_{t} &amp;=&amp; F_{t}^{\\mathrm{\\scriptscriptstyle T}}\\beta_{t} + \\nu_{t}, \\nu_{t}\\sim N(0, \\sigma^{2}V_{t})\\\\ Y_{t}\\vert \\sigma^{2} &amp;\\sim&amp; N(F_{t}^{\\mathrm{\\scriptscriptstyle T}}m^{*}_{t}, \\sigma^{2}(F_{t}^{\\mathrm{\\scriptscriptstyle T}}R_{t}F_{t} + V_{t})) \\end{eqnarray*}\\] Since \\(\\sigma^{2} \\sim IG(a_{t-1},b_{t-1})\\), we marginalize it out of \\(Y_{t}\\vert \\sigma^{2}\\) to get \\[\\begin{eqnarray*} Y_{t} &amp;\\sim&amp; T_{2a_{t-1}}(F_{t}^{\\mathrm{\\scriptscriptstyle T}}m^{*}_{t}, \\frac{b_{t-1}}{a_{t-1}}(F_{t}^{\\mathrm{\\scriptscriptstyle T}}R_{t}F_{t} + V_{t})) \\end{eqnarray*}\\] We now have the apparatus needed to compute the sequential posterior \\(\\beta_{t}\\vert Y_{t}\\) and \\(\\sigma^{2}\\vert Y_{t}\\): 4.2.1 Deriving \\(\\beta_{t}\\vert Y_{t}\\) \\[\\begin{eqnarray*} p(\\beta_{t} \\vert Y_{t}, \\sigma^{2}) &amp;\\propto&amp; p(\\beta_{t}, Y_{t}\\vert \\sigma^{2})\\\\ &amp;\\propto&amp; p(Y_{t}\\vert \\beta_{t},\\sigma^{2})p(\\beta_{t}\\vert \\sigma^{2})\\\\ &amp;\\propto&amp; \\sigma^{-n}\\exp(-\\frac{1}{2\\sigma^{2}}(y_{t} - F_{t}^{\\mathrm{\\scriptscriptstyle T}}\\beta_{t})^{\\mathrm{\\scriptscriptstyle T}}V_{t}^{-1}(y_{t} - F_{t}^{\\mathrm{\\scriptscriptstyle T}}\\beta_{t}))\\sigma^{-p}\\exp(-\\frac{1}{2\\sigma^{2}}(\\beta_{t} - m^{*}_{t})^{\\mathrm{\\scriptscriptstyle T}}R_{t}^{-1}(\\beta_{t} - m^{*}_{t}))\\\\ &amp;\\propto&amp; \\sigma^{-(n+p)}\\exp(-\\frac{1}{2\\sigma^{2}}[(y_{t} - F_{t}^{\\mathrm{\\scriptscriptstyle T}}\\beta_{t})^{\\mathrm{\\scriptscriptstyle T}}V_{t}^{-1}(y_{t} - F_{t}^{\\mathrm{\\scriptscriptstyle T}}\\beta_{t}) + (\\beta_{t} - m^{*}_{t})^{\\mathrm{\\scriptscriptstyle T}}R_{t}^{-1}(\\beta_{t} - m^{*}_{t})])\\\\ \\end{eqnarray*}\\] Note next that \\[\\begin{eqnarray*} \\begin{bmatrix}Y_{t}\\\\ \\beta_{t}\\end{bmatrix}\\vert \\sigma^{2} &amp;\\sim&amp; N\\left(\\begin{bmatrix}F_{t}^{\\mathrm{\\scriptscriptstyle T}}m^{*}_{t}\\\\ m^{*}_{t}\\end{bmatrix},\\sigma^{2}\\begin{bmatrix}F_{t}^{\\mathrm{\\scriptscriptstyle T}}R_{t}F_{t} + V_{t} &amp; F_{t}^{\\mathrm{\\scriptscriptstyle T}}R_{t}\\\\ R_{t}F_{t} &amp; R_{t}\\end{bmatrix}\\right) \\end{eqnarray*}\\] with the cross-terms \\(\\mathrm{Cov}(Y_{t},\\beta_{t}) = \\mathrm{Cov}(F_{t}^{\\mathrm{\\scriptscriptstyle T}}\\beta_{t} + \\nu_{t},\\beta_{t}) = F_{t}^{\\mathrm{\\scriptscriptstyle T}}\\mathrm{Cov}(\\beta_{t}, \\beta_{t}) = F_{t}^{\\mathrm{\\scriptscriptstyle T}}R_{t}\\). Since, for the following block-normal system \\[\\begin{eqnarray*} \\begin{bmatrix}x_{1}\\\\ x_{2}\\end{bmatrix} &amp;\\sim&amp; N\\left(\\begin{bmatrix}\\mu_{1}\\\\ \\mu_{2}\\end{bmatrix}, \\begin{bmatrix}\\Sigma_{11} &amp; \\Sigma_{12}\\\\ \\Sigma_{21} &amp; \\Sigma_{22}\\end{bmatrix}\\right) \\end{eqnarray*}\\] we have \\[\\begin{eqnarray*} x_{2}\\vert x_{1} &amp;\\sim&amp; N(\\mu_{2} + \\Sigma_{21}\\Sigma_{11}^{-1}(x_{1} - \\mu_{1}), \\Sigma_{22} - \\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12}) \\end{eqnarray*}\\] (The derivation of the density of \\(x_{2}\\vert x_{1}\\) can be found in the Appendix.) We arrive at, \\[\\begin{eqnarray*} \\beta_{t}\\vert \\sigma^{2},Y_{t} &amp;\\sim&amp; N(m_{t}^{*} + R_{t}F_{t}(F_{t}^{\\mathrm{\\scriptscriptstyle T}}R_{t}F_{t} + V_{t})^{-1}(Y_{t} - F_{t}^{\\mathrm{\\scriptscriptstyle T}}m_{t}^{*}), R_{t} - R_{t}F_{t}(F_{t}^{\\mathrm{\\scriptscriptstyle T}}R_{t}F_{t} + V_{t})^{-1}F_{t}^{\\mathrm{\\scriptscriptstyle T}}R_{t})\\\\ &amp;\\sim&amp; N(m_{t}^{*} + R_{t}F_{t}Q_{t}^{-1}(Y_{t} - F_{t}^{\\mathrm{\\scriptscriptstyle T}}m_{t}^{*}), R_{t} - R_{t}F_{t}Q_{t}^{-1}F_{t}^{\\mathrm{\\scriptscriptstyle T}}R_{t}) \\end{eqnarray*}\\] where \\(Q_{t} = F_{t}^{\\mathrm{\\scriptscriptstyle T}}R_{t}F_{t} + V_{t}\\). (Note that Petris’s expression for the variance suffers from a typo; to see this, simply take their \\(\\widetilde{C}_{t}^{\\mathrm{\\scriptscriptstyle T}}\\).) 4.2.2 Deriving \\(\\sigma^{2}\\vert Y_{t}\\) We next deduce the density of \\(\\sigma^{2}\\vert Y_{t}\\). Note before we begin that since \\(Y_{t} \\sim T_{2a_{t-1}}(F_{t}^{\\mathrm{\\scriptscriptstyle T}}m^{*}_{t}, Q_{t}) = \\int NIG_{Y_{t}}(F_{t}^{\\mathrm{\\scriptscriptstyle T}}m^{*}_{t}, Q_{t}, a_{t-1}, b_{t-1})d\\sigma^{2}\\), we can write \\(Y_{t}\\vert \\sigma^{2} \\sim N(F_{t}m^{*}_{t}, \\sigma^{2}Q_{t})\\). Hence: \\[\\begin{eqnarray*} p(\\sigma^{2}\\vert Y_{t}) &amp;\\propto&amp; p(Y_{t}\\vert \\sigma^{2})p(\\sigma^{2})\\\\ &amp;\\propto&amp; \\sigma^{-n}\\exp(-\\frac{1}{2\\sigma^{2}}(y_{t} - F_{t}^{\\mathrm{\\scriptscriptstyle T}}m^{*}_{t})^{\\mathrm{\\scriptscriptstyle T}}Q_{t}^{-1}(y_{t} - F_{t}^{\\mathrm{\\scriptscriptstyle T}}m^{*}_{t}))\\sigma^{-2(a_{t-1} + 1)}\\exp(-b_{t-1}\\sigma^{-2})\\\\ &amp;\\propto&amp; \\sigma^{-2(a_{t-1} + \\frac{n}{2} + 1)}\\exp(-\\sigma^{-2}[\\frac{1}{2}(y_{t} - F_{t}^{\\mathrm{\\scriptscriptstyle T}}m^{*}_{t})^{\\mathrm{\\scriptscriptstyle T}}Q_{t}^{-1}(y_{t} - F_{t}^{\\mathrm{\\scriptscriptstyle T}}m^{*}_{t}) + b_{t-1}]) \\end{eqnarray*}\\] We conclude that \\(\\sigma^{-2}\\vert Y_{t} \\sim \\Gamma(a_{t},b_{t})\\), where \\(a_{t} = a_{t-1} + \\frac{n}{2}\\) and \\(b_{t} = b_{t-1} + \\frac{1}{2}(y_{t} - F_{t}^{\\mathrm{\\scriptscriptstyle T}}m^{*}_{t})^{\\mathrm{\\scriptscriptstyle T}}Q_{t}^{-1}(y_{t} - F_{t}^{\\mathrm{\\scriptscriptstyle T}}m^{*}_{t})\\). This gives us the set of updating equations according to Petris Proposition 4.1. 4.2.3 Commentary Note that we have derived the forward filtering step for the set of equations for time \\(t\\) given the parameters for the distributions at time \\(t-1\\). Hence the equation’s setup is Markovian, i.e. the state of this set of equations only depends on that of the preceding time point. Nevertheless, applications where the forward filter’s equations propagate from an initial time point \\(t=0\\) are written so that the dependence of the parameters’ values \\(\\beta_{t}\\) and \\(\\sigma^{2}\\) on the data up to time \\(t-1\\) or time \\(t\\) are made explicit. Specifically, letting \\(D_{t} = \\{Y_{\\tau}\\}_{\\tau=1,\\ldots,t}\\), we may write the set of equations in our setup as: \\[\\begin{eqnarray*} Y_{t},\\sigma^{2}\\vert \\beta_{t},D_{t-1} &amp;\\sim&amp; NIG(F_{t}^{\\mathrm{\\scriptscriptstyle T}}\\beta_{t}, V_{t}, a_{t-1}, b_{t-1})\\\\ \\beta_{t},\\sigma^{2}\\vert \\beta_{t-1},D_{t-1} &amp;\\sim&amp; NIG(G_{t}\\beta_{t-1}, W_{t}, a_{t-1}, b_{t-1})\\\\ \\beta_{t-1},\\sigma^{2}\\vert D_{t-1} &amp;\\sim&amp; NIG(m_{t-1}, C_{t-1}, a_{t-1}, b_{t-1}) \\end{eqnarray*}\\] and the sequential posteriors we have derived, \\(\\beta_{t}\\vert Y_{t}\\) and \\(\\sigma^{2}\\vert Y_{t}\\), as \\(\\beta_{t}\\vert D_{t}\\) and \\(\\sigma^{2} \\vert D_{t}\\) respectively. 4.3 Derivation of the Backwards Sampling Now that we have the parameters \\(\\{\\theta_{t},\\phi\\vert D_{t}\\}_{t=1,\\ldots,T}\\), we would like to work backwards and derive \\(\\{\\theta_{t},\\phi\\vert \\theta_{t+1}, D_{T}\\}_{t=1,\\ldots,T-1}\\) to smooth our initial variable estimates: \\[\\begin{eqnarray*} p(\\theta_{t}\\vert \\theta_{(t+1):T},\\sigma^{2},D_{T}) &amp;=&amp; p(\\theta_{t}\\vert \\theta_{t+1},\\sigma^{2},D_{t})\\\\ &amp;=&amp; p(\\theta_{t}\\vert \\theta_{t+1},\\sigma^{2},D_{t})\\\\ &amp;=&amp; \\frac{p(\\theta_{t+1}\\vert \\theta_{t},D_{t})p(\\theta_{t}\\vert D_{t})}{p(\\theta_{t+1}\\vert D_{t})}\\\\ &amp;\\propto&amp; p(\\theta_{t+1}\\vert \\theta_{t},D_{t})p(\\theta_{t}\\vert D_{t})\\\\ &amp;\\propto&amp; \\exp\\left(-\\frac{1}{2\\sigma^{2}}\\left[(\\theta_{t+1} - G_{t+1}\\theta_{t})^{\\mathrm{\\scriptscriptstyle T}}W_{t+1}^{-1}(\\theta_{t+1} - G_{t+1}\\theta_{t})\\right.\\right.\\\\ &amp;&amp;\\left.\\left.+ (\\theta_{t} - m_{t})^{\\mathrm{\\scriptscriptstyle T}}C_{t}^{-1}(\\theta_{t} - m_{t})\\right]\\right)\\\\ &amp;\\propto&amp; \\exp\\left(-\\frac{1}{2\\sigma^{2}}\\left[\\theta_{t+1}^{\\mathrm{\\scriptscriptstyle T}}W_{t+1}^{-1}\\theta_{t+1} - 2\\theta_{t+1}^{\\mathrm{\\scriptscriptstyle T}}W_{t+1}^{-1}G_{t+1}\\theta_{t} + \\theta_{t}^{\\mathrm{\\scriptscriptstyle T}}G_{t+1}^{\\mathrm{\\scriptscriptstyle T}}W_{t+1}^{-1}G_{t+1}\\theta_{t}\\right.\\right.\\\\ &amp;&amp;\\left.\\left.+\\theta_{t}^{\\mathrm{\\scriptscriptstyle T}}C_{t}^{-1}\\theta_{t} - 2m_{t}^{\\mathrm{\\scriptscriptstyle T}}C_{t}^{-1}\\theta_{t} + m_{t}^{\\mathrm{\\scriptscriptstyle T}}C_{t}^{-1}m_{t}\\right]\\right)\\\\ &amp;\\propto&amp; \\exp\\left(-\\frac{1}{2\\sigma^{2}}\\left[\\theta_{t}^{\\mathrm{\\scriptscriptstyle T}}(G_{t+1}^{\\mathrm{\\scriptscriptstyle T}}W_{t+1}^{-1}G_{t+1} + C_{t}^{-1})\\theta_{t} - 2(C_{t}^{-1}m_{t} + G_{t+1}^{\\mathrm{\\scriptscriptstyle T}}W_{t+1}^{-1}\\theta_{t+1})^{\\mathrm{\\scriptscriptstyle T}}\\theta_{t}\\right]\\right)\\\\ \\theta_{t}\\vert\\theta_{t+1},\\sigma^{2},D_{T} &amp;\\sim&amp; N\\left((G_{t+1}^{\\mathrm{\\scriptscriptstyle T}}W_{t+1}^{-1}G_{t+1} + C_{t}^{-1})^{-1}(C_{t}^{-1}m_{t} + G_{t+1}^{\\mathrm{\\scriptscriptstyle T}}W_{t+1}^{-1}\\theta_{t+1}),\\right.\\\\ &amp;&amp;\\left.\\sigma^{-2}(G_{t+1}^{\\mathrm{\\scriptscriptstyle T}}W_{t+1}^{-1}G_{t+1} + C_{t}^{-1})^{-1}\\right)\\\\ &amp;\\sim&amp; N\\left(m_{t} - C_{t}G_{t+1}^{\\mathrm{\\scriptscriptstyle T}}(W_{t+1} + G_{t+1}C_{t}G_{t+1}^{\\mathrm{\\scriptscriptstyle T}})^{-1}G_{t+1}m_{t} + C_{t}G_{t+1}^{\\mathrm{\\scriptscriptstyle T}}W_{t+1}^{-1}\\theta_{t+1}\\right.\\\\ &amp;&amp;\\left.- C_{t}G_{t+1}^{\\mathrm{\\scriptscriptstyle T}}(W_{t+1} + G_{t+1}C_{t}G_{t+1}^{\\mathrm{\\scriptscriptstyle T}})^{-1}G_{t+1}C_{t}G_{t+1}^{\\mathrm{\\scriptscriptstyle T}}W_{t+1}^{-1}\\theta_{t+1},\\right.\\\\ &amp;&amp;\\left.C_{t} - C_{t}G_{t+1}^{\\mathrm{\\scriptscriptstyle T}}(W_{t+1} + G_{t+1}C_{t}G_{t+1}^{\\mathrm{\\scriptscriptstyle T}})^{-1}G_{t+1}C_{t}\\right)\\\\ &amp;\\sim&amp; N\\left(m_{t} - C_{t}G_{t+1}^{\\mathrm{\\scriptscriptstyle T}}R_{t+1}^{-1}G_{t+1}m_{t} + C_{t}G_{t+1}^{\\mathrm{\\scriptscriptstyle T}}W_{t+1}^{-1}\\theta_{t+1}\\right.\\\\ &amp;&amp;\\left. - C_{t}G_{t+1}^{\\mathrm{\\scriptscriptstyle T}}R_{t+1}^{-1}G_{t+1}C_{t}G_{t+1}^{\\mathrm{\\scriptscriptstyle T}}W_{t+1}^{-1}\\theta_{t+1},\\right.\\\\ &amp;&amp;\\left.C_{t} - C_{t}G_{t+1}^{\\mathrm{\\scriptscriptstyle T}}R_{t+1}^{-1}G_{t+1}C_{t}\\right)\\\\ \\end{eqnarray*}\\] Notice that \\[\\begin{eqnarray*} C_{t}G_{t+1}^{\\mathrm{\\scriptscriptstyle T}}R_{t+1}^{-1}G_{t+1}C_{t}G_{t+1}^{\\mathrm{\\scriptscriptstyle T}}W_{t+1}^{-1}\\theta_{t+1} &amp;=&amp; C_{t}G_{t+1}^{\\mathrm{\\scriptscriptstyle T}}R_{t+1}^{-1}(G_{t+1}C_{t}G_{t+1}^{\\mathrm{\\scriptscriptstyle T}} + W_{t+1} - W_{t+1})W_{t+1}^{-1}\\theta_{t+1}\\\\ &amp;=&amp; C_{t}G_{t+1}^{\\mathrm{\\scriptscriptstyle T}}R_{t+1}^{-1}(R_{t+1} - W_{t+1})W_{t+1}^{-1}\\theta_{t+1}\\\\ &amp;=&amp; C_{t}G_{t+1}^{\\mathrm{\\scriptscriptstyle T}}W_{t+1}^{-1}\\theta_{t+1} - C_{t}G_{t+1}^{\\mathrm{\\scriptscriptstyle T}}R_{t+1}^{-1}\\theta_{t+1} \\end{eqnarray*}\\] Hence, \\[\\begin{eqnarray*} \\theta_{t}\\vert\\theta_{t+1},\\sigma^{2},D_{T} &amp;\\sim&amp; N\\left(m_{t} - C_{t}G_{t+1}^{\\mathrm{\\scriptscriptstyle T}}R_{t+1}^{-1}G_{t+1}m_{t} + C_{t}G_{t+1}^{\\mathrm{\\scriptscriptstyle T}}W_{t+1}^{-1}\\theta_{t+1}\\right.\\\\ &amp;&amp;\\left. - C_{t}G_{t+1}^{\\mathrm{\\scriptscriptstyle T}}W_{t+1}^{-1}\\theta_{t+1} + C_{t}G_{t+1}^{\\mathrm{\\scriptscriptstyle T}}R_{t+1}^{-1}\\theta_{t+1},\\right.\\\\ &amp;&amp;\\left.C_{t} - C_{t}G_{t+1}^{\\mathrm{\\scriptscriptstyle T}}R_{t+1}^{-1}G_{t+1}C_{t}\\right)\\\\ &amp;\\sim&amp; N(m_{t} + C_{t}G_{t+1}^{\\mathrm{\\scriptscriptstyle T}}R_{t+1}^{-1}(\\theta_{t+1} - a_{t+1}), C_{t} - C_{t}G_{t+1}^{\\mathrm{\\scriptscriptstyle T}}R_{t+1}^{-1}G_{t+1}C_{t}) \\end{eqnarray*}\\] "],["appendix.html", "Appendix Deriving \\(x_{2} \\vert x_{1}\\) when \\((x_{1} x_{2})^{\\mathrm{\\scriptscriptstyle T}}\\) is a block-normal multivariate random variable", " Appendix Deriving \\(x_{2} \\vert x_{1}\\) when \\((x_{1} x_{2})^{\\mathrm{\\scriptscriptstyle T}}\\) is a block-normal multivariate random variable Recall our block normal system: \\[\\begin{eqnarray*} \\begin{bmatrix}x_{1}\\\\ x_{2}\\end{bmatrix} &amp;\\sim&amp; N\\left(\\begin{bmatrix}\\mu_{1}\\\\ \\mu_{2}\\end{bmatrix}, \\begin{bmatrix}\\Sigma_{11} &amp; \\Sigma_{12}\\\\ \\Sigma_{21} &amp; \\Sigma_{22}\\end{bmatrix}\\right) \\end{eqnarray*}\\] Assuming that \\(\\Sigma_{11}\\) is invertible (though unless \\(x_{1}\\) contains degenerate terms, we have nothing to worry about), we then have \\[\\begin{eqnarray*} p(x_{2}\\vert x_{1}) &amp;=&amp; \\frac{p(x_{1},x_{2})}{p(x_{1})}\\\\ &amp;\\propto&amp; \\exp\\left(-\\frac{1}{2}\\left[\\begin{bmatrix}x_{1} - \\mu_{1}\\\\ x_{2} - \\mu_{2}\\end{bmatrix}^{\\mathrm{\\scriptscriptstyle T}}\\begin{bmatrix}\\Sigma_{11} &amp; \\Sigma_{12}\\\\ \\Sigma_{21} &amp; \\Sigma_{22}\\end{bmatrix}^{-1}\\begin{bmatrix}x_{1} - \\mu_{1}\\\\ x_{2} - \\mu_{2}\\end{bmatrix} - (x_{1} - \\mu_{1})^{\\mathrm{\\scriptscriptstyle T}}\\Sigma_{11}^{-1}(x_{1} - \\mu_{1})\\right]\\right) \\end{eqnarray*}\\] Now, one of the expressions we may use to invert the block covariance matrix is: \\[\\begin{eqnarray*} \\begin{bmatrix}\\Sigma_{11} &amp; \\Sigma_{12}\\\\ \\Sigma_{21} &amp; \\Sigma_{22}\\end{bmatrix}^{-1} &amp;=&amp; \\begin{bmatrix}\\Sigma_{11}^{-1} + \\Sigma_{11}^{-1}\\Sigma_{12}(\\Sigma_{22} - \\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12})^{-1}\\Sigma_{21}\\Sigma_{11}^{-1} &amp; -\\Sigma_{11}^{-1}\\Sigma_{12}(\\Sigma_{22} - \\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12})^{-1}\\\\ -(\\Sigma_{22} - \\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12})^{-1}\\Sigma_{21}\\Sigma_{11}^{-1} &amp; (\\Sigma_{22} - \\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12})^{-1}\\end{bmatrix} \\end{eqnarray*}\\] Hence, \\[\\begin{eqnarray*} p(x_{2}\\vert x_{1}) &amp;\\propto&amp; \\exp\\left(-\\frac{1}{2}\\left[(x_{1} - \\mu_{1})^{\\mathrm{\\scriptscriptstyle T}}\\Sigma_{11}^{-1}\\Sigma_{12}(\\Sigma_{22} - \\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12})^{-1}\\Sigma_{21}\\Sigma_{11}^{-1}(x_{1} - \\mu_{1})\\right.\\right.\\\\ &amp;&amp;\\left.\\left.- 2(x_{1} - \\mu_{1})\\Sigma_{11}^{-1}\\Sigma_{12}(\\Sigma_{22} - \\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12})^{-1}(x_{2} - \\mu_{2})\\right.\\right.\\\\ &amp;&amp;\\left.\\left. + (x_{2} - \\mu_{2})^{\\mathrm{\\scriptscriptstyle T}}(\\Sigma_{22} - \\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12})^{-1}(x_{2} - \\mu_{2})\\right]\\right)\\\\ &amp;\\propto&amp; \\exp\\left(-\\frac{1}{2}\\left[((x_{2} - \\mu_{2}) - \\Sigma_{21}\\Sigma_{11}^{-1}(x_{1} - \\mu_{1}))^{\\mathrm{\\scriptscriptstyle T}}(\\Sigma_{22} - \\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12})^{-1}\\right.\\right.\\\\ &amp;&amp;\\left.\\left.((x_{2} - \\mu_{2}) - \\Sigma_{21}\\Sigma_{11}^{-1}(x_{1} - \\mu_{1}))\\right]\\right) \\end{eqnarray*}\\] i.e. \\(x_{2} \\vert x_{1} \\sim N(\\mu_{2} + \\Sigma_{21}\\Sigma_{11}^{-1}(x_{1} - \\mu_{1}), \\Sigma_{22} - \\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12})\\). \\(\\square\\) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
