[["posteriors-using-sufficient-statistics.html", "Chapter 3 Posteriors Using Sufficient Statistics 3.1 Methods 3.2 Posterior From Improper Priors 3.3 Extension to Divide &amp; Conquer Algorithm", " Chapter 3 Posteriors Using Sufficient Statistics Recall the following Bayesian (hierarchical) linear regression model from part 1: \\[\\begin{equation*} NIG(M_0, m_0, a_0, b_0) \\times N(y \\mid X\\beta, \\sigma^2 I_n) \\tag{3.1} \\end{equation*}\\] where \\(y = (y_1, y_2, ..., y_n)^{\\mathrm{\\scriptscriptstyle T}}\\) is an \\(n \\times 1\\) vector of outcomes and \\(X\\) is a fixed \\(n \\times p\\) matrix of predictors with full column rank. Suppose that due to privacy reasons, instead of \\(y = (y_1, y_2, ..., y_n)^{\\mathrm{\\scriptscriptstyle T}}\\) we are given data that only consists of the sufficient statistics, \\(s^2\\) and \\(\\bar{y}\\). Further suppose that we are working with the following simpler model: \\[\\begin{equation*} y = 1_n \\beta_0 + \\epsilon \\tag{3.2} \\end{equation*}\\] How do we obtain \\(p(\\beta_0, \\sigma^2 \\mid y)\\) using this data set? 3.1 Methods We can use the factorization theorem to factor the normal distribution’s pdf into a function of the sufficient statistics, \\(s^2\\) and \\(\\bar{y}.\\) \\[\\begin{equation*} \\begin{split} f(y \\mid \\beta_0, \\sigma^2) &amp;= \\Pi_{i=1}^{n} (2\\pi\\sigma^2)^{-\\frac{1}{2}} \\exp\\{-\\frac{1}{2\\sigma^2}(y_i-\\beta_0)^2\\}\\\\ &amp;= (2\\pi\\sigma^2)^{-\\frac{n}{2}} \\exp\\{-\\frac{1}{2\\sigma^2}\\Sigma(y_i - \\beta_0)^2\\} \\\\ &amp;= (2\\pi\\sigma^2)^{-\\frac{n}{2}} \\exp\\{-\\frac{1}{2\\sigma^2}\\Sigma(y_i - \\bar{y} + \\bar{y} - \\beta_0)^2\\} \\\\ &amp;= (2\\pi\\sigma^2)^{-\\frac{n}{2}} \\exp\\{-\\frac{1}{2\\sigma^2}\\Sigma[(y_i - \\bar{y})^2 + (\\bar{y} - \\beta_0)^2]\\} \\\\ &amp;= (2\\pi\\sigma^2)^{-\\frac{n}{2}} \\exp\\{-\\frac{1}{2\\sigma^2}[(n-1)s^2 + n(\\bar{y} - \\beta_0)^2]\\} \\end{split} \\tag{3.3} \\end{equation*}\\] We have now successfully expressed our likelihood as a function of the sufficient statistics \\(s^2\\) and \\(\\bar{x}\\) and can use this factorized likelihood along with Bayes’ Rule to compute \\(p(\\beta_0, \\sigma^2)\\). \\[\\begin{equation*} \\begin{split} p(\\beta_0, \\sigma^2 \\mid y) =&amp; p(\\beta_0 \\mid \\mu_\\beta, \\sigma^2) \\times p(\\sigma^2 \\mid a_0, b_0) \\times p(y \\mid \\beta_0, \\sigma^2) \\\\ =&amp; N(\\beta_0 \\mid \\mu_\\beta, \\sigma^2) \\times IG(\\sigma^2 \\mid a_0,b_0) \\times N(y \\mid \\beta_0, \\sigma^2) \\\\ =&amp; (2\\pi\\sigma^2)^{-\\frac{1}{2}}\\exp\\{-\\frac{1}{2\\sigma^2}(\\beta_0 - \\mu_\\beta)^2\\} \\times \\frac{b_0^{a_0}}{\\Gamma(a_0)}(\\sigma^2)^{-a-1}\\exp\\{-\\frac{b}{\\sigma^2}\\} \\\\ &amp;\\times (2\\pi\\sigma^2)^{-\\frac{n}{2}}\\exp\\{-\\frac{1}{2\\sigma^2}[(n-1)s^2 + n(\\bar{y} - \\beta_0)^2]\\} \\\\ \\propto&amp; (\\sigma^2)^{-a_0-1} \\times (\\sigma^2)^{-\\frac{n+1}{2}} \\times \\exp\\{-\\frac{b_0}{\\sigma^2}\\} \\times \\exp\\{-\\frac{1}{2\\sigma^2}(\\beta_0 - \\mu_\\beta)^2\\} \\\\ &amp;\\times \\exp\\{-\\frac{1}{2\\sigma^2}[(n-1)s^2 + n(\\bar{y} - \\beta_0)^2]\\} \\\\ \\propto&amp; (\\sigma^2)^{-a_0-1} \\times (\\sigma^2)^{-\\frac{n+1}{2}} \\exp\\{-\\frac{b_0}{\\sigma^2}\\}\\\\ &amp;\\times \\exp\\{-\\frac{1}{2\\sigma^2}[(\\beta_0 - \\mu_{\\beta})^2 +(n-1)s^2 + n(\\bar{y}-\\beta_0)^2]\\} \\end{split} \\tag{3.4} \\end{equation*}\\] Complete the square on the last term of (3.4): \\[\\begin{equation*} \\begin{split} &amp;(\\beta_0 - \\mu_{\\beta})^2 +(n-1)s^2 + n(\\bar{y}-\\beta_0)^2 \\\\ &amp;= (n+1)\\beta_0^2 - 2(\\mu_{\\beta} + n\\bar{y})\\beta_0 + n\\bar{y}^2 + \\mu_\\beta^2 + (n-1)s^2 \\\\ &amp;= (n+1)(\\beta_0 - \\frac{\\mu_{\\beta} + n\\bar{y}}{n+1})^2 + n\\bar{y}^2 + \\mu_\\beta^2 + (n-1)s^2 - \\frac{(\\mu_\\beta + n\\bar{y})^2}{n+1}\\\\ &amp;= (n+1)(\\beta_0 - \\frac{\\mu_{\\beta} + n\\bar{y}}{n+1})^2 + y^{\\mathrm{\\scriptscriptstyle T}}\\Big(\\frac{1_n1_n^{\\mathrm{\\scriptscriptstyle T}}}{n}\\Big)y + y^{\\mathrm{\\scriptscriptstyle T}}\\Big(I - \\frac{1_n1_n^{\\mathrm{\\scriptscriptstyle T}}}{n}\\Big)y + \\mu_\\beta^2 - \\frac{(\\mu_\\beta + n\\bar{y})^2}{n+1} \\\\ &amp;= (n+1)(\\beta_0 - \\frac{\\mu_{\\beta} + n\\bar{y}}{n+1})^2 + y^{\\mathrm{\\scriptscriptstyle T}}y + \\mu_\\beta^2 - \\frac{(\\mu_\\beta + n\\bar{y})^2}{n+1} \\end{split} \\tag{3.5} \\end{equation*}\\] .ul { border: 2px solid black; } We now let: \\[\\begin{equation*} \\begin{split} \\mu_\\beta^{*} &amp;= \\frac{n\\bar{y} + \\mu_\\beta}{n+1} \\\\ c &amp;= y^{\\mathrm{\\scriptscriptstyle T}}y + \\mu_\\beta^2 - \\frac{(n\\bar{y} + \\mu_\\beta)^2}{n+1} \\end{split} \\tag{3.6} \\end{equation*}\\] and apply (3.5) and (3.6) to (3.4): \\[\\begin{equation*} \\begin{split} =&amp;(\\sigma^2)^{-a_0-1} \\times (\\sigma^2)^{-\\frac{n+1}{2}} \\times \\exp\\{-\\frac{b_0}{\\sigma^2}\\}\\\\ &amp;\\times \\exp\\{-\\frac{1}{2\\sigma^2}(n+1)(\\beta_0 - \\mu_\\beta^{*})^2\\} \\times \\exp\\{-\\frac{c}{2\\sigma^2}\\} \\\\ =&amp; (\\sigma^2)^{-a_0-1} \\times (\\sigma^2)^{-\\frac{n+1}{2}} \\times \\exp\\{-\\frac{b_0}{\\sigma^2}\\} \\times \\exp\\{-\\frac{1}{2\\sigma^{2\\ast}}(\\beta_0 - \\mu_\\beta^{*})^2\\} \\times \\exp\\{-\\frac{c}{2\\sigma^2}\\} \\\\ =&amp; (\\sigma^2)^{-a_0-1-\\frac{n}{2}} \\times \\exp\\{-\\frac{(b_0 + \\frac{c}{2})}{\\sigma^2}\\} \\times (\\sigma^2)^{-\\frac{1}{2}} \\times \\exp\\{-\\frac{1}{2\\sigma^{2\\ast}}(\\beta_0 - \\mu_\\beta^{*})^2\\} \\\\ =&amp; \\underbrace{(\\sigma^2)^{a_1-1} \\times \\exp\\{-\\frac{b_1}{\\sigma^2}\\}}_\\text{kernal of $IG(a_1, b_1)$} \\times \\underbrace{(\\sigma^2)^{\\frac{1}{2}} \\times \\exp\\{-\\frac{1}{2\\sigma^{2\\ast}}(\\beta_0 - \\mu_\\beta^{*})^2\\}}_\\text{kernal of $N(\\mu_\\beta^{\\ast}, \\sigma^{2\\ast})$} \\end{split} \\tag{3.7} \\end{equation*}\\] where: \\[\\begin{equation*} \\begin{split} \\sigma^{2*} &amp;= \\frac{\\sigma^2}{n+1}\\\\ a_1 &amp;= a_0 + \\frac{n}{2}\\\\ b_1 &amp;= b_0 + \\frac{c}{2} \\end{split} \\tag{3.8} \\end{equation*}\\] 3.2 Posterior From Improper Priors Recall that when we work with this model in a general regression setting where \\(X\\) is an \\(n \\times p\\) matrix predictors with full column rank, the posterior density is: \\[\\begin{equation*} \\begin{split} p(\\beta, \\sigma^2 \\mid y) &amp;= IG(\\sigma^2 \\mid a_1, b_1) \\times N(\\beta \\mid M_1m_1, \\sigma^2M_1) \\end{split} \\tag{3.9} \\end{equation*}\\] We end up with: \\[\\begin{equation*} \\begin{split} m_0^{*} &amp;= Mm\\\\ M_1 &amp;= (X^{\\mathrm{\\scriptscriptstyle T}}X + M_{0}^{-1})^{-1}\\\\ m_1 &amp;= X^{\\mathrm{\\scriptscriptstyle T}}y + M_{0}^{-1}m_0\\\\ a_1 &amp;= a_0 + \\frac{n}{2}\\\\ b_1 &amp;= b_0 + \\frac{c}{2}\\\\ c &amp;= y^{\\mathrm{\\scriptscriptstyle T}}y + m_0^{\\mathrm{\\scriptscriptstyle T}}M_0^{-1}m_0 - m_1^{\\mathrm{\\scriptscriptstyle T}}M_1m_1 \\end{split} \\tag{3.10} \\end{equation*}\\] If we take \\(M_0^{-1} \\rightarrow 0\\) (i.e. the null matrix), and \\(a_0 \\rightarrow -\\frac{p}{2}\\), and \\(b_0 \\rightarrow 0\\) we can see that it leads to the improper prior \\(p(\\beta, \\sigma^2) \\propto \\frac{1}{\\sigma^2}\\): \\[\\begin{equation*} \\begin{split} p(\\beta, \\sigma^2) =&amp; IG(\\sigma^2 \\mid a_0, b_0) \\times N(\\beta \\mid m_0, \\sigma^2M_0) \\\\ =&amp; \\frac{b_0^{a_0}}{\\Gamma(a_0)}(\\sigma^2)^{-a_0-1} \\exp\\{-\\frac{b_0}{\\sigma^2}\\} \\times (2\\pi)^{-p/2}\\det(\\sigma^2M_0)^{-\\frac{1}{2}}\\\\ &amp;\\exp\\{-\\frac{1}{2\\sigma^2}(\\beta - m_0)^{\\mathrm{\\scriptscriptstyle T}}\\frac{M_0^{-1}}{\\sigma^2}(\\beta - m_0)\\} \\\\ \\propto&amp; (\\sigma^2)^{-a_0-1}\\exp\\{-\\frac{b_0}{\\sigma^2}\\} \\times (\\sigma^2)^{-\\frac{p}{2}}\\exp\\{-\\frac{1}{2}(\\beta - m_0)^{\\mathrm{\\scriptscriptstyle T}}\\frac{M_0^{-1}}{\\sigma^2}(\\beta - m_0)\\} \\\\ =&amp; (\\sigma^2)^{-(\\frac{p}{2}+1})\\exp\\{-\\frac{b_0}{\\sigma^2}\\} \\times (\\sigma^2)^{-\\frac{p}{2}}\\exp\\{-\\frac{1}{2}(\\beta - m_0)^{\\mathrm{\\scriptscriptstyle T}} \\frac{0}{\\sigma^2}(\\beta - m_0)\\}\\\\ =&amp; \\frac{1}{\\sigma^2}, \\end{split} \\tag{3.11} \\end{equation*}\\] and ultimately yields a posterior distribution with: \\[\\begin{equation*} \\begin{split} m_1 &amp;= (X^{\\mathrm{\\scriptscriptstyle T}}X)^{-1}X^{\\mathrm{\\scriptscriptstyle T}}y\\\\ a_1 &amp;= \\frac{n-p}{2}\\\\ b_1 &amp;= b_0 + \\frac{c^{*}}{2}\\\\ c^{*} &amp;= y^{\\mathrm{\\scriptscriptstyle T}}y - m_1^{\\mathrm{\\scriptscriptstyle T}}M_1m_1 = y^{\\mathrm{\\scriptscriptstyle T}}(I - P_x)y = (n-p)s^2 \\end{split} \\tag{3.12} \\end{equation*}\\] where \\(P_x = X(X^{\\mathrm{\\scriptscriptstyle T}}X)^{-1}X^{\\mathrm{\\scriptscriptstyle T}}\\). If we apply this framework to (3.2), our posteriors should be: \\[\\begin{equation*} \\begin{split} \\mu_\\beta^{*} =&amp; \\bar{y}\\\\ a_1 =&amp; \\frac{n-1}{2}\\\\ b_1 =&amp; \\frac{(n-1)s^2}{2}\\\\ c =&amp; y^{\\mathrm{\\scriptscriptstyle T}}y - m_1^{\\mathrm{\\scriptscriptstyle T}}M_1m_1 = y^{\\mathrm{\\scriptscriptstyle T}}(I - P_x)y = (n-1)s^2 \\end{split} \\tag{3.13} \\end{equation*}\\] 3.3 Extension to Divide &amp; Conquer Algorithm While maintaining the frameworks we established in sections 3.1 and 3.2, let’s now additionally assume \\(n\\) is so large that we are unable to store or load \\(y\\) or \\(X\\) into our CPU to carry out computations. As a result, we divide the data set into \\(K\\) mutually exclusive and exhaustive subsets, each comprising a manageable number of points. Let \\(y_k\\) denote the \\(q_k \\times 1\\) subvector of \\(y\\) and \\(X_k\\) be the \\(q_k \\times p\\) submatrix of \\(X\\) in subset \\(k\\), such that \\(q_k &gt; p\\) and is small enough so that we can now fit the desired model on \\(\\{y_k, X_k\\}\\). It is shown below that it is possible to still compute \\(a_1\\), \\(b_1\\), \\(M_1\\) and \\(m_1\\) wihout ever having to store or compute \\(y\\) or \\(X\\), but with quantities computed using only the subsets \\(\\{y_k, X_k\\}\\), for \\(k = 1,2,...,K\\). Starting with a simple example where \\(K = 2\\), and where we are taking the assumptions made in sections 3.1 and 3.2 into consideration, we have: \\[\\begin{equation*} \\begin{split} \\bar{y_1} =&amp; \\frac{\\sum_{i=1}^{q_1}y_{1i}}{q_1}\\\\ \\bar{y_2} =&amp; \\frac{\\sum_{i=1}^{q_2}y_{2i}}{q_2} \\end{split} \\tag{3.14} \\end{equation*}\\] (3.14) implies that \\(\\bar{y} = \\frac{q_1\\bar{y_1} + q_2\\bar{y2}}{n}\\), where \\(q_1+q_2 = n\\). Extending it to \\(k\\) subsets gives us \\(\\bar{y} = \\frac{\\sum_{i=1}^k{q_i\\bar{y_i}}}{n}\\), where \\(n = \\sum_{i=1}^k{q_i}\\). Similarly for the sample variances, \\[\\begin{equation*} \\begin{split} s_1^2 =&amp; \\sum_{i=1}^{q_1}\\frac{({y_{1i}-\\bar{y_1}})^2}{q_1}\\\\ s_2^2 =&amp; \\sum_{i=1}^{q_2}\\frac{({y_{2i}-\\bar{y_2}})^2}{q_2} \\end{split} \\tag{3.15} \\end{equation*}\\] (3.15) implies that \\(s^2 = \\frac{(q_1-1)s_1^2 + (q_2-1)s_2^2}{n-1}\\), where \\(n = q_1+q_2\\). Extending it to \\(k\\) subsets gives us \\(s^2 = \\frac{\\sum_{i=1}^k(q_i -1)s_i^2}{n-1}\\), where \\(n = \\sum_{i=1}^{k}{q_i}\\). Applying what we already derived in the previous parts, we can see that for \\(i=1,2\\): \\[\\begin{equation*} \\begin{split} \\mu_{i\\beta}^* =&amp; \\bar{y_i}\\\\ a_{i1}^{*} =&amp; \\frac{q_i- 1}{2}\\\\ b_{i1}^{*} =&amp; \\frac{(q_i - 1)s_i^2}{2}\\\\ c_{i1}=&amp; (q_i -1)s_i^2 \\end{split} \\tag{3.16} \\end{equation*}\\] We also know that the posteriors for the full data set are: \\[\\begin{equation*} \\begin{split} \\mu_\\beta^{*} &amp;= \\bar{y}\\\\ a_1 &amp;= \\frac{n-1}{2}\\\\ b_1 &amp;= \\frac{(n-1)s^2}{2}\\\\ c &amp;= y^{\\mathrm{\\scriptscriptstyle T}}y - m_1^{\\mathrm{\\scriptscriptstyle T}}M_1m_1 = y^{\\mathrm{\\scriptscriptstyle T}}(I - P_x)y = (n-1)s^2 \\end{split} \\tag{3.17} \\end{equation*}\\] We can see that the posteriors for the full data set can be expressed as functions of the posteriors of the subsets \\((k = 1, 2)\\), \\[\\begin{equation*} \\begin{split} \\mu_\\beta^{*} =&amp; \\bar{y} = \\frac{q_1\\bar{y_1} + q_2\\bar{y_2}}{n}\\\\ a_1 =&amp; \\frac{q_1 + q_2-1}{2}\\\\ b_1 =&amp; \\frac{(q_1 - 1)s_1^2 + (q_2 -1)s_2^2}{2}\\\\ c =&amp; (q_1 - 1)s_1^2 + (q_2 -1)s_2^2 \\end{split} \\tag{3.18} \\end{equation*}\\] which proves that it is possible to still compute \\(a_1\\), \\(b_1\\), \\(M_1\\) and \\(m_1\\) without ever having to store or compute with \\(y\\) or \\(X\\), but with quantities computed using only the subsets (for \\(k = 1,2\\)). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
