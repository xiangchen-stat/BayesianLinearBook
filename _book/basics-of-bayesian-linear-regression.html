<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Basics of Bayesian linear regression | Bayesian Linear Regression</title>
  <meta name="description" content="This is a first tutorial for Bayesian Linear Regression assembled in book form." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Basics of Bayesian linear regression | Bayesian Linear Regression" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a first tutorial for Bayesian Linear Regression assembled in book form." />
  <meta name="github-repo" content="xiangchen-stat/BayesianLinearBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Basics of Bayesian linear regression | Bayesian Linear Regression" />
  
  <meta name="twitter:description" content="This is a first tutorial for Bayesian Linear Regression assembled in book form." />
  

<meta name="author" content="Xiang Chen, Valentina Arputhasamy, Daniel Zhou, Sudipto Banerjee" />


<meta name="date" content="2023-01-28" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="the-divide-conquer-algorithm.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html"><i class="fa fa-check"></i><b>1</b> Basics of Bayesian linear regression</a>
<ul>
<li class="chapter" data-level="1.1" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html#bayes-theorem"><i class="fa fa-check"></i><b>1.1</b> Bayes’ theorem</a></li>
<li class="chapter" data-level="1.2" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html#normal-inverse-gamma-nig-prior"><i class="fa fa-check"></i><b>1.2</b> Normal-Inverse-Gamma (NIG) prior</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html#joint-distribution-of-nig-prior"><i class="fa fa-check"></i><b>1.2.1</b> Joint distribution of NIG prior</a></li>
<li class="chapter" data-level="1.2.2" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html#marginal-distribution-of-nig-prior"><i class="fa fa-check"></i><b>1.2.2</b> Marginal distribution of NIG prior</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html#conjugate-bayesian-linear-regression-and-mm-formula"><i class="fa fa-check"></i><b>1.3</b> Conjugate Bayesian linear regression and M&amp;m formula</a></li>
<li class="chapter" data-level="1.4" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html#updating-form-of-the-posterior-distribution"><i class="fa fa-check"></i><b>1.4</b> Updating form of the posterior distribution</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html#method-1-sherman-woodbury-morrison-identity"><i class="fa fa-check"></i><b>1.4.1</b> Method 1: Sherman-Woodbury-Morrison identity</a></li>
<li class="chapter" data-level="1.4.2" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html#method-2-distribution-theory"><i class="fa fa-check"></i><b>1.4.2</b> Method 2: Distribution theory</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html#bayesian-prediction"><i class="fa fa-check"></i><b>1.5</b> Bayesian prediction</a></li>
<li class="chapter" data-level="1.6" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html#sampling-from-the-posterior-distribution"><i class="fa fa-check"></i><b>1.6</b> Sampling from the posterior distribution</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-divide-conquer-algorithm.html"><a href="the-divide-conquer-algorithm.html"><i class="fa fa-check"></i><b>2</b> The Divide &amp; Conquer Algorithm</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-divide-conquer-algorithm.html"><a href="the-divide-conquer-algorithm.html#the-problem"><i class="fa fa-check"></i><b>2.1</b> The Problem</a></li>
<li class="chapter" data-level="2.2" data-path="the-divide-conquer-algorithm.html"><a href="the-divide-conquer-algorithm.html#the-solution"><i class="fa fa-check"></i><b>2.2</b> The Solution</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="forward-filtering-backward-sampling---shared-variance.html"><a href="forward-filtering-backward-sampling---shared-variance.html"><i class="fa fa-check"></i><b>3</b> Forward Filtering Backward Sampling - Shared Variance</a>
<ul>
<li class="chapter" data-level="3.1" data-path="forward-filtering-backward-sampling---shared-variance.html"><a href="forward-filtering-backward-sampling---shared-variance.html#background"><i class="fa fa-check"></i><b>3.1</b> Background</a></li>
<li class="chapter" data-level="3.2" data-path="forward-filtering-backward-sampling---shared-variance.html"><a href="forward-filtering-backward-sampling---shared-variance.html#derivation-of-the-forward-filter"><i class="fa fa-check"></i><b>3.2</b> Derivation of the Forward Filter</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="forward-filtering-backward-sampling---shared-variance.html"><a href="forward-filtering-backward-sampling---shared-variance.html#deriving-beta_tvert-y_t"><i class="fa fa-check"></i><b>3.2.1</b> Deriving <span class="math inline">\(\beta_{t}\vert Y_{t}\)</span></a></li>
<li class="chapter" data-level="3.2.2" data-path="forward-filtering-backward-sampling---shared-variance.html"><a href="forward-filtering-backward-sampling---shared-variance.html#deriving-sigma2vert-y_t"><i class="fa fa-check"></i><b>3.2.2</b> Deriving <span class="math inline">\(\sigma^{2}\vert Y_{t}\)</span></a></li>
<li class="chapter" data-level="3.2.3" data-path="forward-filtering-backward-sampling---shared-variance.html"><a href="forward-filtering-backward-sampling---shared-variance.html#commentary"><i class="fa fa-check"></i><b>3.2.3</b> Commentary</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="forward-filtering-backward-sampling---shared-variance.html"><a href="forward-filtering-backward-sampling---shared-variance.html#derivation-of-the-backwards-sampling"><i class="fa fa-check"></i><b>3.3</b> Derivation of the Backwards Sampling</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a>
<ul>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#deriving-x_2-vert-x_1-when-x_1-x_2mathrmscriptscriptstyle-t-is-a-block-normal-multivariate-random-variable"><i class="fa fa-check"></i>Deriving <span class="math inline">\(x_{2} \vert x_{1}\)</span> when <span class="math inline">\((x_{1} x_{2})^{\mathrm{\scriptscriptstyle T}}\)</span> is a block-normal multivariate random variable</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Linear Regression</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="basics-of-bayesian-linear-regression" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Chapter 1</span> Basics of Bayesian linear regression<a href="basics-of-bayesian-linear-regression.html#basics-of-bayesian-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="bayes-theorem" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Bayes’ theorem<a href="basics-of-bayesian-linear-regression.html#bayes-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="theorem">
<p><span id="thm:bayes" class="theorem"><strong>Theorem 1.1  (Bayes' theorem) </strong></span>For events <span class="math inline">\(U, K\)</span> and <span class="math inline">\(P(K) \neq 0\)</span>, we have</p>
<p><span class="math display">\[P(U \mid K) = \frac{P(K \mid U) P(U)}{P(K)}\]</span></p>
</div>
<p>We denote <span class="math inline">\(U\)</span> as unknown parameters and <span class="math inline">\(K\)</span> as known parameters. We call <span class="math inline">\(P(U)\)</span> prior and <span class="math inline">\(P(K|U)\)</span> likelihood. The Bayes’ theorem gives us the posterior distribution of unknown parameters given the known parameters
<span class="math display">\[ P(U \mid K) \propto P(U) \cdot P(K \mid U)\]</span></p>
</div>
<div id="normal-inverse-gamma-nig-prior" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Normal-Inverse-Gamma (NIG) prior<a href="basics-of-bayesian-linear-regression.html#normal-inverse-gamma-nig-prior" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="joint-distribution-of-nig-prior" class="section level3 hasAnchor" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> Joint distribution of NIG prior<a href="basics-of-bayesian-linear-regression.html#joint-distribution-of-nig-prior" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definition">
<p><span id="def:NIG" class="definition"><strong>Definition 1.1  (Normal-Inverse-Gamma Distribution) </strong></span>Suppose
<span class="math display">\[
\begin{aligned}
&amp; \beta \mid \sigma^2, \mu, M \sim N(\mu,\sigma^2 M) \\
&amp; \sigma^2 \mid a, b \sim IG(a, b)
\end{aligned}
\]</span>
Then <span class="math inline">\((\beta,\sigma^2)\)</span> has a Normal-Inverse-Gamma distribution, denoted as <span class="math inline">\((\beta,\sigma^2) \sim NIG(\mu,M,a,b)\)</span>.</p>
</div>
<p>We use a Normal-Inverse-Gamma prior for <span class="math inline">\((\beta, \sigma^2)\)</span></p>
<p><span class="math display">\[\begin{align}
    P(\beta, \sigma^{2}) &amp;= NIG \left(\beta, \sigma^{2} \mid m_{0}, M_{0}, a_{0}, b_{0}\right) \\
    &amp;= \frac{b_0^{a_0}}{\Gamma\left(a_{0}\right)}
    \left(\frac{1}{\sigma^{2}}\right)^{a_{0}+1} e^{-\frac{b_{0}}{\sigma^{2}}} \frac{1}{(2 \pi \sigma^{2})^{\frac{p}{2}}\left| M_{0}\right|^{\frac{1}{2}}} e^{-\frac{1}{2 \sigma^{2}} Q \left(\beta, m_{0}, M_{0}\right)}
\end{align}\]</span></p>
<p>where <span class="math inline">\(Q(x, m, M)=(x-m)^{\mathrm{\scriptscriptstyle T}} M^{-1} (x-m) \, .\)</span></p>
<details>
<summary>
Click to show or hide details
</summary>
<p><span class="math display">\[\begin{align}
    P(\beta, \sigma^{2})
    &amp;= NIG \left(\beta, \sigma^{2} \mid m_{0}, M_{0}, a_{0}, b_{0}\right) \\
    &amp;= IG\left(\sigma^{2} \mid a_{0}, b_{0}\right) \cdot N\left(\beta \mid m_{0}, \sigma^{2} M_{0}\right) \\
    &amp;= \frac{b_0^{a_0}}{\Gamma\left(a_{0}\right)}
    \left(\frac{1}{\sigma^{2}}\right)^{a_{0}+1} e^{-\frac{b_{0}}{\sigma^{2}}} \frac{1}{(2 \pi)^{\frac{p}{2}}\left|\sigma^{2} M_{0}\right|^{\frac{1}{2}}} e^{-\frac{1}{2 \sigma^{2}} Q \left(\beta, m_{0}, M_{0}\right)} \\
    &amp;= \frac{b_0^{a_0}}{\Gamma\left(a_{0}\right)}
    \left(\frac{1}{\sigma^{2}}\right)^{a_{0}+1} e^{-\frac{b_{0}}{\sigma^{2}}} \frac{1}{(2 \pi \sigma^{2})^{\frac{p}{2}}\left| M_{0}\right|^{\frac{1}{2}}} e^{-\frac{1}{2 \sigma^{2}} Q \left(\beta, m_{0}, M_{0}\right)} \\
\end{align}\]</span></p>
<p>where <span class="math inline">\(Q(x, m, M)=(x-m)^{\mathrm{\scriptscriptstyle T}} M^{-1} (x-m) \, .\)</span></p>
<p>Note: the Inverse-Gamma (<span class="math inline">\(IG\)</span>) distribution has a relationship with Gamma distribution. Given <span class="math inline">\(X \sim Gamma(\alpha, \beta)\)</span>, the density function of <span class="math inline">\(X\)</span> is <span class="math inline">\(f(x)=\frac{\beta^{\alpha}}{\Gamma(\alpha)} x^{\alpha-1} e^{-\beta x}\)</span>. Then <span class="math inline">\(Y=\frac{1}{X} \sim IG\left(\alpha, \beta\right)\)</span> with density function <span class="math inline">\(f(y)=\frac{\beta^{\alpha}}{\Gamma(\alpha)} x^{-\alpha-1} e^{-\frac{\beta}{x}}\)</span>.</p>
</details>
</div>
<div id="marginal-distribution-of-nig-prior" class="section level3 hasAnchor" number="1.2.2">
<h3><span class="header-section-number">1.2.2</span> Marginal distribution of NIG prior<a href="basics-of-bayesian-linear-regression.html#marginal-distribution-of-nig-prior" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As for marginal priors, we can can get it by integration</p>
<p><span class="math display">\[
\begin{aligned}
P(\sigma^2) &amp; = \int N I G\left(\beta, \sigma^{2} \mid m_{0}, M_{0}, a_{0}, b_{0}\right) \  d\beta=I G\left(\sigma^{2} \mid a_{0}, b_{0}\right) \\
P(\beta) &amp; = \int N I G\left(\beta, \sigma^{2} \mid m_{0}, M_{0}, a_{0}, b_{0}\right) \  d\sigma^{2}=t_{2a_0}\left(m_0, \frac{b_0}{a_0}M_0\right) \\
\end{aligned}
\]</span></p>
<details>
<summary>
Click to show or hide details
</summary>
<p><span class="math display">\[\begin{align}
    P\left(\sigma^{2} \right) &amp;= \int NIG\left(\beta, \sigma^{2} \mid m_{0}, M_{0}, a_{0}, b_{0}\right) \  d\beta \\
    &amp;=IG\left(\sigma^{2} \mid a_{0}, b_{0}\right) \int N\left(\beta \mid m_{0}, \sigma^{2} M_{0}\right) \  d\beta \\
    &amp;=IG\left(\sigma^{2} \mid a_{0}, b_{0}\right)
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
P(\beta ) &amp;=\int NIG \left(\beta, \sigma^{2} \mid m_{0}, M_{0}, a_{0}, b_{0}\right) \  d\sigma^{2} \\
&amp;= \int \frac{b_0^{a_0}}{\Gamma\left(a_{0}\right)}
\left(\frac{1}{\sigma^{2}}\right)^{a_{0}+1} e^{-\frac{b_{0}}{\sigma^{2}}} \frac{1}{(2 \pi \sigma^{2})^{\frac{p}{2}}\left| M_{0}\right|^{\frac{1}{2}}} e^{-\frac{1}{2 \sigma^{2}} Q \left(\beta, m_{0}, M_{0}\right)} \  d\sigma^{2} \\
&amp;= \frac{b_0^{a_0}}{\Gamma\left(a_{0}\right)(2 \pi)^{\frac{p}{2}}\left| M_{0}\right|^{\frac{1}{2}}}
\int
\left(\frac{1}{\sigma^{2}}\right)^{a_{0}+\frac{p}{2}+1} e^{-\frac{1}{\sigma^{2}}\left(b_{0}+\frac{1}{2} Q \left(\beta, m_{0}, M_{0}\right)\right)} \  d\sigma^{2} \\
&amp; \quad \left(\text{let } u = \frac{1}{\sigma^2}, \left|d\sigma^{2}\right|=\frac{1}{u^2} d u\right) \\
&amp;= \frac{b_0^{a_0}}{\Gamma\left(a_{0}\right)(2 \pi)^{\frac{p}{2}}\left| M_{0}\right|^{\frac{1}{2}}}
\int
u^{a_{0}+\frac{p}{2}+1} e^{-\left(b_{0}+\frac{1}{2} Q \left(\beta, m_{0}, M_{0}\right)\right) u } \frac{1}{u^2} \  du \\
&amp;= \frac{b_0^{a_0}}{\Gamma\left(a_{0}\right)(2 \pi)^{\frac{p}{2}}\left| M_{0}\right|^{\frac{1}{2}}}
\int
u^{a_{0}+\frac{p}{2}-1} e^{-\left(b_{0}+\frac{1}{2} Q \left(\beta, m_{0}, M_{0}\right)\right) u} \  du \\
&amp; \quad \left(\text{by Gamma integral function:} \int x^{\alpha - 1} exp^{-\beta x} dx = \frac{\Gamma(\alpha)}{\beta^{\alpha}}\right) \\
&amp;= \frac{b_{0}^{a_{0}} }{\Gamma\left(a_{0}\right)(2 \pi)^\frac{p}{2}\left|M_{0}\right|^{\frac{1}{2}}} \frac{\Gamma\left(a_{0}+\frac{p}{2}\right)}{\left(b_{0}+\frac{1}{2} Q(\beta,m_0,M_0)\right)^{\left(a_{0}+\frac{p}{2}\right)}} \\
&amp; = \frac{b_0^{a_0}\Gamma\left(a_{0}+\frac{p}{2}\right)}{\Gamma\left(a_{0}\right)(2 \pi)^ \frac{p}{2}\left|M_{0}\right|^{\frac{1}{2}}}
\left(b_0(1+\frac{1}{2 b_0} Q(\beta,m_0,M_0))\right)^{-\left(a_{0}+\frac{p}{2}\right)} \\
&amp; = \frac{b_0^{a_0}\Gamma\left(a_{0}+\frac{p}{2}\right) b_0^{- \left( a_0+\frac{p}{2}\right)}}{\Gamma\left(a_{0}\right)(2 \pi)^ \frac{p}{2}\left|M_{0}\right|^{\frac{1}{2}}}
\left(1+\frac{1}{2 b_0} \left(\beta-m_{0}\right)^{\mathrm{\scriptscriptstyle T}} M_{0}^{-1}\left(\beta-m_{0}\right) \right)^{-\left(a_{0}+\frac{p}{2}\right)} \\
&amp; =\frac{\Gamma\left(a_{0}+\frac{p}{2}\right)}{\left(2 \pi \right)^{\frac{p}{2}} b_{0}^{\frac{p}{2}} \Gamma\left(a_{0}\right)|M|^{\frac{1}{2}}}\left(1+\frac{1}{2 b_{0}} \left(\beta-m_{0}\right)^{\mathrm{\scriptscriptstyle T}} M_{0}^{-1}\left(\beta-m_{0}\right) \right)^{-\left(a_{0}+\frac{p}{2}\right)} \\
&amp; =\frac{\Gamma\left(a_{0}+\frac{p}{2}\right)}
{\left(2 \pi \right)^{\frac{p}{2}}\left(a_{0} \cdot \frac{b_{0}}{a_{0}}\right)^{\frac{p}{2}} \Gamma\left(a_{0}\right)|M|^{\frac{1}{2}}} \left(1+\frac{1}{2 a_{0} \cdot \frac{b_{0}}{a_{0}}} \left(\beta-m_{0}\right)^{\mathrm{\scriptscriptstyle T}} M_{0}^{-1}\left(\beta-m_{0}\right)\right)^{-\left(a_{0}+\frac{p}{2}\right)}\\
&amp; =\frac{\Gamma\left(a_{0}+\frac{p}{2}\right)}{\left(2 a_{0} \pi\right)^{\frac{p}{2}} \Gamma\left(a_{0}\right)\left|\frac{b_{0}}{a_{0}} M\right|^{\frac{1}{2}}}\left(1+\frac{1}{2 a_{0}} \left(\beta-m_{0}\right)^{\mathrm{\scriptscriptstyle T}}\left(\frac{b_{0}}{a_{0}} M_{0}\right)^{-1}\left(\beta-m_{0}\right)\right)^{-\left(a_{0}+\frac{p}{2}\right)} \\
&amp; =t_{2a_0}\left(m_0, \frac{b_0}{a_0}M_0\right) \;
\end{align}\]</span></p>
</details>
<p>Note: the density of multivariate t-distribution is given by</p>
<p><span class="math display">\[
t_v(\mu, \Sigma)=\frac{\Gamma\left(\frac{v+p}{2}\right)}{(v \pi)^{\frac{p}{2}} \Gamma\left(\frac{v}{2}\right)  |\Sigma|^{\frac{1}{2}}}\left(1+\frac{1}{v}(x-\mu)^{\mathrm{\scriptscriptstyle T}} \Sigma^{-1}(x-\mu)\right)^{-\frac{v+p}{2}}
\]</span></p>
</div>
</div>
<div id="conjugate-bayesian-linear-regression-and-mm-formula" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> Conjugate Bayesian linear regression and M&amp;m formula<a href="basics-of-bayesian-linear-regression.html#conjugate-bayesian-linear-regression-and-mm-formula" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let <span class="math inline">\(y_{n \times 1}\)</span> be outcome variable and <span class="math inline">\(X_{n \times p}\)</span> be corresponding covariates. Assume <span class="math inline">\(V\)</span> is known. The model is given by</p>
<p><span class="math display">\[\begin{align}
&amp; y=X \beta+\epsilon \ , \  \epsilon \sim N\left(0, \sigma^2 V\right) \\
&amp; \beta=m_0+\omega \ ,  \  \omega \sim N\left(0, \sigma^2 M_0\right) \\
&amp; \sigma^2 \sim I G\left(a_0, b_0\right)
\end{align}\]</span></p>
<p>The posterior distribution of <span class="math inline">\((\beta, \sigma^2)\)</span> is given by</p>
<p><span class="math display">\[\begin{align}
P\left(\beta, \sigma^{2} \mid y\right) = NIG\left(\beta, \sigma^{2} \mid M_{1}m_{1}, M_{1}, a_{1}, b_{1}\right) \;
\end{align}\]</span></p>
<p>where</p>
<p><span class="math display">\[\begin{align}
M_{1}^{-1} &amp;= M_{0}^{-1}+X^{\mathrm{\scriptscriptstyle T}} V^{-1} X  \\
m_{1}&amp;=M_{0}^{-1} m_{0}+X^{\mathrm{\scriptscriptstyle T}} V^{-1} y  \\
a_{1}&amp;=a_{0}+\frac{p}{2}  \\
b_{1}&amp;=b_{0}+\frac{c^{\ast}}{2}= b_{0}+\frac{1}{2}\left(m_{0}^{\mathrm{\scriptscriptstyle T}} M_{0}^{-1} m_{0}+y^{\mathrm{\scriptscriptstyle T}} V^{-1} y-m_{1}^{\mathrm{\scriptscriptstyle T}} M_{1} m_{1}\right)
\end{align}\]</span></p>
<details>
<summary>
Click to show or hide details
</summary>
<p><span class="math display">\[\begin{align}\label{eq:post_dist}
P\left(\beta, \sigma^{2} \mid y\right) &amp; \propto NIG\left(\beta, \sigma^{2} \mid m_{0}, M_{0}, a_{0}, b_{0}\right) \cdot N\left(y \mid X \beta, \sigma^{2} V\right) \nonumber\\
&amp; \propto IG\left(\sigma^{2} \mid a_{0}, b_{0}\right) \cdot N\left(\beta \mid m_{0}, \sigma^{2} M_{0}\right) \cdot N\left(y \mid X \beta, \sigma^{2} V\right) \nonumber\\
&amp; \propto \frac{b_0^{a_0}}{\Gamma\left(a_{0}\right)}
\left(\frac{1}{\sigma^{2}}\right)^{a_{0}+1} e^{-\frac{b_{0}}{\sigma^{2}}}
\frac{1}{(2 \pi \sigma^{2})^{\frac{p}{2}}\left| M_{0}\right|^{\frac{1}{2}}} e^{-\frac{1}{2 \sigma^{2}} Q \left(\beta, m_{0}, M_{0}\right)}
\frac{1}{(2 \pi \sigma^{2})^{\frac{p}{2}}\left| V\right|^{\frac{1}{2}}} e^{-\frac{1}{2 \sigma^{2}} Q \left(y, X \beta, V\right)} \nonumber\\
&amp; \propto \left(\frac{1}{\sigma^{2}}\right)^{a_{0}+p+1} e^{-\frac{b_{0}}{\sigma^{2}}}
e^{-\frac{1}{2 \sigma^{2}} \left(Q \left(\beta, m_{0}, M_{0}\right)+Q \left(y, X \beta, V\right)\right)}\;
\end{align}\]</span></p>
<p>where</p>
<p><span class="math display">\[\begin{align}\label{eq:multivariate_completion_square}
Q \left(\beta, m_{0}, M_{0}\right)+Q \left(y, X \beta, V\right) &amp;= (\beta - m_{0})^{\mathrm{\scriptscriptstyle T}}M_{0}^{-1}(\beta - m_{0}) + (y - X\beta)^{\mathrm{\scriptscriptstyle T}}V^{-1}(y - X\beta)\; \nonumber\\
&amp;= \beta^{\mathrm{\scriptscriptstyle T}}M_{0}^{-1}\beta - 2\beta^{\mathrm{\scriptscriptstyle T}}M_{0}^{-1}m_{0} + m_{0}^{\mathrm{\scriptscriptstyle T}}M_{0}^{-1}m_{0} \nonumber\\
  &amp;\qquad + \beta^{\mathrm{\scriptscriptstyle T}}X^{\mathrm{\scriptscriptstyle T}}V^{-1}X\beta - 2\beta^{\mathrm{\scriptscriptstyle T}} X^{\mathrm{\scriptscriptstyle T}}V^{-1}y + y^{\mathrm{\scriptscriptstyle T}}V^{-1}y \nonumber\\
  &amp;= \beta^{\mathrm{\scriptscriptstyle T}} \left(M_{0}^{-1} + X^{\mathrm{\scriptscriptstyle T}}V^{-1}X\right) \beta - 2\beta^{\mathrm{\scriptscriptstyle T}}\left(M_{0}^{-1}m_{0} + X^{\mathrm{\scriptscriptstyle T}}V^{-1}y\right) \nonumber\\
  &amp;\qquad + m_{0}^{\mathrm{\scriptscriptstyle T}} M_{0}^{-1}m_{0} + y^{\mathrm{\scriptscriptstyle T}}V^{-1}y \nonumber \\
  &amp;= \beta^{\mathrm{\scriptscriptstyle T}}M_{1}^{-1}\beta - 2\beta^{\mathrm{\scriptscriptstyle T}} m_{1} + c\nonumber\\
  &amp;= (\beta - M_{1}m_{1})^{\mathrm{\scriptscriptstyle T}}M_{1}^{-1}(\beta - M_{1}m_{1}) - m_{1}^{\mathrm{\scriptscriptstyle T}}M_{1}m_{1} +c \nonumber\\   
  &amp;= (\beta - M_{1}m_{1})^{\mathrm{\scriptscriptstyle T}}M_{1}^{-1}(\beta - M_{1}m_{1}) +c^{\ast}\;
\end{align}\]</span></p>
<p>where <span class="math inline">\(M_{1}\)</span> is a symmetric positive definite matrix, <span class="math inline">\(m_{1}\)</span> is a vector, and <span class="math inline">\(c\)</span> &amp; <span class="math inline">\(c^{\ast}\)</span> are scalars given by</p>
<p><span class="math display">\[\begin{align}
M_{1}^{-1} &amp;= M_{0}^{-1} + X^{\mathrm{\scriptscriptstyle T}}V^{-1}X  \\
m_{1} &amp;= M_{0}^{-1}m_{0} + X^{\mathrm{\scriptscriptstyle T}}V^{-1}y \\
c &amp;= m_{0}^{\mathrm{\scriptscriptstyle T}} M_{0}^{-1}m_{0} + y^{\mathrm{\scriptscriptstyle T}}V^{-1}y \\
  c^{\ast} &amp;= c - m^{\mathrm{\scriptscriptstyle T}}Mm = m_{0}^{\mathrm{\scriptscriptstyle T}} M_{0}^{-1}m_{0} + y^{\mathrm{\scriptscriptstyle T}}V^{-1}y - m_{1}^{\mathrm{\scriptscriptstyle T}}M_{1}m_{1}
\end{align}\]</span></p>
<p>Note: <span class="math inline">\(M_{1}\)</span>, <span class="math inline">\(m_{1}\)</span> and <span class="math inline">\(c\)</span> do not depend upon <span class="math inline">\(\beta\)</span>.</p>
<p>Then, we have</p>
<p><span class="math display">\[\begin{align}
P\left(\beta, \sigma^{2} \mid y\right) &amp; \propto \left(\frac{1}{\sigma^{2}}\right)^{a_{0}+p+1} e^{-\frac{b_{0}}{\sigma^{2}}}
e^{-\frac{1}{2 \sigma^{2}} ((\beta - M_{1}m_{1})^{\mathrm{\scriptscriptstyle T}}M_{1}^{-1}(\beta - M_{1}m_{1}) +c^{\ast})}\\
&amp; \propto \left(\frac{1}{\sigma^{2}}\right)^{a_{0}+p+1} e^{-\frac{b_{0}+\frac{c^{\ast}}{2}}{\sigma^{2}}}
e^{-\frac{1}{2 \sigma^{2}} (\beta - M_{1}m_{1})^{\mathrm{\scriptscriptstyle T}}M_{1}^{-1}(\beta - M_{1}m_{1})}\\
&amp; \propto \left(\frac{1}{\sigma^{2}}\right)^{a_{0}+\frac{p}{2}+1} e^{-\frac{b_{0}+\frac{c^{\ast}}{2}}{\sigma^{2}}}
(\frac{1}{\sigma^2})^{\frac{p}{2}}
e^{-\frac{1}{2 \sigma^{2}} (\beta - M_{1}m_{1})^{\mathrm{\scriptscriptstyle T}}M_{1}^{-1}(\beta - M_{1}m_{1})}\\
&amp; \propto IG\left(\sigma^{2} \mid a_{0}+\frac{p}{2}, b_{0}+\frac{c^{\ast}}{2} \right) \cdot N\left(\beta \mid M_{1}m_{1}, \sigma^{2} M_{1}\right) \\
&amp; \propto IG\left(\sigma^{2} \mid a_{1}, b_{1} \right) \cdot N\left(\beta \mid M_{1}m_{1}, \sigma^{2} M_{1}\right) \\
&amp; \propto NIG\left(\beta, \sigma^{2} \mid M_{1}m_{1}, M_{1}, a_{1}, b_{1}\right) \;
\end{align}\]</span></p>
<p>where</p>
<p><span class="math display">\[\begin{align}
M_{1}^{-1} &amp;= M_{0}^{-1}+X^{\mathrm{\scriptscriptstyle T}} V^{-1} X  \\
m_{1}&amp;=M_{0}^{-1} m_{0}+X^{\mathrm{\scriptscriptstyle T}} V^{-1} y  \\
a_{1}&amp;=a_{0}+\frac{p}{2}  \\
b_{1}&amp;=b_{0}+\frac{c^{\ast}}{2}= b_{0}+\frac{1}{2}\left(m_{0}^{\mathrm{\scriptscriptstyle T}} M_{0}^{-1} m_{0}+y^{\mathrm{\scriptscriptstyle T}} V^{-1} y-m_{1}^{\mathrm{\scriptscriptstyle T}} M_{1} m_{1}\right)
\end{align}\]</span></p>
</details>
<p>From derivation in marginal priors, the marginal posterior distributions can be easily get by updating corresponding parameters</p>
<p><span class="math display">\[
\begin{aligned}
&amp;P\left(\sigma^{2} \mid y\right)=I G\left(\sigma^{2} \mid a_{1}, b_{1}\right) \\
&amp;P(\beta \mid y)=t_{2a_1}\left(M_1m_1, \frac{b_1}{a_1}M_1\right)
\end{aligned}
\]</span></p>
</div>
<div id="updating-form-of-the-posterior-distribution" class="section level2 hasAnchor" number="1.4">
<h2><span class="header-section-number">1.4</span> Updating form of the posterior distribution<a href="basics-of-bayesian-linear-regression.html#updating-form-of-the-posterior-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We will use two ways to derive the updating form of the posterior distribution.</p>
<div id="method-1-sherman-woodbury-morrison-identity" class="section level3 hasAnchor" number="1.4.1">
<h3><span class="header-section-number">1.4.1</span> Method 1: Sherman-Woodbury-Morrison identity<a href="basics-of-bayesian-linear-regression.html#method-1-sherman-woodbury-morrison-identity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="theorem">
<p><span id="thm:swm" class="theorem"><strong>Theorem 1.2  (Sherman-Woodbury-Morrison identity) </strong></span>We have
<span class="math display">\[\begin{equation}\label{ShermanWoodburyMorrison}
\left(A + BDC\right)^{-1} = A^{-1} - A^{-1}B\left(D^{-1}+CA^{-1}B\right)^{-1}CA^{-1}
\end{equation}\]</span>
where <span class="math inline">\(A\)</span> and <span class="math inline">\(D\)</span> are square matrices that are invertible and <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span> are rectangular (square if <span class="math inline">\(A\)</span> and <span class="math inline">\(D\)</span> have the same dimensions) matrices such that the multiplications are well-defined.</p>
</div>
<p><a href="https://en.wikipedia.org/wiki/Woodbury_matrix_identity">Sherman-Woodbury-Morrison identity</a> is easily verified by multiplying the right hand side with <span class="math inline">\(A + BDC\)</span> and simplifying to reduce it to the identity matrix. Using this formula, we have</p>
<p><span class="math display">\[
\begin{aligned}
M_1 &amp; = (M_{0}^{-1} + X^{\mathrm{\scriptscriptstyle T}}V^{-1}X)^{-1} \\
&amp; = M_0-M_0 X^{\mathrm{\scriptscriptstyle T}}\left(V+X M_0 X^{\mathrm{\scriptscriptstyle T}}\right)^{-1} X M_0 \\
&amp; = M_0-M_0 X^{\mathrm{\scriptscriptstyle T}} Q^{-1} X M_0
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(Q = V + X M_0 X^{\mathrm{\scriptscriptstyle T}}\)</span></p>
<p>We can show that
<span class="math display">\[
\begin{align}
M_1 m_1 &amp; =m_0+M_0 X^{\mathrm{\scriptscriptstyle T}} Q^{-1}\left(y-X m_0\right)
\end{align}
\]</span></p>
<details>
<summary>
Click to show or hide details
</summary>
<p><span class="math display">\[\begin{align}
  M_1 m_1 &amp; = \left(M_0^{-1}+X^{\mathrm{\scriptscriptstyle T}} V^{-1} X\right)^{-1} m_1 \\
  &amp; = \left(M_0-M_0 X^{\mathrm{\scriptscriptstyle T}}\left(V+X M_0 X^{\mathrm{\scriptscriptstyle T}}\right)^{-1} X M_0\right)m_1 \\
  &amp; = \left(M_0-M_0 X^{\mathrm{\scriptscriptstyle T}} Q^{-1} X M_0\right) m_1 \\
  &amp; = \left(M_0-M_0 X^{\mathrm{\scriptscriptstyle T}} Q^{-1} X M_0\right)\left(M_0^{-1} m_0+X^{\mathrm{\scriptscriptstyle T}} V^{-1} y\right) \\
  &amp; = m_0+M_0 X^{\mathrm{\scriptscriptstyle T}} V^{-1} y-M_0 X^{\mathrm{\scriptscriptstyle T}} Q^{-1} X m_0 - M_0 X^{\mathrm{\scriptscriptstyle T}} Q^{-1} X M_0 X^{\mathrm{\scriptscriptstyle T}} V^{-1} y \\
  &amp; = m_0+M_0 X^{\mathrm{\scriptscriptstyle T}}\left(I-Q^{-1} X M_0 X^{\mathrm{\scriptscriptstyle T}}\right) V^{-1} y - M_0 X^{\mathrm{\scriptscriptstyle T}} Q^{-1} X m_0 \\
  &amp; = m_0+M_0 X^{\mathrm{\scriptscriptstyle T}} Q^{-1}\left(Q-X M_0 X^{\mathrm{\scriptscriptstyle T}}\right)V^{-1} y - M_0 X^{\mathrm{\scriptscriptstyle T}} Q^{-1} X m_0 \\
  &amp; \left(\text { since } Q=V+X M_0 X^{\mathrm{\scriptscriptstyle T}}\right) \\
  &amp; = m_0+M_0 X^{\mathrm{\scriptscriptstyle T}} Q^{-1}V V^{-1} y-M_0 X^{\mathrm{\scriptscriptstyle T}} Q^{-1} X m_0 \\
  &amp; = m_0+M_0 X^{\mathrm{\scriptscriptstyle T}} Q^{-1} y-M_0 X^{\mathrm{\scriptscriptstyle T}} Q^{-1} X m_0 \\
  &amp; = m_0+M_0 X^{\mathrm{\scriptscriptstyle T}} Q^{-1}\left(y-X m_0\right) \\
  \end{align}\]</span></p>
</details>
<p>Furthermore, we can simplify that
<span class="math display">\[
\begin{align}
m_0^{\mathrm{\scriptscriptstyle T}} M_0^{-1} m_0+y^{\mathrm{\scriptscriptstyle T}} V^{-1} y-m_1^{\mathrm{\scriptscriptstyle T}} M_1 m_1 &amp; = \left(y-X m_0\right)^{\mathrm{\scriptscriptstyle T}} Q^{-1}\left(y-X m_0\right)
\end{align}
\]</span></p>
<details>
<summary>
Click to show or hide details
</summary>
<p><span class="math display">\[\begin{align}
  &amp; \quad \  m_0^{\mathrm{\scriptscriptstyle T}} M_0^{-1} m_0+y^{\mathrm{\scriptscriptstyle T}} V^{-1} y-m_1^{\mathrm{\scriptscriptstyle T}} M_1 m_1 \\
  &amp; = m_0^{\mathrm{\scriptscriptstyle T}} M_0^{-1} m_0+y^{\mathrm{\scriptscriptstyle T}} V^{-1} y-m_1^{\mathrm{\scriptscriptstyle T}} \left(m_0+M_0 X^{\mathrm{\scriptscriptstyle T}} Q^{-1} (y - X m_0)\right) \\
  &amp; = m_0^{\mathrm{\scriptscriptstyle T}} M_0^{-1} m_0+y^{\mathrm{\scriptscriptstyle T}} V^{-1} y-m_1^{\mathrm{\scriptscriptstyle T}} m_0 - m_1^{\mathrm{\scriptscriptstyle T}} M_0 X^{\mathrm{\scriptscriptstyle T}} Q^{-1}\left(y-X m_0\right) \\
  &amp; = m_0^{\mathrm{\scriptscriptstyle T}} M_0^{-1} m_0+y^{\mathrm{\scriptscriptstyle T}} V^{-1} y -m_0^{\mathrm{\scriptscriptstyle T}}\left(M_0^{-1} m_0+X^{\mathrm{\scriptscriptstyle T}} V^{-1} y\right) \\
  &amp; \qquad \qquad \qquad - m_1^{\mathrm{\scriptscriptstyle T}} M_0 X^{\mathrm{\scriptscriptstyle T}} Q^{-1}\left(y-X m_0\right) \\
  &amp; = y^{\mathrm{\scriptscriptstyle T}} V^{-1} y-y^{\mathrm{\scriptscriptstyle T}} V^{-1} X m_0 - m_1^{\mathrm{\scriptscriptstyle T}} M_0 X^{\mathrm{\scriptscriptstyle T}} Q^{-1}\left(y-X m_0\right) \\
  &amp; = y^{\mathrm{\scriptscriptstyle T}} V^{-1}\left(y-X m_0 \right)-m_1^{\mathrm{\scriptscriptstyle T}} M_0 X^{\mathrm{\scriptscriptstyle T}} Q^{-1}\left(y-X m_0\right) \\
  &amp; =y^{\mathrm{\scriptscriptstyle T}} V^{-1}\left(y-X m_0\right)-\underbrace{m_1^{\mathrm{\scriptscriptstyle T}} M_0 X^{\mathrm{\scriptscriptstyle T}} Q^{-1}\left(y-X m_0\right)}_{\substack{\text { simplify from left to right }}} \\
  &amp; =y^{\mathrm{\scriptscriptstyle T}} V^{-1}\left(y-X m_0\right)-\left(M_0 m_1\right)^{\mathrm{\scriptscriptstyle T}} X^{\mathrm{\scriptscriptstyle T}} Q^{-1}\left(y-X m_0\right) \\
  &amp; =y^{\mathrm{\scriptscriptstyle T}} V^{-1}\left(y-X m_0\right)-\left(m_0+M_0 X^{\mathrm{\scriptscriptstyle T}} V^{-1} y\right)^{\mathrm{\scriptscriptstyle T}} X^{\mathrm{\scriptscriptstyle T}} Q^{-1}\left(y-m_0\right) \\
  &amp; =y^{\mathrm{\scriptscriptstyle T}} V^{-1}\left(y-X m_0\right)-\left(X m_0+X M_0 X^{\mathrm{\scriptscriptstyle T}} V^{-1} y\right)^{\mathrm{\scriptscriptstyle T}} Q^{-1}\left(y-X m_0\right)\\
  &amp; =y^{\mathrm{\scriptscriptstyle T}} V^{-1}\left(y-X m_0\right)-\left(Q^{-1} X m_0+Q^{-1}\left(X M_0 X^{\mathrm{\scriptscriptstyle T}}\right)V^{-1} y\right)\left(y-X m_0\right) \\
  &amp; =y^{\mathrm{\scriptscriptstyle T}} V^{-1}\left(y-X m_0\right)-\left(Q^{-1} X m_0+Q^{-1}(Q-V) V^{-1} y\right)^{\mathrm{\scriptscriptstyle T}}(y-X m_0) \\
  &amp; =y^{\mathrm{\scriptscriptstyle T}} V^{-1}\left(y-X m_0\right)-\left(Q^{-1} X m_0+V^{-1} y- Q^{-1} y \right)^{\mathrm{\scriptscriptstyle T}}\left(y-X m_0\right) \\
  &amp; =y^{\mathrm{\scriptscriptstyle T}} V^{-1}\left(y-X m_0\right)-\left(V^{-1} y+Q^{-1}\left(X m_0-y\right)\right)^{\mathrm{\scriptscriptstyle T}}\left(y-X m_0\right) \\
  &amp; =y^{\mathrm{\scriptscriptstyle T}} V^{-1}\left(y-X m_0\right)-y^{\mathrm{\scriptscriptstyle T}} V^{-1}\left(y-X m_0\right) +\left(y-X m_0\right)^{\mathrm{\scriptscriptstyle T}} Q^{-1}\left(y-X m_0\right) \\
  &amp; =\left(y-X m_0\right)^{\mathrm{\scriptscriptstyle T}} Q^{-1}\left(y-X m_0\right) \\
  \end{align}\]</span></p>
</details>
<p>So, we get the following updating form of the posterior distribution from Bayesian linear regression</p>
<p><span class="math display">\[
\begin{aligned}
P\left(\beta, \sigma^{2} \mid y\right) = NIG\left(\beta, \sigma^{2} \mid \tilde{m}_1, \tilde{M}_1, a_{1}, b_{1}\right) \;
\end{aligned}
\]</span>
where</p>
<p><span class="math display">\[
\begin{aligned}
\tilde{m}_1 &amp; =M_1 m_1=m_0+M_0 X^{\mathrm{\scriptscriptstyle T}} Q^{-1}\left(y-X m_0\right) \\
\tilde{M}_1 &amp; =M_1=M_0-M_0 X^{\mathrm{\scriptscriptstyle T}} Q^{-1} X M_0 \\
a_1 &amp; =a_0+\frac{p}{2} \\
b_1 &amp; =b_0+\frac{1}{2}\left(y-X m_0\right)^{\mathrm{\scriptscriptstyle T}} Q^{-1}\left(y-X m_0\right) \\
Q &amp; =V+X M_0 X^{\mathrm{\scriptscriptstyle T}}
\end{aligned}
\]</span></p>
</div>
<div id="method-2-distribution-theory" class="section level3 hasAnchor" number="1.4.2">
<h3><span class="header-section-number">1.4.2</span> Method 2: Distribution theory<a href="basics-of-bayesian-linear-regression.html#method-2-distribution-theory" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Previously, we got the Bayesian Linear Regression Updater using <a href="https://en.wikipedia.org/wiki/Woodbury_matriX_identity">Sherman-Woodbury-Morrison identity</a>. Here, we will derive the results without resorting to it. Recall that the model is given by</p>
<p><span class="math display">\[\begin{align}
&amp; y=X \beta+\epsilon \ , \  \epsilon \sim N\left(0, \sigma^2 V\right) \\
&amp; \beta=m_0+\omega \ ,  \  \omega \sim N\left(0, \sigma^2 M_0\right) \\
&amp; \sigma^2 \sim I G\left(a_0, b_0\right)
\end{align}\]</span></p>
<p>This corresponds to the posterior distribution</p>
<p><span class="math display">\[\begin{align}
P\left(\beta, \sigma^2 \mid y\right) \propto I G\left(\sigma^2 \mid a_0, b_0\right) &amp; \times N\left(\beta \mid m_0, \sigma^2 M_0\right) \times N\left(y \mid X \beta, \sigma^2 V\right)
\end{align}\]</span></p>
<p>We will derive <span class="math inline">\(P\left(\sigma^2 \mid y\right)\)</span> and <span class="math inline">\(P\left(\beta \mid \sigma^2, y\right)\)</span> in a form that will reflect updates from the prior to the posterior. Integrating out <span class="math inline">\(\beta\)</span> from the model is equivalent to substituting <span class="math inline">\(\beta\)</span> from its prior model. Thus, <span class="math inline">\(P\left(y \mid \sigma^2\right)\)</span> is derived simply from <span class="math inline">\(y =X \beta+\epsilon =X\left(m_0+\omega\right)+\epsilon =X m_0 + X \omega + \epsilon =X m_0+ \eta\)</span></p>
<!-- \begin{align} -->
<!-- y =X \beta+\epsilon =X\left(m_0+\omega\right)+\epsilon =X m_0 + X \omega + \epsilon =X m_0+ \eta -->
<!-- \end{align} -->
<p>where <span class="math inline">\(\eta = X \omega + \epsilon \sim N\left(0, \sigma^2Q\right)\)</span> and <span class="math inline">\(Q=X M_0 X^{\mathrm{\scriptscriptstyle T}}+V \, .\)</span></p>
<p>Therefore,</p>
<p><span class="math display">\[\begin{align}
y \mid \sigma^2 \sim N\left(X m_0, \sigma^2 Q\right)\\
\end{align}\]</span></p>
<p>The posterior distribution is given by
<span class="math display">\[\begin{align}
P\left(\sigma^2 \mid y\right) &amp; \propto IG \left(\sigma^2 \mid a_1, b_1\right)
\end{align}\]</span></p>
<p>where
<span class="math display">\[\begin{align}
&amp; a_1 = a_0 + \frac{p}{2} \\
&amp; b_1 = b_0 + \frac{1}{2} (y-Xm_0)^{\mathrm{\scriptscriptstyle T}} Q^{-1} \left(y-Xm_0\right)
\end{align}\]</span></p>
<details>
<summary>
Click to show or hide details
</summary>
<p><span class="math display">\[\begin{align}
  P\left(\sigma^2 \mid y\right) &amp; \propto P\left(\sigma^2\right) P\left(y \mid \sigma^2\right) \\
  &amp; \propto\left(\sigma^2 \mid a_0, b_0\right) \times N\left(y \mid X m_0, \sigma^2 Q\right) \\
  &amp; \propto\left(\frac{1}{\sigma^2}\right)^{a_0+1} e^{-\frac{b_0} {\sigma^2} \times\left(\frac{1}{\sigma^2}\right)^{\frac{n}{2}} e^{-\frac{1}{2 \sigma^2}}\left(y-Xm_0\right)^{\mathrm{\scriptscriptstyle T}} Q^{-1}\left(y-Xm_0\right)} \\
  &amp; \propto\left(\frac{1}{\sigma^2}\right)^{a_0+\frac{p}{2}+1} e^{-\frac{1}{\sigma^2}\left(b_0+\frac{1}{2}\left(y-Xm_0\right)^{\mathrm{\scriptscriptstyle T}} Q^{-1}\left(y-Xm_0\right)\right)} \\
  &amp; \propto IG \left(\sigma^2 \mid a_1, b_1\right)
  \end{align}\]</span></p>
<p>where
<span class="math display">\[\begin{align}
  &amp; a_1 = a_0 + \frac{p}{2} \\
  &amp; b_1 = b_0 + \frac{1}{2} (y-Xm_0)^{\mathrm{\scriptscriptstyle T}} Q^{-1} \left(y-Xm_0\right)
  \end{align}\]</span></p>
</details>
Next, we turn to <span class="math inline">\(P\left(\beta \mid \sigma^2, y\right)\)</span>.
Note that
<span class="math display">\[
\left(\begin{array}{l}
y \\
\beta
\end{array}\right) \mid \sigma^2 \sim N\left(\left(\begin{array}{l}
Xm_0 \\
m_0
\end{array}\right), \quad \sigma^2 \left(\begin{array}{cc}
Q &amp; X M_0 \\
M_0 X^{\mathrm{\scriptscriptstyle T}} &amp; M_0
\end{array}\right)\right) \;
\]</span>
<details>
<summary>
Click to show or hide details
</summary>
<p>We have used the facts</p>
<p><span class="math display">\[
  \begin{aligned}
  &amp; \operatorname{E}\left[y \mid \sigma^2\right] = Xm_0 \, , \
  \operatorname{Var}\left(y \mid \sigma^2\right)=\sigma^2 Q \, ; \\
  &amp; \operatorname{E}\left[\beta \mid \sigma^2\right] = m_0 \, , \
  \operatorname{Var}\left(\beta \mid \sigma^2\right)=\sigma^2 M_0 \, ;
  \end{aligned}
  \]</span></p>
<p><span class="math display">\[
  \begin{aligned}
  \operatorname{Cov}\left(y, \beta \mid \sigma^2\right) &amp;= \operatorname{Cov}\left(X \beta+\epsilon, \beta \mid \sigma^2\right) \\
  &amp; =\operatorname{Cov}\left(X\left(m_0+\omega\right)+\epsilon, m_0+\omega \mid \sigma^2\right) \\
  &amp; \quad \left( \text {Since } m_0 \text { is constant and } \operatorname{Cov}(\omega, \epsilon)=0 \right) \\
  &amp; =\operatorname{Cov}\left(X \omega, \omega \mid \sigma^2\right) \\
  &amp; =\sigma^2 X M_0
  \end{aligned}
  \]</span></p>
</details>
<p>From the expression of a conditional distribution derived from a multivariate Gaussian, we obtain
<span class="math display">\[
\beta \mid \sigma^2, y \sim N\left(\tilde{m}_1, \sigma^2 \tilde{M}_1\right)
\]</span></p>
<p>where
<span class="math display">\[\begin{align}
&amp; \tilde{m}_1=\operatorname{E}\left[\beta \mid \sigma^2, y\right]=m_0+M_0 X^{\mathrm{\scriptscriptstyle T}} Q^{-1}\left(y-X{m_0}\right) \\
&amp; \tilde{M}_1=M_0-M_0 X^{\mathrm{\scriptscriptstyle T}} Q^{-1} X M_0 \\
\end{align}\]</span></p>
<details>
<summary>
Click to show or hide details
</summary>
<p>Note:</p>
<!-- \begin{align} -->
<!-- & \left(\begin{array}{l} -->
<!-- X_1 \\ -->
<!-- X_2 -->
<!-- \end{array}\right) \sim N\left(\left(\begin{array}{l} -->
<!-- \mu_1 \\ -->
<!-- \mu_2 -->
<!-- \end{array}\right),\left(\begin{array}{ll} -->
<!-- \Sigma_{11} & \Sigma_{12} \\ -->
<!-- \Sigma_{21} & \Sigma_{22} -->
<!-- \end{array}\right)\right) \text { with } \Sigma_{21} = \Sigma_{12}^{\T} \\ -->
<!-- & \Rightarrow X_2 \mid X_1  \sim N\left(\mu_{2 \cdot 1}, \Sigma_{2 \cdot 1}\right) \;, \\ -->
<!-- \end{align} -->
<p><span class="math display">\[\begin{align}
&amp; \left(\begin{array}{l}
X_1 \\
X_2
\end{array}\right) \sim N\left(\left(\begin{array}{l}
\mu_1 \\
\mu_2
\end{array}\right),\left(\begin{array}{ll}
\Sigma_{11} &amp; \Sigma_{12} \\
\Sigma_{21} &amp; \Sigma_{22}
\end{array}\right)\right) \text { with } \Sigma_{21} = \Sigma_{12}^{\mathrm{\scriptscriptstyle T}}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
&amp; \Rightarrow X_2 \mid X_1  \sim N\left(\mu_{2 \cdot 1}, \Sigma_{2 \cdot 1}\right)
\end{align}\]</span></p>
<p>where <span class="math inline">\(\mu_{2 \cdot 1}= \mu_2+\Sigma_{21} \Sigma_{11}^{-1}\left(X_1-\mu_1\right) \text { and } \Sigma_{2 \cdot 1}=\Sigma_{22}-\Sigma_{21} \Sigma_{11}^{-1} \Sigma_{12} \, .\)</span></p>
</details>
</div>
</div>
<div id="bayesian-prediction" class="section level2 hasAnchor" number="1.5">
<h2><span class="header-section-number">1.5</span> Bayesian prediction<a href="basics-of-bayesian-linear-regression.html#bayesian-prediction" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Assume <span class="math inline">\(V=I_{n}\)</span>. Let <span class="math inline">\(\tilde{y}\)</span> denote an <span class="math inline">\(\tilde{n}\times 1\)</span> vector of outcomes. <span class="math inline">\(\tilde{X}\)</span> is corresponding predictors. We seek to predict <span class="math inline">\(\tilde{y}\)</span> based upon <span class="math inline">\(y\)</span></p>
<p><span class="math display">\[\begin{align}
P(\tilde{y} \mid y)
&amp;= t_{2a_1}\left(\tilde{X} M_1 m_1, \frac{b_1}{a_1}\left(I_{\tilde{n}} + \tilde{X} M_{1} \tilde{X}^{\mathrm{\scriptscriptstyle T}}\right)\right) \;
\end{align}\]</span></p>
<details>
<summary>
Click to show or hide details
</summary>
<p><span class="math display">\[\begin{align}
P\left(\beta, \sigma^{2}, \tilde{y} \mid y\right)
&amp;=P\left(\beta, \sigma^{2} \mid y\right) \cdot P\left(\tilde{y} \mid \beta, \sigma^{2}, y\right) \\
&amp;= P\left(\beta, \sigma^{2}\right) \cdot P\left(y \mid \beta, \sigma^{2}\right) \cdot P\left(\tilde{y} \mid \beta, \sigma^{2}, y\right) \\
&amp;= NIG \left(\beta, \sigma^{2} \mid m_{0}, M_{0}, a_{0}, b_{0}\right)
\cdot N\left(y \mid X \beta, \sigma^{2} I_{n}\right)
\cdot N\left(\tilde{y} \mid \tilde{X} \beta, \sigma^{2} I_{\tilde{n}}\right) \\
&amp;= NIG \left(\beta, \sigma^{2} \mid M_{1} m_{1}, M_{1}, a_{1}, b_{1}\right) \cdot N\left(\tilde{y} \mid \tilde{X} \beta, \sigma^{2} I_{\tilde{n}}\right) \\
&amp;= IG\left(\sigma^{2} \mid a_{1}, b_{1}\right) \cdot N\left(\beta \mid M_{1} m_{1}, \sigma^{2} M_{1} \right) \cdot N\left(\tilde{y} \mid \tilde{X} \beta, \sigma^{2} I_{\tilde{n}} \right) \;
\end{align}\]</span></p>
<p>Then we can calculate posterior predictive density <span class="math inline">\(P\left(\tilde{y} \mid y\right)\)</span> from <span class="math inline">\(P\left(\beta, \sigma^{2}, \tilde{y} \mid y\right)\)</span></p>
<p><span class="math display">\[\begin{align}
P\left(\tilde{y} \mid y\right)
&amp;=\iint P\left(\beta, \sigma^{2}, \tilde{y} \mid y\right) \  d\beta \  d\sigma^{2} \\
&amp;=\iint IG\left(\sigma^{2} \mid a_{1}, b_{1}\right) \cdot N\left(\beta \mid M_{1} m_{1}, \sigma^{2} M_{1} \right) \cdot N\left(\tilde{y} \mid \tilde{X} \beta, \sigma^{2} I_{\tilde{n}}\right) \  d\beta \  d\sigma^{2} \\
&amp;=\int IG\left(\sigma^{2} \mid a_{1}, b_{1}\right) \int N\left(\beta \mid M_{1} m_{1}, \sigma^{2} M_{1} \right) \cdot N\left(\tilde{y} \mid \tilde{X} \beta, \sigma^{2} I_{\tilde{n}}\right) \  d\beta \  d\sigma^{2} \\
\end{align}\]</span></p>
<p>As for <span class="math inline">\(\int N\left(\beta \mid M_{1} m_{1}, \sigma^{2} M_{1}\right) \cdot N\left(\tilde{y} \mid \tilde{X} \beta, \sigma^{2} I_{\tilde{n}}\right) \  d\beta\)</span>, we provide an easy way to derive it avoiding any integration at all. Note that we can write the above model as</p>
<p><span class="math display">\[\begin{align}
\tilde{y} &amp;= \tilde{X} \beta + \tilde{\epsilon}, \text{ where } \tilde{\epsilon} \sim N\left(0,\sigma^2 I_{\tilde{n}}\right) \\
\beta &amp;= M_{1} m_{1} + \epsilon_{\beta \mid y}, \text{ where } \epsilon_{\beta \mid y} \sim N\left(0,\sigma^2M_{1}\right)
\end{align}\]</span></p>
<p>where <span class="math inline">\(\tilde{\epsilon}\)</span> and <span class="math inline">\(\epsilon_{\beta \mid y}\)</span> are independent of each other. It then follows that</p>
<p><span class="math display">\[\begin{align}
\tilde{y} &amp;= \tilde{X} M_{1} m_{1} + \tilde{X} \epsilon_{\beta \mid y} + \tilde{\epsilon}
\sim N\left(\tilde{X} M_{1} m_{1}, \sigma^2\left(I_{\tilde{n}} + \tilde{X} M_{1} \tilde{X}^{\mathrm{\scriptscriptstyle T}}\right)\right)
\end{align}\]</span></p>
<p>As a result</p>
<p><span class="math display">\[\begin{align}
P\left(\tilde{y} \mid y\right)
&amp;=\int IG\left(\sigma^{2} \mid a_{1}, b_{1}\right) \cdot N\left(\tilde{X} M_{1} m_{1}, \sigma^2\left(I_{\tilde{n}} + \tilde{X} M_{1} \tilde{X}^{\mathrm{\scriptscriptstyle T}}\right)\right) \  d\sigma^{2} \\
&amp;= t_{2a_1}\left(\tilde{X} M_1 m_1, \frac{b_1}{a_1}\left(I_{\tilde{n}} + \tilde{X} M_{1} \tilde{X}^{\mathrm{\scriptscriptstyle T}}\right)\right) \;
\end{align}\]</span></p>
</details>
</div>
<div id="sampling-from-the-posterior-distribution" class="section level2 hasAnchor" number="1.6">
<h2><span class="header-section-number">1.6</span> Sampling from the posterior distribution<a href="basics-of-bayesian-linear-regression.html#sampling-from-the-posterior-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We can get the joint posterior density <span class="math inline">\(P\left(\beta, \sigma^{2}, \tilde{y} \mid y\right)\)</span> by sampling process</p>
<ol style="list-style-type: decimal">
<li><p>Draw <span class="math inline">\(\hat{\sigma}_{(i)}^{2}\)</span> from <span class="math inline">\(I G\left(a_{1}, b_{1}\right)\)</span></p></li>
<li><p>Draw <span class="math inline">\(\hat{\beta}_{(i)}\)</span> from <span class="math inline">\(N\left(M_{1} m_{1}, \hat{\sigma}_{(i)}^{2} M_{1}\right)\)</span></p></li>
<li><p>Draw <span class="math inline">\(\tilde{y}_{(i)}\)</span> from <span class="math inline">\(N\left(\tilde{X} \hat{\beta}_{(i)}, \hat{\sigma}_{(i)}^{2}I_{\tilde{n}}\right)\)</span></p></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="the-divide-conquer-algorithm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
