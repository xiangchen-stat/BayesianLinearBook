<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Forward Filtering Backward Sampling - Shared Variance | Bayesian Linear Regression Tutorial</title>
  <meta name="description" content="This is a first tutorial for Bayesian Linear Regression assembled in book form." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Forward Filtering Backward Sampling - Shared Variance | Bayesian Linear Regression Tutorial" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a first tutorial for Bayesian Linear Regression assembled in book form." />
  <meta name="github-repo" content="xiangchen-stat/BayesianLinearBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Forward Filtering Backward Sampling - Shared Variance | Bayesian Linear Regression Tutorial" />
  
  <meta name="twitter:description" content="This is a first tutorial for Bayesian Linear Regression assembled in book form." />
  

<meta name="author" content="Xiang Chen, Valentina Arputhasamy, Daniel Zhou, Sudipto Banerjee" />


<meta name="date" content="2023-02-13" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="the-divide-conquer-algorithm.html"/>
<link rel="next" href="appendix.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html"><i class="fa fa-check"></i><b>1</b> Basics of Bayesian linear regression</a>
<ul>
<li class="chapter" data-level="1.1" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html#bayes-theorem"><i class="fa fa-check"></i><b>1.1</b> Bayes’ theorem</a></li>
<li class="chapter" data-level="1.2" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html#normal-inverse-gamma-nig-prior"><i class="fa fa-check"></i><b>1.2</b> Normal-Inverse-Gamma (NIG) prior</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html#joint-distribution-of-nig-prior"><i class="fa fa-check"></i><b>1.2.1</b> Joint distribution of NIG prior</a></li>
<li class="chapter" data-level="1.2.2" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html#marginal-distribution-of-nig-prior"><i class="fa fa-check"></i><b>1.2.2</b> Marginal distribution of NIG prior</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html#conjugate-bayesian-linear-regression-and-mm-formula"><i class="fa fa-check"></i><b>1.3</b> Conjugate Bayesian linear regression and M&amp;m formula</a></li>
<li class="chapter" data-level="1.4" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html#updating-form-of-the-posterior-distribution"><i class="fa fa-check"></i><b>1.4</b> Updating form of the posterior distribution</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html#method-1-sherman-woodbury-morrison-identity"><i class="fa fa-check"></i><b>1.4.1</b> Method 1: Sherman-Woodbury-Morrison identity</a></li>
<li class="chapter" data-level="1.4.2" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html#method-2-distribution-theory"><i class="fa fa-check"></i><b>1.4.2</b> Method 2: Distribution theory</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html#bayesian-prediction"><i class="fa fa-check"></i><b>1.5</b> Bayesian prediction</a></li>
<li class="chapter" data-level="1.6" data-path="basics-of-bayesian-linear-regression.html"><a href="basics-of-bayesian-linear-regression.html#sampling-from-the-posterior-distribution"><i class="fa fa-check"></i><b>1.6</b> Sampling from the posterior distribution</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-divide-conquer-algorithm.html"><a href="the-divide-conquer-algorithm.html"><i class="fa fa-check"></i><b>2</b> The Divide &amp; Conquer Algorithm</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-divide-conquer-algorithm.html"><a href="the-divide-conquer-algorithm.html#the-problem"><i class="fa fa-check"></i><b>2.1</b> The Problem</a></li>
<li class="chapter" data-level="2.2" data-path="the-divide-conquer-algorithm.html"><a href="the-divide-conquer-algorithm.html#the-solution"><i class="fa fa-check"></i><b>2.2</b> The Solution</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="forward-filtering-backward-sampling---shared-variance.html"><a href="forward-filtering-backward-sampling---shared-variance.html"><i class="fa fa-check"></i><b>3</b> Forward Filtering Backward Sampling - Shared Variance</a>
<ul>
<li class="chapter" data-level="3.1" data-path="forward-filtering-backward-sampling---shared-variance.html"><a href="forward-filtering-backward-sampling---shared-variance.html#background"><i class="fa fa-check"></i><b>3.1</b> Background</a></li>
<li class="chapter" data-level="3.2" data-path="forward-filtering-backward-sampling---shared-variance.html"><a href="forward-filtering-backward-sampling---shared-variance.html#derivation-of-the-forward-filter"><i class="fa fa-check"></i><b>3.2</b> Derivation of the Forward Filter</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="forward-filtering-backward-sampling---shared-variance.html"><a href="forward-filtering-backward-sampling---shared-variance.html#deriving-beta_tvert-y_t"><i class="fa fa-check"></i><b>3.2.1</b> Deriving <span class="math inline">\(\beta_{t}\vert Y_{t}\)</span></a></li>
<li class="chapter" data-level="3.2.2" data-path="forward-filtering-backward-sampling---shared-variance.html"><a href="forward-filtering-backward-sampling---shared-variance.html#deriving-sigma2vert-y_t"><i class="fa fa-check"></i><b>3.2.2</b> Deriving <span class="math inline">\(\sigma^{2}\vert Y_{t}\)</span></a></li>
<li class="chapter" data-level="3.2.3" data-path="forward-filtering-backward-sampling---shared-variance.html"><a href="forward-filtering-backward-sampling---shared-variance.html#commentary"><i class="fa fa-check"></i><b>3.2.3</b> Commentary</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="forward-filtering-backward-sampling---shared-variance.html"><a href="forward-filtering-backward-sampling---shared-variance.html#derivation-of-the-backwards-sampling"><i class="fa fa-check"></i><b>3.3</b> Derivation of the Backwards Sampling</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a>
<ul>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#deriving-x_2-vert-x_1-when-x_1-x_2mathrmscriptscriptstyle-t-is-a-block-normal-multivariate-random-variable"><i class="fa fa-check"></i>Deriving <span class="math inline">\(x_{2} \vert x_{1}\)</span> when <span class="math inline">\((x_{1} x_{2})^{\mathrm{\scriptscriptstyle T}}\)</span> is a block-normal multivariate random variable</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Linear Regression Tutorial</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="forward-filtering-backward-sampling---shared-variance" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> Forward Filtering Backward Sampling - Shared Variance<a href="forward-filtering-backward-sampling---shared-variance.html#forward-filtering-backward-sampling---shared-variance" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This chapter discusses the Dynamic Linear Model with a scale factor for the variance shared across time and its derivations at each step. The approach taken in this chapter is borrowed from West and Harrison (1997), with some details derived from Petris et al (2009). The solution we take to estimate the parameters of this model is utilized via Forward Filtering Backward Sampling.</p>
<p>For full generality and to maintain a multivariate normal system in both the data and parameter matrices, we assume all <span class="math inline">\(Y_{t} \in \mathbb{R}^{n}\)</span>, <span class="math inline">\(\beta_{t} \in \mathbb{R}^{p}\)</span>, and <span class="math inline">\(t \in \{1,\ldots,T\}\)</span> for some integer <span class="math inline">\(T\)</span>.</p>
<div id="background" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Background<a href="forward-filtering-backward-sampling---shared-variance.html#background" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The model we are concerned with studying is a class of time-varying models called the Dynamic Linear Model. The setup for the equation follows:</p>
<p><span class="math display">\[\begin{eqnarray*}
Y_{t}\vert\beta_{t}, \sigma^{2} &amp;\sim&amp; N(F_{t}^{\mathrm{\scriptscriptstyle T}}\beta_{t}, \sigma^{2}V_{t})\\
\beta_{t}\vert \beta_{t-1},\sigma^{2} &amp;\sim&amp; N(G_{t}\beta_{t-1}, \sigma^{2}W_{t})\\
\sigma^{-2} &amp;\sim&amp; \Gamma(a_{t-1},b_{t-1})\\
\beta_{t-1}\vert \sigma^{2} &amp;\sim&amp; N(m_{t-1}, \sigma^{2}C_{t-1})\\
\end{eqnarray*}\]</span></p>
<p>Alternatively, using Normal-Inverse Gamma notation, where, if <span class="math inline">\(\sigma^{-2} \sim \Gamma(a_{t-1},b_{t-1})\)</span>, <span class="math inline">\(\sigma^{2} \sim IG(a_{t-1},b_{t-1})\)</span>, where <span class="math inline">\(IG\)</span> denotes an inverse Gamma distribution, we may write the above set of equations as the following:
<span class="math display">\[\begin{eqnarray*}
Y_{t},\sigma^{2}\vert \beta_{t} &amp;\sim&amp; NIG(F_{t}^{\mathrm{\scriptscriptstyle T}}\beta_{t}, V_{t}, a_{t-1}, b_{t-1})\\
\beta_{t},\sigma^{2}\vert \beta_{t-1} &amp;\sim&amp; NIG(G_{t}\beta_{t-1}, W_{t}, a_{t-1}, b_{t-1})\\
\beta_{t-1},\sigma^{2} &amp;\sim&amp; NIG(m_{t-1}, C_{t-1}, a_{t-1}, b_{t-1})
\end{eqnarray*}\]</span></p>
<p>The task is to acquire estimates for <span class="math inline">\(\beta_{0,\ldots,T}\)</span> and <span class="math inline">\(\sigma^{2}\)</span>. This task may be divided into the forward filter and backwards sampling steps (collectively referred to as the Forward Filter-Backwards Sampling (FFBS) algorithm): The forward filter to acquire sequential estimates, and the backwards sampling step to retroactively “smooth” our initial estimates given estimates at the last time stamp. We are given a set of observations <span class="math inline">\(Y_{t,j}\)</span>, and known parameters <span class="math inline">\(F_{t}\)</span>, <span class="math inline">\(G_{t}\)</span>, <span class="math inline">\(V_{t}\)</span>, <span class="math inline">\(W_{t}\)</span>, and <span class="math inline">\(n_{t-1}\)</span>, although Frankenburg and Banerjee also apply FFBS to cases where <span class="math inline">\(F_{t}\)</span> and <span class="math inline">\(G_{t}\)</span> are not pre-specified.</p>
</div>
<div id="derivation-of-the-forward-filter" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Derivation of the Forward Filter<a href="forward-filtering-backward-sampling---shared-variance.html#derivation-of-the-forward-filter" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We proceed for some arbitrary <span class="math inline">\(t\)</span>:</p>
<p><span class="math display">\[\begin{eqnarray*}
\beta_{t} &amp;=&amp; G_{t}\beta_{t-1} + \omega_{t}, \omega_{t} \sim N(0, \sigma^{2}W_{t})\\
\beta_{t}\vert \sigma^{2} &amp;\sim&amp; N(G_{t}m_{t-1}, \sigma^{2}(G_{t}C_{t-1}G_{t}^{\mathrm{\scriptscriptstyle T}} + W_{t}))\\
\end{eqnarray*}\]</span></p>
<p>Now, let <span class="math inline">\(m^{*}_{t} = G_{t}m_{t-1}\)</span> and <span class="math inline">\(R_{t} = G_{t}C_{t-1}G_{t}^{\mathrm{\scriptscriptstyle T}} + W_{t}\)</span>. We then have:</p>
<p><span class="math display">\[\begin{eqnarray*}
Y_{t} &amp;=&amp; F_{t}^{\mathrm{\scriptscriptstyle T}}\beta_{t} + \nu_{t}, \nu_{t}\sim N(0, \sigma^{2}V_{t})\\
Y_{t}\vert \sigma^{2} &amp;\sim&amp; N(F_{t}^{\mathrm{\scriptscriptstyle T}}m^{*}_{t}, \sigma^{2}(F_{t}^{\mathrm{\scriptscriptstyle T}}R_{t}F_{t} + V_{t}))
\end{eqnarray*}\]</span></p>
<p>Since <span class="math inline">\(\sigma^{2} \sim IG(a_{t-1},b_{t-1})\)</span>, we marginalize it out of <span class="math inline">\(Y_{t}\vert \sigma^{2}\)</span> to get</p>
<p><span class="math display">\[\begin{eqnarray*}
Y_{t} &amp;\sim&amp; T_{2a_{t-1}}(F_{t}^{\mathrm{\scriptscriptstyle T}}m^{*}_{t}, \frac{b_{t-1}}{a_{t-1}}(F_{t}^{\mathrm{\scriptscriptstyle T}}R_{t}F_{t} + V_{t}))
\end{eqnarray*}\]</span></p>
<p>We now have the apparatus needed to compute the sequential posterior <span class="math inline">\(\beta_{t}\vert Y_{t}\)</span> and <span class="math inline">\(\sigma^{2}\vert Y_{t}\)</span>:</p>
<div id="deriving-beta_tvert-y_t" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Deriving <span class="math inline">\(\beta_{t}\vert Y_{t}\)</span><a href="forward-filtering-backward-sampling---shared-variance.html#deriving-beta_tvert-y_t" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math display">\[\begin{eqnarray*}
p(\beta_{t} \vert Y_{t}, \sigma^{2}) &amp;\propto&amp; p(\beta_{t}, Y_{t}\vert \sigma^{2})\\
&amp;\propto&amp; p(Y_{t}\vert \beta_{t},\sigma^{2})p(\beta_{t}\vert \sigma^{2})\\
&amp;\propto&amp; \sigma^{-n}\exp(-\frac{1}{2\sigma^{2}}(y_{t} - F_{t}^{\mathrm{\scriptscriptstyle T}}\beta_{t})^{\mathrm{\scriptscriptstyle T}}V_{t}^{-1}(y_{t} - F_{t}^{\mathrm{\scriptscriptstyle T}}\beta_{t}))\sigma^{-p}\exp(-\frac{1}{2\sigma^{2}}(\beta_{t} - m^{*}_{t})^{\mathrm{\scriptscriptstyle T}}R_{t}^{-1}(\beta_{t} - m^{*}_{t}))\\
&amp;\propto&amp; \sigma^{-(n+p)}\exp(-\frac{1}{2\sigma^{2}}[(y_{t} - F_{t}^{\mathrm{\scriptscriptstyle T}}\beta_{t})^{\mathrm{\scriptscriptstyle T}}V_{t}^{-1}(y_{t} - F_{t}^{\mathrm{\scriptscriptstyle T}}\beta_{t}) + (\beta_{t} - m^{*}_{t})^{\mathrm{\scriptscriptstyle T}}R_{t}^{-1}(\beta_{t} - m^{*}_{t})])\\
\end{eqnarray*}\]</span></p>
<p>Note next that
<span class="math display">\[\begin{eqnarray*}
\begin{bmatrix}Y_{t}\\ \beta_{t}\end{bmatrix}\vert \sigma^{2} &amp;\sim&amp; N\left(\begin{bmatrix}F_{t}^{\mathrm{\scriptscriptstyle T}}m^{*}_{t}\\ m^{*}_{t}\end{bmatrix},\sigma^{2}\begin{bmatrix}F_{t}^{\mathrm{\scriptscriptstyle T}}R_{t}F_{t} + V_{t} &amp; F_{t}^{\mathrm{\scriptscriptstyle T}}R_{t}\\
R_{t}F_{t} &amp; R_{t}\end{bmatrix}\right)
\end{eqnarray*}\]</span></p>
<p>with the cross-terms <span class="math inline">\(\mathrm{Cov}(Y_{t},\beta_{t}) = \mathrm{Cov}(F_{t}^{\mathrm{\scriptscriptstyle T}}\beta_{t} + \nu_{t},\beta_{t}) = F_{t}^{\mathrm{\scriptscriptstyle T}}\mathrm{Cov}(\beta_{t}, \beta_{t}) = F_{t}^{\mathrm{\scriptscriptstyle T}}R_{t}\)</span>.</p>
<p>Since, for the following block-normal system</p>
<p><span class="math display">\[\begin{eqnarray*}
\begin{bmatrix}x_{1}\\ x_{2}\end{bmatrix} &amp;\sim&amp; N\left(\begin{bmatrix}\mu_{1}\\ \mu_{2}\end{bmatrix}, \begin{bmatrix}\Sigma_{11} &amp; \Sigma_{12}\\
\Sigma_{21} &amp; \Sigma_{22}\end{bmatrix}\right)
\end{eqnarray*}\]</span></p>
<p>we have</p>
<p><span class="math display">\[\begin{eqnarray*}
x_{2}\vert x_{1} &amp;\sim&amp; N(\mu_{2} + \Sigma_{21}\Sigma_{11}^{-1}(x_{1} - \mu_{1}), \Sigma_{22} - \Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12})
\end{eqnarray*}\]</span></p>
<p>(The derivation of the density of <span class="math inline">\(x_{2}\vert x_{1}\)</span> can be found in the Appendix.)</p>
<p>We arrive at,</p>
<p><span class="math display">\[\begin{eqnarray*}
\beta_{t}\vert \sigma^{2},Y_{t} &amp;\sim&amp; N(m_{t}^{*} + R_{t}F_{t}(F_{t}^{\mathrm{\scriptscriptstyle T}}R_{t}F_{t} + V_{t})^{-1}(Y_{t} - F_{t}^{\mathrm{\scriptscriptstyle T}}m_{t}^{*}), R_{t} - R_{t}F_{t}(F_{t}^{\mathrm{\scriptscriptstyle T}}R_{t}F_{t} + V_{t})^{-1}F_{t}^{\mathrm{\scriptscriptstyle T}}R_{t})\\
&amp;\sim&amp; N(m_{t}^{*} + R_{t}F_{t}Q_{t}^{-1}(Y_{t} - F_{t}^{\mathrm{\scriptscriptstyle T}}m_{t}^{*}), R_{t} - R_{t}F_{t}Q_{t}^{-1}F_{t}^{\mathrm{\scriptscriptstyle T}}R_{t})
\end{eqnarray*}\]</span></p>
<p>where <span class="math inline">\(Q_{t} = F_{t}^{\mathrm{\scriptscriptstyle T}}R_{t}F_{t} + V_{t}\)</span>.</p>
<p>(Note that Petris’s expression for the variance suffers from a typo; to see this, simply take their <span class="math inline">\(\widetilde{C}_{t}^{\mathrm{\scriptscriptstyle T}}\)</span>.)</p>
</div>
<div id="deriving-sigma2vert-y_t" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Deriving <span class="math inline">\(\sigma^{2}\vert Y_{t}\)</span><a href="forward-filtering-backward-sampling---shared-variance.html#deriving-sigma2vert-y_t" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We next deduce the density of <span class="math inline">\(\sigma^{2}\vert Y_{t}\)</span>. Note before we begin that since <span class="math inline">\(Y_{t} \sim T_{2a_{t-1}}(F_{t}^{\mathrm{\scriptscriptstyle T}}m^{*}_{t}, Q_{t}) = \int NIG_{Y_{t}}(F_{t}^{\mathrm{\scriptscriptstyle T}}m^{*}_{t}, Q_{t}, a_{t-1}, b_{t-1})d\sigma^{2}\)</span>, we can write <span class="math inline">\(Y_{t}\vert \sigma^{2} \sim N(F_{t}m^{*}_{t}, \sigma^{2}Q_{t})\)</span>. Hence:</p>
<p><span class="math display">\[\begin{eqnarray*}
p(\sigma^{2}\vert Y_{t}) &amp;\propto&amp; p(Y_{t}\vert \sigma^{2})p(\sigma^{2})\\
&amp;\propto&amp; \sigma^{-n}\exp(-\frac{1}{2\sigma^{2}}(y_{t} - F_{t}^{\mathrm{\scriptscriptstyle T}}m^{*}_{t})^{\mathrm{\scriptscriptstyle T}}Q_{t}^{-1}(y_{t} - F_{t}^{\mathrm{\scriptscriptstyle T}}m^{*}_{t}))\sigma^{-2(a_{t-1} + 1)}\exp(-b_{t-1}\sigma^{-2})\\
&amp;\propto&amp; \sigma^{-2(a_{t-1} + \frac{n}{2} + 1)}\exp(-\sigma^{-2}[\frac{1}{2}(y_{t} - F_{t}^{\mathrm{\scriptscriptstyle T}}m^{*}_{t})^{\mathrm{\scriptscriptstyle T}}Q_{t}^{-1}(y_{t} - F_{t}^{\mathrm{\scriptscriptstyle T}}m^{*}_{t}) + b_{t-1}])
\end{eqnarray*}\]</span></p>
<p>We conclude that <span class="math inline">\(\sigma^{-2}\vert Y_{t} \sim \Gamma(a_{t},b_{t})\)</span>, where <span class="math inline">\(a_{t} = a_{t-1} + \frac{n}{2}\)</span> and <span class="math inline">\(b_{t} = b_{t-1} + \frac{1}{2}(y_{t} - F_{t}^{\mathrm{\scriptscriptstyle T}}m^{*}_{t})^{\mathrm{\scriptscriptstyle T}}Q_{t}^{-1}(y_{t} - F_{t}^{\mathrm{\scriptscriptstyle T}}m^{*}_{t})\)</span>.</p>
<p>This gives us the set of updating equations according to Petris Proposition 4.1.</p>
</div>
<div id="commentary" class="section level3 hasAnchor" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Commentary<a href="forward-filtering-backward-sampling---shared-variance.html#commentary" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Note that we have derived the forward filtering step for the set of equations for time <span class="math inline">\(t\)</span> given the parameters for the distributions at time <span class="math inline">\(t-1\)</span>. Hence the equation’s setup is Markovian, i.e. the state of this set of equations only depends on that of the preceding time point. Nevertheless, applications where the forward filter’s equations propagate from an initial time point <span class="math inline">\(t=0\)</span> are written so that the dependence of the parameters’ values <span class="math inline">\(\beta_{t}\)</span> and <span class="math inline">\(\sigma^{2}\)</span> on the data up to time <span class="math inline">\(t-1\)</span> or time <span class="math inline">\(t\)</span> are made explicit. Specifically, letting <span class="math inline">\(D_{t} = \{Y_{\tau}\}_{\tau=1,\ldots,t}\)</span>, we may write the set of equations in our setup as:</p>
<p><span class="math display">\[\begin{eqnarray*}
Y_{t},\sigma^{2}\vert \beta_{t},D_{t-1} &amp;\sim&amp; NIG(F_{t}^{\mathrm{\scriptscriptstyle T}}\beta_{t}, V_{t}, a_{t-1}, b_{t-1})\\
\beta_{t},\sigma^{2}\vert \beta_{t-1},D_{t-1} &amp;\sim&amp; NIG(G_{t}\beta_{t-1}, W_{t}, a_{t-1}, b_{t-1})\\
\beta_{t-1},\sigma^{2}\vert D_{t-1} &amp;\sim&amp; NIG(m_{t-1}, C_{t-1}, a_{t-1}, b_{t-1})
\end{eqnarray*}\]</span></p>
<p>and the sequential posteriors we have derived, <span class="math inline">\(\beta_{t}\vert Y_{t}\)</span> and <span class="math inline">\(\sigma^{2}\vert Y_{t}\)</span>, as <span class="math inline">\(\beta_{t}\vert D_{t}\)</span> and <span class="math inline">\(\sigma^{2} \vert D_{t}\)</span> respectively.</p>
</div>
</div>
<div id="derivation-of-the-backwards-sampling" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Derivation of the Backwards Sampling<a href="forward-filtering-backward-sampling---shared-variance.html#derivation-of-the-backwards-sampling" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now that we have the parameters <span class="math inline">\(\{\theta_{t},\phi\vert D_{t}\}_{t=1,\ldots,T}\)</span>, we would like to work backwards and derive <span class="math inline">\(\{\theta_{t},\phi\vert \theta_{t+1}, D_{T}\}_{t=1,\ldots,T-1}\)</span> to smooth our initial variable estimates:</p>
<!-- TODO: Include pictoral graph here -->
<p><span class="math display">\[\begin{eqnarray*}
p(\theta_{t}\vert \theta_{(t+1):T},\sigma^{2},D_{T}) &amp;=&amp; p(\theta_{t}\vert \theta_{t+1},\sigma^{2},D_{t})\\
&amp;=&amp; p(\theta_{t}\vert \theta_{t+1},\sigma^{2},D_{t})\\
&amp;=&amp; \frac{p(\theta_{t+1}\vert \theta_{t},D_{t})p(\theta_{t}\vert D_{t})}{p(\theta_{t+1}\vert D_{t})}\\
&amp;\propto&amp; p(\theta_{t+1}\vert \theta_{t},D_{t})p(\theta_{t}\vert D_{t})\\
&amp;\propto&amp; \exp\left(-\frac{1}{2\sigma^{2}}\left[(\theta_{t+1} - G_{t+1}\theta_{t})^{\mathrm{\scriptscriptstyle T}}W_{t+1}^{-1}(\theta_{t+1} - G_{t+1}\theta_{t})\right.\right.\\
&amp;&amp;\left.\left.+ (\theta_{t} - m_{t})^{\mathrm{\scriptscriptstyle T}}C_{t}^{-1}(\theta_{t} - m_{t})\right]\right)\\
&amp;\propto&amp; \exp\left(-\frac{1}{2\sigma^{2}}\left[\theta_{t+1}^{\mathrm{\scriptscriptstyle T}}W_{t+1}^{-1}\theta_{t+1} - 2\theta_{t+1}^{\mathrm{\scriptscriptstyle T}}W_{t+1}^{-1}G_{t+1}\theta_{t} + \theta_{t}^{\mathrm{\scriptscriptstyle T}}G_{t+1}^{\mathrm{\scriptscriptstyle T}}W_{t+1}^{-1}G_{t+1}\theta_{t}\right.\right.\\
&amp;&amp;\left.\left.+\theta_{t}^{\mathrm{\scriptscriptstyle T}}C_{t}^{-1}\theta_{t} - 2m_{t}^{\mathrm{\scriptscriptstyle T}}C_{t}^{-1}\theta_{t} + m_{t}^{\mathrm{\scriptscriptstyle T}}C_{t}^{-1}m_{t}\right]\right)\\
&amp;\propto&amp; \exp\left(-\frac{1}{2\sigma^{2}}\left[\theta_{t}^{\mathrm{\scriptscriptstyle T}}(G_{t+1}^{\mathrm{\scriptscriptstyle T}}W_{t+1}^{-1}G_{t+1} + C_{t}^{-1})\theta_{t} - 2(C_{t}^{-1}m_{t} + G_{t+1}^{\mathrm{\scriptscriptstyle T}}W_{t+1}^{-1}\theta_{t+1})^{\mathrm{\scriptscriptstyle T}}\theta_{t}\right]\right)\\
\theta_{t}\vert\theta_{t+1},\sigma^{2},D_{T} &amp;\sim&amp; N\left((G_{t+1}^{\mathrm{\scriptscriptstyle T}}W_{t+1}^{-1}G_{t+1} + C_{t}^{-1})^{-1}(C_{t}^{-1}m_{t} + G_{t+1}^{\mathrm{\scriptscriptstyle T}}W_{t+1}^{-1}\theta_{t+1}),\right.\\
&amp;&amp;\left.\sigma^{-2}(G_{t+1}^{\mathrm{\scriptscriptstyle T}}W_{t+1}^{-1}G_{t+1} + C_{t}^{-1})^{-1}\right)\\
&amp;\sim&amp; N\left(m_{t} - C_{t}G_{t+1}^{\mathrm{\scriptscriptstyle T}}(W_{t+1} + G_{t+1}C_{t}G_{t+1}^{\mathrm{\scriptscriptstyle T}})^{-1}G_{t+1}m_{t} + C_{t}G_{t+1}^{\mathrm{\scriptscriptstyle T}}W_{t+1}^{-1}\theta_{t+1}\right.\\
&amp;&amp;\left.- C_{t}G_{t+1}^{\mathrm{\scriptscriptstyle T}}(W_{t+1} + G_{t+1}C_{t}G_{t+1}^{\mathrm{\scriptscriptstyle T}})^{-1}G_{t+1}C_{t}G_{t+1}^{\mathrm{\scriptscriptstyle T}}W_{t+1}^{-1}\theta_{t+1},\right.\\
&amp;&amp;\left.C_{t} - C_{t}G_{t+1}^{\mathrm{\scriptscriptstyle T}}(W_{t+1} + G_{t+1}C_{t}G_{t+1}^{\mathrm{\scriptscriptstyle T}})^{-1}G_{t+1}C_{t}\right)\\
&amp;\sim&amp; N\left(m_{t} - C_{t}G_{t+1}^{\mathrm{\scriptscriptstyle T}}R_{t+1}^{-1}G_{t+1}m_{t} + C_{t}G_{t+1}^{\mathrm{\scriptscriptstyle T}}W_{t+1}^{-1}\theta_{t+1}\right.\\
&amp;&amp;\left. - C_{t}G_{t+1}^{\mathrm{\scriptscriptstyle T}}R_{t+1}^{-1}G_{t+1}C_{t}G_{t+1}^{\mathrm{\scriptscriptstyle T}}W_{t+1}^{-1}\theta_{t+1},\right.\\
&amp;&amp;\left.C_{t} - C_{t}G_{t+1}^{\mathrm{\scriptscriptstyle T}}R_{t+1}^{-1}G_{t+1}C_{t}\right)\\
\end{eqnarray*}\]</span></p>
<p>Notice that
<span class="math display">\[\begin{eqnarray*}
C_{t}G_{t+1}^{\mathrm{\scriptscriptstyle T}}R_{t+1}^{-1}G_{t+1}C_{t}G_{t+1}^{\mathrm{\scriptscriptstyle T}}W_{t+1}^{-1}\theta_{t+1} &amp;=&amp; C_{t}G_{t+1}^{\mathrm{\scriptscriptstyle T}}R_{t+1}^{-1}(G_{t+1}C_{t}G_{t+1}^{\mathrm{\scriptscriptstyle T}} + W_{t+1} - W_{t+1})W_{t+1}^{-1}\theta_{t+1}\\
&amp;=&amp; C_{t}G_{t+1}^{\mathrm{\scriptscriptstyle T}}R_{t+1}^{-1}(R_{t+1} - W_{t+1})W_{t+1}^{-1}\theta_{t+1}\\
&amp;=&amp; C_{t}G_{t+1}^{\mathrm{\scriptscriptstyle T}}W_{t+1}^{-1}\theta_{t+1} - C_{t}G_{t+1}^{\mathrm{\scriptscriptstyle T}}R_{t+1}^{-1}\theta_{t+1}
\end{eqnarray*}\]</span></p>
<p>Hence,
<span class="math display">\[\begin{eqnarray*}
\theta_{t}\vert\theta_{t+1},\sigma^{2},D_{T} &amp;\sim&amp; N\left(m_{t} - C_{t}G_{t+1}^{\mathrm{\scriptscriptstyle T}}R_{t+1}^{-1}G_{t+1}m_{t} + C_{t}G_{t+1}^{\mathrm{\scriptscriptstyle T}}W_{t+1}^{-1}\theta_{t+1}\right.\\
&amp;&amp;\left. - C_{t}G_{t+1}^{\mathrm{\scriptscriptstyle T}}W_{t+1}^{-1}\theta_{t+1} + C_{t}G_{t+1}^{\mathrm{\scriptscriptstyle T}}R_{t+1}^{-1}\theta_{t+1},\right.\\
&amp;&amp;\left.C_{t} - C_{t}G_{t+1}^{\mathrm{\scriptscriptstyle T}}R_{t+1}^{-1}G_{t+1}C_{t}\right)\\
&amp;\sim&amp; N(m_{t} + C_{t}G_{t+1}^{\mathrm{\scriptscriptstyle T}}R_{t+1}^{-1}(\theta_{t+1} - a_{t+1}), C_{t} - C_{t}G_{t+1}^{\mathrm{\scriptscriptstyle T}}R_{t+1}^{-1}G_{t+1}C_{t})
\end{eqnarray*}\]</span></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="the-divide-conquer-algorithm.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="appendix.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
